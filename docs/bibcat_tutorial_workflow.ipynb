{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71ff86",
   "metadata": {},
   "source": [
    "# Bibliography Categorization: 'BibCat'\n",
    "## Tutorial: High-Level Workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ab7e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234a479",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction.\n",
    "\n",
    "### What is bibcat?\n",
    "\n",
    "In a nutshell, bibcat is a codebase meant to automate the classification of the way a 'mission', e.g. an observatory or telescope, is used.  It is useful for institutions to keep track of missions within the community, i.e. track the way their data is being used by the community. It is useful for the Space Telescope Science Insttute (STScI), for example, to track the number of published papers that use the Hubble Space Telescope (HST) for science, as opposed to, say, just mentioning HST in passing. However, many, many papers have been published in astronomy, with new papers published each day. Doing this classification by hand can therefore be quite time-consuming.\n",
    "\n",
    "This codebase, known as \"better title pending\" and affectionately nicknamed `bibcat`, was designed to automate this process: to accept text, and ultimately return the classification of how the target mission was used in that text, based on the way the text talks about that mission.\n",
    "\n",
    "### So how does bibcat work?\n",
    "\n",
    "In short, bibcat performs its classification in three stages:\n",
    "\n",
    "* Extract the part of the text that refers to the target mission.\n",
    "* Based on the user's specifications, streamline, simplify, and/or anonymize that subset of the text to remove as much extraneous information from that subset as able.  This stage was implemented to reduce the amount of \"noise\" or \"bias\" contained within the text subset.\n",
    "* Feed that streamlined text subset into one of the available classifier types and return a classification.\n",
    "\n",
    "There is quite a lot of technical detail contained within these (extremely!) high-level stages, but for now: let's visualize this process with a demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f050727",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59681d1f",
   "metadata": {},
   "source": [
    "## User Workflow: Minimum Use-Case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef5f4c7",
   "metadata": {},
   "source": [
    "For our minimum workflow, our goals are to:\n",
    "* Read in some text.\n",
    "* Compile a \"paragraph\" from that text, containing all sentences that refer to our target mission (e.g., to HST).\n",
    "* Classify that text (e.g., as \"science\", \"mention\", etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131384b",
   "metadata": {},
   "source": [
    "We'll start by importing the module and setting some global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffdeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for fetching necessary bibcat modules for the tutorial\n",
    "# Check work directories: src/ is where all source python scripts are available. \n",
    "current_dir= os.path.dirname(os.path.abspath('__file__'))\n",
    "_parent = os.path.dirname(current_dir)\n",
    "src_dir = os.path.join(_parent, \"src\")\n",
    "\n",
    "print(f'Current Directory: {current_dir}')\n",
    "print(f'Source directory: {src_dir}')\n",
    "\n",
    "# move to the ../src/ directory to import necessary modules. \n",
    "os.chdir(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fdf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import bibcat modules\n",
    "import bibcat_classes as bibcat #The collection of bibcat classes\n",
    "import bibcat_config as config  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ae504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load file locations\n",
    "filepath_allmodels = config.dir_allmodels\n",
    "filepath_allmodels\n",
    "name_model = config.name_model\n",
    "filepath_model = os.path.join(filepath_allmodels, name_model, (name_model+\".npy\"))\n",
    "filepath_model\n",
    "fileloc_ML = os.path.join(filepath_allmodels, name_model, (\"tfoutput_\"+name_model))\n",
    "fileloc_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6745da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set some global variables\n",
    "lookup = \"HST\" #\"Kepler\"\n",
    "mode = \"none\" #\"none\" = Does not simplify or modify the text, just for first tutorial\n",
    "\n",
    "#Set some probability thresholds for classification\n",
    "thres_ML = 0.80\n",
    "thres_rules = 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build some fake text to classify\n",
    "text = (\" Modern telescopes allow us to observe the night sky at higher sensitivity than ever.\"\n",
    "        +\" This has allowed us to push our observations toward fainter and fainter stars.\"\n",
    "        +\" In this paper, we present a statistical analysis of new Hubble\"\n",
    "        +\" ultraviolet spectra and their stark dependence on stellar mass,\"\n",
    "        +\" and show that the latest data illustrate strong trends in the so far unexplored\"\n",
    "        +\" parameter space.\"\n",
    "        +\" We ultimately characterize stellar radiation for both faint and bright stars.\"\n",
    "        +\" The target stars are discussed in Section 1.\"\n",
    "        +\" The new HST observations are fully presented in Section 2.\"\n",
    "        +\" The spectra are plotted, and the UV models fitted, in Section 3.\"\n",
    "        +\" The statistical trends in the data are presented in Section 4 and MCMC fits in Appendix C.\"\n",
    "        +\" Finally, results are summarized and discussed in Sections 5 and 6, respectively.\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82703ca",
   "metadata": {},
   "source": [
    "Next, we define what our target missions are using the Keyword class.  As an example, let's define the missions HST, Kepler, and K2.\n",
    "\n",
    "Note: There is no internal hardcoding of any missions within the codebase, so any missions could be used here.  If the target mission does not appear in the text, then the text will just not be classified.\n",
    "\n",
    "(Advanced Note: There is a subtlety here, where the machine learning model can be trained on a training set containing text about specific missions.  Potential biases can result from this training (e.g., if most HST text in the training set is \"science\", then all future input HST text might be classified as \"science\" based on that association).  This bias can be circumvented by having the codebase anonymize the training set (with respect to the missions within) before training the machine learning model on that anonymized text.  But we won't get into those details for this minimum-use case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f557f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch some Keyword objects (for specifying the mission)\n",
    "#For HST\n",
    "kobj_hubble = params.keyword_obj_HST\n",
    "#For Kepler\n",
    "kobj_kepler = params.keyword_obj_Kepler\n",
    "#For K2\n",
    "kobj_k2 = params.keyword_obj_K2\n",
    "\n",
    "#Collect the missions into a convenient list\n",
    "all_keyobjs = [kobj_hubble, kobj_kepler, kobj_k2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199aa38",
   "metadata": {},
   "source": [
    "Now, let's initialize our classifiers.  We have two types of classifiers implemented: the 'machine learning' (aka, the 'data-driven') classifier and the 'rule-based' (aka, the 'attempt-at-analytical') classifier.\n",
    "\n",
    "Initializing the machine learning classifier may take a few seconds, but only has to be done once.  The same classifier can be recycled/reused for multiple texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initialize classifiers\n",
    "#The machine learning classifier\n",
    "print(filepath_model)\n",
    "print(fileloc_ML)\n",
    "classifier_ML = bibcat.Classifier_ML(filepath_model=filepath_model, fileloc_ML=fileloc_ML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The rule-based classifier\n",
    "classifier_rules = bibcat.Classifier_Rules()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda5f39",
   "metadata": {},
   "source": [
    "Finally, let's initialize our operators.  These operators are what take in the text, process the text if requested (i.e., do any text streamlining or simplification, based on the specified modif), and ultimately classify the text.\n",
    "\n",
    "Like the classifiers, the operators only have to be initialized once, and then can be recycled/reused for multiple texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b904618",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initialize operators\n",
    "#The machine learning operator\n",
    "tabby_ML = bibcat.Operator(classifier=classifier_ML,\n",
    "                        mode=mode, keyword_objs=all_keyobjs, do_verbose=False)\n",
    "\n",
    "#The rule-based operator\n",
    "tabby_rules = bibcat.Operator(classifier=classifier_rules,\n",
    "                        mode=mode, keyword_objs=all_keyobjs,\n",
    "                        do_verbose=False, do_verbose_deep=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5248f",
   "metadata": {},
   "source": [
    "Now, we can use our operators to accept and classify some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4489d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run the operators\n",
    "#The machine learning operator\n",
    "results_ML = tabby_ML.classify(text=text,\n",
    "                                    lookup=lookup, buffer=0,\n",
    "                                    threshold=thres_ML,\n",
    "                                    do_raise_innererror=False,\n",
    "                                    do_check_truematch=True)\n",
    "\n",
    "#The rule-based operator\n",
    "results_rules = tabby_rules.classify(text=text,\n",
    "                                    lookup=lookup, buffer=0,\n",
    "                                    threshold=thres_rules,\n",
    "                                    do_raise_innererror=False,\n",
    "                                    do_check_truematch=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf71bb1",
   "metadata": {},
   "source": [
    "Let's print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Print the results\n",
    "#Print the text and classes as a reminder\n",
    "print(\"Text:\\n\\\"\\n{0}\\n\\\"\\n\".format(text))\n",
    "print(\"Lookup: {0}\\n\".format(lookup))\n",
    "\n",
    "#The machine learning results\n",
    "print(\"> Machine learning results:\")\n",
    "print(\"Paragraph:\\n\\\"\\n{0}\\n\\\"\".format(results_ML[\"modif\"]))\n",
    "print(\"Verdict: {0}\".format(results_ML[\"verdict\"]))\n",
    "print(\"Probabilities: {0}\".format(results_ML[\"uncertainty\"]))\n",
    "print(\"-\\n\")\n",
    "\n",
    "#The rule-based results\n",
    "print(\"> Rule-based results:\")\n",
    "print(\"Paragraph:\\n\\\"\\n{0}\\n\\\"\".format(results_rules[\"modif\"]))\n",
    "print(\"Verdict: {0}\".format(results_rules[\"verdict\"]))\n",
    "print(\"Probabilities: {0}\".format(results_rules[\"uncertainty\"]))\n",
    "print(\"-\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f06c8",
   "metadata": {},
   "source": [
    "And with that, we're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442791c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e508517",
   "metadata": {},
   "source": [
    "## Full Workflow: High-Level Methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1d940",
   "metadata": {},
   "source": [
    "The full workflow of bibcat, at a high level, is broken down into steps within the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e3088",
   "metadata": {},
   "source": [
    "![Graphic of the full high-level workflow of bibcat.](workflow_graphic.png \"Graphic of the full high-level workflow of bibcat.\")\n",
    "\n",
    "![Procedural steps describing the full high-level workflow of bibcat.](workflow_procedure.png \"Procedural steps describing the full high-level workflow of bibcat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43994554",
   "metadata": {},
   "source": [
    "Figure 1 (top panel) illustrates the full workflow of bibcat at a high level.  The user inputs and outputs are written in bold white and bold gold boxes, respectively.  Classes that are used by the user are colored in blue and outlined in bold.  Classes that are called internally within the code (and are not meant to ever be seen or touched by the user) are colored in purple and outlined with dashed lines.\n",
    "\n",
    "We describe each bibcat class, and their primary functions, within the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38fb49b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2100748b",
   "metadata": {},
   "source": [
    "## Full Workflow: Classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f13b26",
   "metadata": {},
   "source": [
    "### The _Base Class.\n",
    "\n",
    "The \\_Base class is *not* meant to be used by users.  It is a class that is purely meant to be inherited by other classes.  Essentially, \\_Base is a collection of methods that other classes often use.\n",
    "\n",
    "The primary (internal!) methods and use-cases of _Base include:\n",
    "* `_get_info`, `_store_info`: Store and retrieve information (values, booleans, etc.) for a given class instance.\n",
    "* `_assemble_keyword_wordchunks`: Build noun chunks containing target mission keywords from given text.\n",
    "* `_check_importance`: Check if some given text contains any important terms (where important terms includes mission keywords, 1st-person and 3rd-person pronouns, a paper citation, etc.).\n",
    "* `_check_truematch`: Check if some given ambiguous text relates to a given mission (e.g. Hubble observations), or is instead likely a false match (e.g. Edwin Hubble).\n",
    "* `_cleanse_text`: Cleanse some given text of, e.g., excessive whitespace and punctuation. Can also, e.g., replace citations with an 'Authoretal' placeholder of sorts.\n",
    "* `_extract_core_from_phrase`: Formulate a core representative 'meaning' for some given text.\n",
    "* `_is_pos_word`: Check if some given word (of the NLP type) has a particular part-of-speech.\n",
    "* `_process_database_ambig`: Load, process, and store external table of ambiguous mission-related phrases.\n",
    "* `_search_text`: Search some given text for mission keywords/acronyms (e.g., search for \"HST\").\n",
    "* `_streamline_phrase`: Run _cleanse_text(), and also streamline, e.g., websites by replacing them with uniform placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0888b",
   "metadata": {},
   "source": [
    "### The Keyword Class.\n",
    "\n",
    "The Keyword class is a user-friendly class that serves as a collection of 'terms' (i.e., titles, keywords, and/or acronyms) for a given mission.  \"Hubble Space Telescope\", \"Hubble\", and \"HST\", for example, are all terms that describe the Hubble Space Telescope (HST), and so would be included in a Keyword instance for Hubble.  These terms were used in the demo above, as reproduced below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7794f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate an example Keyword object for HST\n",
    "kobj_example = bibcat.Keyword(\n",
    "                keywords=[\"Hubble\", \"Hubble Telescope\",\n",
    "                          \"Hubble Space Telescope\"],\n",
    "                acronyms=[\"hst\", \"ht\"])\n",
    "\n",
    "#Print the Keyword instance\n",
    "print(kobj_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a741fd",
   "metadata": {},
   "source": [
    "The primary (internal!) methods and use-cases of Keyword include:\n",
    "* `get_name`: Return a representative name for this Keyword instance.\n",
    "* `is_keyword`: Return whether or not some given text contains terms that match to this Keyword instance.\n",
    "* `replace_keyword`: Replace occurrence of terms that match to this Keyword instance within the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6962ad1d",
   "metadata": {},
   "source": [
    "### The Paper Class.\n",
    "\n",
    "The Paper class is a user-friendly class; that being said, users never have to interact with the Paper class directly, as during the User Workflow all calls to the Paper class are handled internally within the codebase.  The main purpose of this class is to extract any and all sentences from within a larger block of text that refer to a given mission(s).  This collection of sentences for each mission, denoted from here on as a 'paragraph', is created to focus in on the portions of the original text that actually relate to the target mission.  Using paragraphs for classification, instead of the full text, allows us to remove much of the 'noise' inherent to the rest of the text.\n",
    "\n",
    "Here is a snippet illustrating how the Paper class is used internally to produce paragraphs for each mission (although again, users would never have to run such code themselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aebcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate an example Paper object for the text and for example Keyword instance\n",
    "buffer = 0 #+/- sentences to include within paragraph around each sentence with target terms\n",
    "paper_example = bibcat.Paper(text=text, keyword_objs=[kobj_hubble], do_check_truematch=True)\n",
    "paper_example.process_paragraphs(buffer=buffer, do_overwrite=False)\n",
    "paragraphs = paper_example.get_paragraphs()\n",
    "\n",
    "#Print what is stored within paragraphs\n",
    "print(\"Full printout of what is stored within paragraphs:\")\n",
    "print(paragraphs)\n",
    "\n",
    "#Print the paragraphs for the example Keyword instance\n",
    "print(\"Printout of paragraph stored for example Keyword instance:\")\n",
    "name = kobj_example.get_name() #Fetch the name of this Keyword instance\n",
    "print(paragraphs[name]) #Use name to access paragraph for this Keyword instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3a50e",
   "metadata": {},
   "source": [
    "The buffer parameter allows us to provide some context around the sentences with target terms. A buffer of # will include # sentence(s) from the original text before and after each target sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a36fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the Paper class again with a different buffer\n",
    "buffer = 1 #+/- sentences to include within paragraph around each sentence with target terms\n",
    "paper_example.process_paragraphs(buffer=buffer, do_overwrite=True) #Overwrite stored paragraphs\n",
    "paragraphs = paper_example.get_paragraphs()\n",
    "\n",
    "#Print the paragraphs for the example Keyword instance\n",
    "print(\"Printout of paragraph stored for example Keyword instance, now with buffer={0}:\"\n",
    "     .format(buffer))\n",
    "print(paragraphs[name]) #Use name to access paragraph for this Keyword instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947da187",
   "metadata": {},
   "source": [
    "The primary (internal!) methods and use-cases of Paper include:\n",
    "* `get_paragraphs`: Fetch stored paragraphs previously generated with `process_paragraphs`.\n",
    "* `process_paragraphs`: Collect all sentences containing mission terms into 'paragraphs'. Buffer each sentence by including +/- surrounding sentences if given buffer is nonzero.\n",
    "* `_buffer_indices`: Given a set of indices and a buffer value, return the index spans of the applied buffer.\n",
    "* `_extract_paragraph`: Perform the actual extraction of the paragraph from the text.\n",
    "* `_split_text`: Split the given text into sentences using naive sentence boundaries.\n",
    "* `_verify_acronyms`: Scan the entire text to find possible meanings of given acronyms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e344595",
   "metadata": {},
   "source": [
    "### The Grammar Class.\n",
    "\n",
    "The Grammar class is a user-friendly class on the surface; however, users are not meant to interact with Grammar directly, as much of its content is extremely technical. In a nutshell, the main purpose of the Grammar class is to:\n",
    "1) Break a paragraph (generated by the Paper class) down into its grammatical components (e.g., parts-of-speech, flagging verbs, the hierarchical structure of each sentence, etc).\n",
    "2) Streamline, simplify, and/or anonymize the contents of the paragraph, based on the user's specifications.\n",
    "\n",
    "Essentially, the Grammar class is meant to modify a given paragraph, so as to remove any extra or unnecessary information from within each sentence in that paragraph.  These modifications are meant to further reduce the 'noise' that, e.g., the machine learning classifier must deal with later on in order to classify a given paragraph.  For clarity, from here on we refer to these modified paragraphs as 'modifs'.\n",
    "\n",
    "There are different modes for modifying a given paragraph (thus producing different modifs), which will do none or some combination of the following operations:\n",
    "* `skim`: Remove useless extra words (e.g., adjectives) that are likely not important for readability.\n",
    "* `trim`: Remove clauses and sentence snippets that are likely not relevant to the mission and are not important for readability.\n",
    "* `anon`: Replace any mission terms with a generic placeholder.\n",
    "\n",
    "Here is a snippet demonstrating these different modes of modification and producing example modifs for our example paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68112aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate an example Grammar object for the text and for example Keyword instance\n",
    "grammar_example = bibcat.Grammar(text=text, keyword_obj=kobj_hubble,\n",
    "                                 do_check_truematch=True, buffer=0\n",
    "                                ) #NOTE: buffer=0 required for mode=trim\n",
    "\n",
    "#Run all allowed modifications of paragraph\n",
    "grammar_example.run_modifications(which_modes=[\"none\", \"skim\", \"trim\", \"anon\", \"skim_trim_anon\"])\n",
    "\n",
    "#Fetch modified paragraphs ('modifs') for various modes\n",
    "#\n",
    "#For none\n",
    "try_modif = grammar_example.get_modifs(which_modes=[\"none\"]) #Order/case do not matter\n",
    "print(\"none modif: (i.e., no modifications made)\")\n",
    "print(try_modif)\n",
    "print(\"\")\n",
    "#\n",
    "#For skim\n",
    "try_modif = grammar_example.get_modifs(which_modes=[\"skim\"]) #Order/case do not matter\n",
    "print(\"skim modif:\")\n",
    "print(try_modif)\n",
    "print(\"\")\n",
    "#\n",
    "#For trim\n",
    "try_modif = grammar_example.get_modifs(which_modes=[\"trim\"]) #Order/case do not matter\n",
    "print(\"trim modif:\")\n",
    "print(try_modif)\n",
    "print(\"\")\n",
    "#\n",
    "#For anon\n",
    "try_modif = grammar_example.get_modifs(which_modes=[\"anon\"]) #Order/case do not matter\n",
    "print(\"anon modif:\")\n",
    "print(try_modif)\n",
    "print(\"\")\n",
    "#\n",
    "#For skim+trim+anon\n",
    "try_modif = grammar_example.get_modifs(which_modes=[\"skim_trim_anon\"]) #Order/case do not matter\n",
    "print(\"skim_trim_anon modif:\")\n",
    "print(try_modif)\n",
    "print(\"\")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e1581",
   "metadata": {},
   "source": [
    "The primary (internal!) methods and use-cases of Grammar include the following (but again, note that the large majority of the Grammar class is extremely technical, and not recommended for the user to mess with):\n",
    "* `get_modifs`: Fetch stored modifs previously generated with `run_modifications`.\n",
    "* `run_modifications`: Operates modification of 'paragraphs' according to user-requested modes.\n",
    "* `_add_aux, _add_verb, _add_word`: Given a set of indices and a buffer value, return the index spans of the applied buffer.\n",
    "* `_get_wordchunk`: Fetch the full wordchunk of a given word.\n",
    "* `_modify_structure`: Perform actual modification of a given paragraph using a given mode.\n",
    "* `_recurse_NLP_categorization`: Recurse through the grammar hierarchy of a sentence and categorize+store information (e.g., part-of-speech) for each word within that sentence.\n",
    "* `_run_NLP`: Run an external natural language processing (NLP) package on given text.\n",
    "* `_set_wordchunks`: Assign each word in a sentence to a word chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235137f",
   "metadata": {},
   "source": [
    "### The _Classifier Class.\n",
    "\n",
    "Similarly to the \\_Base class, the \\_Classifier class is *not* meant to be used by users.  It is a class purely meant to be inherited by the various Classifier\\_* classes.  In short, the \\_Classifier class is a collection of methods that the different classifier types often use.\n",
    "\n",
    "The primary (internal!) methods and use-cases of \\_Classifier include:\n",
    "* `classify_text`: Base classification method, overwritten by various classifier types during inheritance.\n",
    "* `_load_text`: Load text from a given filepath.\n",
    "* `_process_text`: Use the Grammar class (and internally the Paper class) to process given text into modifs.\n",
    "* `_write_text`: Write a given text file to a given filepath."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80814030",
   "metadata": {},
   "source": [
    "### The Classifier_* Classes.\n",
    "\n",
    "The Classifier\\_* classes are the user-friendly classifier types implemented in bibcat.  They each support a different type of classification of a given block of text.  Currently there are two types implemented in bibcat:\n",
    "* `Classifier_ML`: Classification using a previously trained machine learning (ML) model.\n",
    "* `Classifier_Rules`: Classification using an internal decision tree.\n",
    "\n",
    "While the internal workings of the Classifier\\_* classes are different, ultimately all Classifier\\_* offer the same method for classification:\n",
    "* `classify_text`: Classify given text using the classification type specific to this Classifier\\_* instance.\n",
    "\n",
    "Although the initialization of the Classifier\\_* classes is user-friendly, the inner workings of these classes are quite technical, are managed completely internally, and are not meant to be operated directly by users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252aef6",
   "metadata": {},
   "source": [
    "### The Operator Class.\n",
    "\n",
    "Last but not least, the Operator class is an extremely user-friendly class that is meant to be run by users. The primary purpose of this class is to direct and run the entire workflow of bibcat, from reading in a given block of text to ultimately classifying that block of text.  Using its `classify` method, the Operator class internally handles all calls to the other classes (Paper, Grammar, and the given classifier) so far discussed.  Indeed, the `classify` method was used in the very first demo of this write-up, in order to classify the example text.\n",
    "\n",
    "Formally, the primary methods and use-cases of Operator are:\n",
    "* `_fetch_keyword_object`: A hidden method for fetching the stored Keyword instance that matches a given term.\n",
    "* `classify`: A method designed for users that prepares and runs the entire bibcat workflow, from input raw text to classified output.\n",
    "* `process`: A method designed for users that processes given text into modifs, from input raw text to output modifs. Does not include classification (for that, run `classify`); useful for preprocessing raw text.\n",
    "* `train_model_ML`: A method designed for users that trains a machine learning (ML) model on input raw text. Under the hood, this calls `process` to carry out the preprocessing of the raw text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d39ae5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f52dd",
   "metadata": {},
   "source": [
    "### That's all to start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50e47a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set end marker for this tutorial.\n",
    "print(\"This tutorial completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
