{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71ff86",
   "metadata": {},
   "source": [
    "# Bibliography Categorization: 'BibCat'\n",
    "## Tutorial: Machine learning (ML) models in bibcat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ab7e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234a479",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction.\n",
    "\n",
    "In this tutorial, we will use bibcat to train a machine learning (ML) model on some raw input text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f050727",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86289680",
   "metadata": {},
   "source": [
    "## User Workflow: Training a machine learning (ML) model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af236d7a",
   "metadata": {},
   "source": [
    "The `Operator` class contains a user-friendly method `train_model_ML` that runs the full workflow for training an ML model, from the input raw text all the way to saving the output ML model.  We overview how this method can be run in the code blocks below.\n",
    "\n",
    "For this tutorial, we have two sets of data: either 1) some short, made-up text for a quick run of the code, or 2) an imported database of text from an external file of the user's choosing. The former case is useful for getting a quick sense of how the code works. The latter case is useful for building an actual model, but of course will take much longer on larger databases of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e1149ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdea077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import external packages\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f14b847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/docs\n",
      "Source directory: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src\n"
     ]
    }
   ],
   "source": [
    "# Set up for fetching necessary bibcat modules for the tutorial\n",
    "# Check work directories: src/ is where all source python scripts are available. \n",
    "current_dir= os.path.dirname(os.path.abspath('__file__'))\n",
    "_parent = os.path.dirname(current_dir)\n",
    "src_dir = os.path.join(_parent, \"src\")\n",
    "\n",
    "print(f'Current Directory: {current_dir}')\n",
    "print(f'Source directory: {src_dir}')\n",
    "\n",
    "# move to the ../src/ directory to import necessary modules. \n",
    "os.chdir(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8643e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory =/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src, parent directory=/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat\n",
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models folder already exists.\n",
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/output folder already exists.\n"
     ]
    }
   ],
   "source": [
    "#Import bibcat packages\n",
    "import bibcat_classes as bibcat\n",
    "import bibcat_config as config\n",
    "import bibcat_parameters as params #Temporary file until contents moved elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ef4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which data would you like to run the ML model on?  Choose from the booleans below.\n",
    "do_quick_run = False #This will train the ML model on short bits of text. Runs pretty quickly.\n",
    "do_real_run = True #This will train the ML model on external text. Will take longer for larger databases.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0ced33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The rest of these parameters can be left as-is for a first run-through.\n",
    "#\n",
    "do_check_truematch = True #A very important parameter - discuss with J.P. first!!!  Set it to either True or False.\n",
    "#If any papers in dataset encountered within the codebase that have unknown ambiguous phrases...\n",
    "#...then a note will be printed and those papers will not be used for training-validation-testing.\n",
    "#...Add the identified ambiguous phrase to the external ambiguous phrase database and rerun to include those papers.\n",
    "#\n",
    "num_papers = 500 #None #500 #None, or an integer; if an integer, will truncate external .json text dataset to this size\n",
    "#Set num_papers=None to use all available papers in external dataset\n",
    "#Note: If set to integer, final paper count might be a little more than target num_papers given\n",
    "#\n",
    "allowed_classifications = params.allowed_classifications #For external data; classifications to include\n",
    "mapper = params.map_papertypes #For masking of classes (e.g., masking 'supermention' as 'mention')\n",
    "#\n",
    "\n",
    "#Fetch filepaths for model and data\n",
    "name_model = config.name_model\n",
    "filepath_json = config.path_json\n",
    "dir_model = os.path.join(config.dir_allmodels, name_model)\n",
    "filesave_error = os.path.join(dir_model,\n",
    "                              \"{0}_processing_errors.txt\".format(name_model)) #Where to save processing errors\n",
    "filesave_unused_bibcodes = os.path.join(dir_model,\n",
    "                              \"{0}_bibcodes_unused_during_trainML.npy\".format(name_model)) #Where to save processing errors\n",
    "#\n",
    "#Set values for generating ML model\n",
    "do_reuse_run = True #Whether or not to reuse any existing output from previous training+validation+testing runs\n",
    "do_shuffle = True #Whether or not to shuffle contents of training vs validation vs testing datasets\n",
    "fraction_TVT = [0.8, 0.1, 0.1] #Fractional breakdown of training vs validation vs testing datasets\n",
    "#\n",
    "mode_TVT = \"uniform\" # \"uniform\" #\"available\"\n",
    "#\"uniform\" = all training datasets will have the same number of entries\n",
    "#\"available\" = all training datasets will use full fraction (from fraction_TVT) of data available\n",
    "#\n",
    "seed_TVT = 10 #Random seed for generating training vs validation vs testing datasets\n",
    "seed_ML = 8 #Random seed for ML model\n",
    "mode_modif = \"none\" #\"skim_anon\" #\"skim_trim_anon\" #Mode to use for processing and generating modifs from input raw text\n",
    "#NOTE: See other modif modes in workflow tutorial\n",
    "buffer = 0\n",
    "#\n",
    "all_kobjs = params.all_kobjs\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97faa2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize an empty ML classifier\n",
    "classifier_ML = bibcat.Classifier_ML(filepath_model=None, fileloc_ML=None, do_verbose=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0f528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance of Operator successfully initialized!\n",
      "Keyword objects:\n",
      "0: Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['HST', 'HT']\n",
      "Banned Overlap: ['Hubble Legacy Archive']\n",
      "\n",
      "1: Keyword Object:\n",
      "Name: Webb Telescope\n",
      "Keywords: ['James Webb Space Telescope', 'Webb Space Telescope', 'James Webb Telescope', 'Webb Telescope']\n",
      "Acronyms: ['JWST', 'JST', 'JT']\n",
      "Banned Overlap: []\n",
      "\n",
      "2: Keyword Object:\n",
      "Name: Transiting Exoplanet Survey Satellite\n",
      "Keywords: ['Transiting Exoplanet Survey Satellite']\n",
      "Acronyms: ['TESS']\n",
      "Banned Overlap: []\n",
      "\n",
      "3: Keyword Object:\n",
      "Name: Kepler\n",
      "Keywords: ['Kepler']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "4: Keyword Object:\n",
      "Name: Pan-STARRS\n",
      "Keywords: ['Panoramic Survey Telescope and Rapid Response System', 'Pan-STARRS1', 'Pan-STARRS']\n",
      "Acronyms: ['PanSTARRS1', 'PanSTARRS', 'PS1']\n",
      "Banned Overlap: []\n",
      "\n",
      "5: Keyword Object:\n",
      "Name: Galaxy Evolution Explorer\n",
      "Keywords: ['Galaxy Evolution Explorer']\n",
      "Acronyms: ['GALEX']\n",
      "Banned Overlap: []\n",
      "\n",
      "6: Keyword Object:\n",
      "Name: K2\n",
      "Keywords: ['K2']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "7: Keyword Object:\n",
      "Name: Hubble Legacy Archive\n",
      "Keywords: ['Hubble Legacy Archive']\n",
      "Acronyms: ['HLA']\n",
      "Banned Overlap: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize an Operator\n",
    "tabby_ML = bibcat.Operator(classifier=classifier_ML, mode=mode_modif, keyword_objs=all_kobjs,\n",
    "                           do_verbose=True, load_check_truematch=do_check_truematch, do_verbose_deep=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dac2f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up data for the quick example case. But in reality, ML models should be trained on MUCH more data than this!!!\n",
    "if do_quick_run:\n",
    "    #Make some fake data\n",
    "    dict_texts_raw = {\"science\":[\"We present HST observations in Figure 4.\",\n",
    "                            \"The HST stars are listed in Table 3b.\",\n",
    "                            \"Despite our efforts to smooth the data, there are still rings in the HST images.\",\n",
    "                            \"See Section 8c for more discussion of the Hubble images.\",\n",
    "                            \"The supernovae detected with HST tend to be brighter than initially predicted.\",\n",
    "                            \"Our spectra from HST fit well to the standard trend first published in Someone et al. 1990.\",\n",
    "                            \"We use the Hubble Space Telescope to build an ultraviolet database of the target stars.\",\n",
    "                            \"The blue points (HST) exhibit more scatter than the red points (JWST).\",\n",
    "                            \"The benefit, then, is the far higher S/N we achieved in our HST observations.\",\n",
    "                            \"Here we employ the Hubble Telescope to observe the edge of the photon-dominated region.\",\n",
    "                            \"The black line shows that the region targeted with Hubble has an extreme UV signature.\"],\n",
    "                     \"datainfluenced\":[\"The simulated Hubble data is plotted in Figure 4.\",\n",
    "                           \"Compared to the HST observations in Someone et al., our JWST follow-up reached higher S/N.\",\n",
    "                           \"We were able to reproduce the luminosities from Hubble using our latest models.\",\n",
    "                           \"We overplot Hubble-observed stars from Someone et al. in Figure 3b.\",\n",
    "                           \"We built the spectral templates using UV data in the Hubble archive.\",\n",
    "                           \"We simulate what our future HST observations will look like to predict the S/N.\",\n",
    "                           \"Our work here with JWST is inspired by our earlier HST study published in 2010.\",\n",
    "                           \"We therefore use the Hubble statistics from Author et al. to guide our stellar predictions.\",\n",
    "                           \"The stars in Figure 3 were plotted based on the HST-fitted trend line in Person et al.\",\n",
    "                           \"The final step is to use the HST exposure tool to put our modeled images in context.\"],\n",
    "                     \"mention\":[\"Person et al. used HST to measure the Hubble constant.\",\n",
    "                            \"We will present new HST observations in a future work.\",\n",
    "                            \"HST is do_a fantastic instrument that has revolutionized our view of space.\",\n",
    "                            \"The Hubble Space Telescope (HST) has its mission center at the STScI.\",\n",
    "                            \"We can use HST to power a variety of science in the ultraviolet regime.\",\n",
    "                            \"It is not clear when the star will be observable with HST.\",\n",
    "                            \"More data can be found and downloaded from the Hubble archive.\",\n",
    "                            \"We note that HST can be used to observe the stars as well, at higher S/N.\",\n",
    "                            \"However, we ended up using the JWST rather than HST observations in this work.\",\n",
    "                            \"We push the analysis of the Hubble component of the dataset to a future study.\",\n",
    "                            \"We expect the HST observations to be released in the fall.\",\n",
    "                            \"We look forward to any follow-up studies with, e.g., the Hubble Telescope.\"]}\n",
    "    #\n",
    "    #Convert into dictionary with: key:text,class,id,mission structure\n",
    "    i_track = 0\n",
    "    dict_texts = {}\n",
    "    for key in dict_texts_raw:\n",
    "        curr_set = dict_texts_raw[key]\n",
    "        for ii in range(0, len(curr_set)):\n",
    "            dict_texts[str(i_track)] = {\"text\":curr_set[ii], \"class\":key, \"id\":\"{0}_{1}\".format(key, ii),\n",
    "                                       \"mission\":\"HST\", \"bibcode\":\"{0}_{1}\".format(key, ii)}\n",
    "            i_track += 1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d25644b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60157"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set up data for the external data case.\n",
    "if do_real_run:\n",
    "    #Load the original data\n",
    "    with open(filepath_json, 'r') as openfile:\n",
    "        dataset = json.load(openfile)\n",
    "        len(dataset)\n",
    "    #\n",
    "    #Initialize holder to keep track of bibcodes used (avoids duplicate dataset entries)\n",
    "    list_bibcodes = []\n",
    "    dict_unused_indsandbibcodes = {} #Dictionary preserves uniqueness of unused bibcodes\n",
    "    #\n",
    "    #Organize a new version of the data with: key:text,class,id,mission structure\n",
    "    i_track = 0 #Track number of papers kept from original dataset\n",
    "    dict_texts = {}\n",
    "    for ii in range(0, len(dataset)):\n",
    "        #Extract mission classifications for current text\n",
    "        curr_data = dataset[ii]\n",
    "        curr_bibcode = curr_data[\"bibcode\"]\n",
    "        #\n",
    "        #Skip if no valid text at all for this text\n",
    "        if (\"body\" not in curr_data):\n",
    "            continue\n",
    "        #\n",
    "        #Skip if no valid missions at all for this text\n",
    "        if (\"class_missions\" not in curr_data):\n",
    "            dict_unused_indsandbibcodes[curr_bibcode] = ii\n",
    "            continue\n",
    "        #\n",
    "        #Otherwise, extract the missions\n",
    "        curr_missions = curr_data[\"class_missions\"]\n",
    "        #print(curr_bibcode)\n",
    "        #print(curr_missions)\n",
    "        #\n",
    "        #Skip if bibcode already encountered (and so duplicate entry)\n",
    "        if (curr_bibcode in list_bibcodes):\n",
    "            print(\"Duplicate bibcode encountered: {0}. Skipping.\".format(curr_bibcode))\n",
    "            continue\n",
    "        #\n",
    "        #Iterate through missions for this text\n",
    "        i_mission = 0\n",
    "        for curr_key in curr_missions:\n",
    "            #If this is not an allowed classification, skip\n",
    "            if (curr_missions[curr_key][\"papertype\"] not in allowed_classifications):\n",
    "                dict_unused_indsandbibcodes[curr_bibcode] = ii\n",
    "                continue\n",
    "            #\n",
    "            #Otherwise, check if this mission is a target mission\n",
    "            fetched_kobj = tabby_ML._fetch_keyword_object(lookup=curr_key,\n",
    "                                                          do_verbose=False, do_raise_emptyerror=False)\n",
    "            #Skip if not a target mission\n",
    "            if (fetched_kobj is None):\n",
    "                dict_unused_indsandbibcodes[curr_bibcode] = ii\n",
    "                continue\n",
    "            #\n",
    "            #Otherwise, store classification info for this entry\n",
    "            curr_class = curr_missions[curr_key][\"papertype\"]\n",
    "            new_dict = {\"text\":curr_data[\"body\"], #Text for this paper\n",
    "                        \"bibcode\":curr_data[\"bibcode\"], #Bibcode for this paper\n",
    "                        \"class\":curr_class, #Classification for this mission\n",
    "                        \"mission\":curr_key, #The mission itself\n",
    "                        \"id\":(\"paper{0}_mission{1}_{2}_{3}\".format(ii, i_mission,\n",
    "                                                                   curr_key, curr_class)) #ID for this entry\n",
    "                       }\n",
    "            dict_texts[str(i_track)] = new_dict\n",
    "            #\n",
    "            #Increment counters\n",
    "            i_mission += 1 #Count of kept missions for this paper\n",
    "            i_track += 1 #Count of kept classifications overall\n",
    "        #\n",
    "        \n",
    "        #Record this bibcode as stored\n",
    "        list_bibcodes.append(curr_bibcode)\n",
    "\n",
    "        #Terminate early if requested number of papers reached\n",
    "        if ((num_papers is not None) and (i_track >= num_papers)):\n",
    "            #Store the remaining bibcodes as unused\n",
    "            dict_unused_indsandbibcodes.update({dataset[jj][\"bibcode\"]:jj\n",
    "                                             for jj in range((ii+1), len(dataset))})\n",
    "            break\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af23be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throw error if not enough text entries collected\n",
    "if do_real_run:\n",
    "    if ((num_papers is not None) and (len(dict_texts) < num_papers)):\n",
    "        raise ValueError(\"Err: Something went wrong during initial processing. Insufficient number of texts extracted.\"\n",
    "                        +\"\\nRequested number of texts: {0}\\nActual number of texts: {1}\"\n",
    "                        .format(num_papers, len(dict_texts)))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d25b4cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Number of processed texts: {0}={1}\\n\".format(i_track, len(dict_texts)))\\nfor curr_key in dict_texts:\\n    print(\"Text #{0}:\".format(curr_key))\\n    print(\"Classification: {0}\".format(dict_texts[curr_key][\"class\"]))\\n    print(\"Mission: {0}\".format(dict_texts[curr_key][\"mission\"]))\\n    print(\"ID: {0}\".format(dict_texts[curr_key][\"id\"]))\\n    print(\"Bibcode: {0}\".format(dict_texts[curr_key][\"bibcode\"]))\\n    print(\"Text snippet:\")\\n    print(dict_texts[curr_key][\"text\"][0:500])\\n    print(\"---\\n\\n\")\\n#'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uncomment the code below to print a snippet of each of the entries in the dataset.\n",
    "\"\"\"\n",
    "print(\"Number of processed texts: {0}={1}\\n\".format(i_track, len(dict_texts)))\n",
    "for curr_key in dict_texts:\n",
    "    print(\"Text #{0}:\".format(curr_key))\n",
    "    print(\"Classification: {0}\".format(dict_texts[curr_key][\"class\"]))\n",
    "    print(\"Mission: {0}\".format(dict_texts[curr_key][\"mission\"]))\n",
    "    print(\"ID: {0}\".format(dict_texts[curr_key][\"id\"]))\n",
    "    print(\"Bibcode: {0}\".format(dict_texts[curr_key][\"bibcode\"]))\n",
    "    print(\"Text snippet:\")\n",
    "    print(dict_texts[curr_key][\"text\"][0:500])\n",
    "    print(\"---\\n\\n\")\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6182daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target missions:\n",
      "Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['HST', 'HT']\n",
      "Banned Overlap: ['Hubble Legacy Archive']\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Webb Telescope\n",
      "Keywords: ['James Webb Space Telescope', 'Webb Space Telescope', 'James Webb Telescope', 'Webb Telescope']\n",
      "Acronyms: ['JWST', 'JST', 'JT']\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Transiting Exoplanet Survey Satellite\n",
      "Keywords: ['Transiting Exoplanet Survey Satellite']\n",
      "Acronyms: ['TESS']\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Kepler\n",
      "Keywords: ['Kepler']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Pan-STARRS\n",
      "Keywords: ['Panoramic Survey Telescope and Rapid Response System', 'Pan-STARRS1', 'Pan-STARRS']\n",
      "Acronyms: ['PanSTARRS1', 'PanSTARRS', 'PS1']\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Galaxy Evolution Explorer\n",
      "Keywords: ['Galaxy Evolution Explorer']\n",
      "Acronyms: ['GALEX']\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: K2\n",
      "Keywords: ['K2']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Hubble Legacy Archive\n",
      "Keywords: ['Hubble Legacy Archive']\n",
      "Acronyms: ['HLA']\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "\n",
      "Number of valid text entries:\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#Print number of texts that fell under given parameters\n",
    "print(\"Target missions:\")\n",
    "for curr_kobj in all_kobjs:\n",
    "    print(curr_kobj)\n",
    "    print(\"\")\n",
    "print(\"\")\n",
    "print(\"Number of valid text entries:\")\n",
    "print(len(dict_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42320d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Running train_model_ML()!\n",
      "Processing text data into modifs...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper2_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJS..265....5H\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST Advanced Camera\\nTaken from this text snippet:\\nTo date, deep-field imaging observations have reached detection limits of ≃000 mag in the wavelength range of 000.000–000.000 μ m with the HST Advanced Camera for Surveys (ACS) and the Wide Field Camera 000 (WFC3) instruments in the Hubble Ultra Deep Field (Beckwithetal 000; see Bouwensetal 000 and references therein) with the moderately deep ultraviolet (UV) extension, UVUDF 000.000–000.000 μ m; Windhorstetal 000; Teplitzetal 000.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper16_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.4755A\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\n000 Hubble E\\nTaken from this text snippet:\\nW, Bouwens R, Oesch P, Smit R, Illingworth G, Labbe I, 000, ApJ, 000, 000 Hubble E, Humason M.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "25 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper26_mission0_HST_SCIENCE\n",
      "Bibcode: 2023AJ....165...13W\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST diffraction limit\\nTaken from this text snippet:\\nThe tendency of faint galaxies to bunch up against the HST diffraction limit at brighter flux levels was first suggested based on the Hubble Deep Field images by Authorsetal and Authorsetal, and later by Welchetal (2022c), Authorsetal, and references therein based on more recent HST images.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "50 of 500 total texts have been processed...\n",
      "75 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper70_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.519..157W\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST /ACS All - Wavelength Extended Groth Strip International Survey\\nTaken from this text snippet:\\nThe ACS mosaics used in this work were produced as part of the Complete Hubble Archive for Galaxy Evolution (CHArGE) project (Kokorev (in prep.)), and include observations obtained from the HST /ACS All-Wavelength Extended Groth Strip International Survey (AEGIS; Davisetal 000, the Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey (CANDELS; Groginetal 000; Koekemoeretal 000, and the Ultraviolet Imaging of the Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey Fields (UVCANDELS 000; PI Teplitz).')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper71_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023MNRAS.519.4632D\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nWe also show the yields expected for stellar sources assuming a maximum population age of 000 Myr (computed as the difference between the Hubble time at z = 000.000, the measured redshift of COS-000, and the Hubble time at z = 000, assumed to be the onset redshift of star formation).')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "100 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper87_mission0_HST_MENTION\n",
      "Bibcode: 2023AJ....165...91B\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST WFC3\\nTaken from this text snippet:\\nMost of the spectroscopic observations either during transit or secondary eclipse use space-based platforms like Hubble Space Telescope (HST) and Spitzer (and currently James Webb Space Telescope (JWST)) with low-to-moderate spectral resolution (R ∼ 000–000 or photometry-based instruments (primarily HST WFC3/Space Telescope Imaging Spectrograph (STIS) and Spitzer IRAC; Singetal 000; Guillotetal 000.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper92_mission0_HST_MENTION\n",
      "Bibcode: 2023ApJ...946...71C\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble sequence\\nTaken from this text snippet:\\nIntroduction The emergence of the Hubble sequence is one of the fundamental challenges of the hierarchical picture of galaxy assembly.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper105_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023ApJ...944...94T\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble parameter\\nTaken from this text snippet:\\nHence, a necessary (not sufficient) criterion a sample should satisfy is approximate constancy in the Hubble parameter for individual galaxies, H i = f i cz i / d i, averaged in velocity bins.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "125 of 500 total texts have been processed...\n",
      "150 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper128_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.1260S\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nprograms HST - GO-000\\nTaken from this text snippet:\\nWhile information about cycle 000 targets are listed in Authorsetal, a brief description of the main characteristics and respective discovery of the cycle 000 sample can be found below in Section 000.000. 000.000 Data and Data Reduction The observations of the lenses in our sample were taken by the Hubble Space Telescope under cycle 000 and cycle 000 programs HST-GO-000 and HST-GO-000 (PI: Treu), respectively, using the Wide Field Camera 000 (WFC3).')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper130_mission1_HST_SCIENCE\n",
      "Bibcode: 2023ApJ...944L..14W\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble compact clusters\\nTaken from this text snippet:\\nWhite circles show the clusters from the Hubble compact clusters catalog. “Historical” clusters are identified by the gold circles.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper134_mission0_HST_MENTION\n",
      "Bibcode: 2023MNRAS.518..305L\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble flow\\nTaken from this text snippet:\\nThis is assuming expansion purely due to the Hubble flow and H0 = 000 For a discussion on the differences among the various luminosity functions obtained with different techniques, see Authorsetal.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper148_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJ...943L..27F\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nSuch a high value is unlikely at high z because the time to grow such a Balmer break would be higher than the Hubble time (e.g, Maraston 000; Nolletal 000.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "175 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper152_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJ...943..110S\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nThe demise of most stars in the universe that evolve in a Hubble time (i.e, in the 000–000 M ⊙ range) is believed to occur as a result of heavy mass loss (with rates up to 000 −000 M ⊙ yr −000 on the Asymptotic Giant Branch (AGB), when the stars are very luminous (L ∼ 000–000, 000 L ⊙) and cool (T eff 000 K) (see, e.g, the review by Decin 000.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper179_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.2123Z\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nprogram number HST - HFnumeric - A\\nTaken from this text snippet:\\nFinally, PB was also partially supported through program number HST-HFnumeric-A, provided by NASA through a Hubble Fellowship grant from the Space Telescope Science Institute, under NASA contract NASnumeric.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "225 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper200_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023MNRAS.518..456D\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble ’s constant\\nTaken from this text snippet:\\nIn total, 000 snapshots spaced in logarithmic intervals of growth factor from redshift z = 000 to z = 000 were produced using the Authorsetal ΛCDM cosmology with cosmological parameters: H0 = h × 100Mpc kms−000, h = 000.000; Ωm = 000.000, Ωb = 000.000 and ΩΛ = 000.000 being the Hubble’s constant, matter density, baryon density and Λ density respectively.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper202_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJS..264...40M\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST WFC3 camera\\nTaken from this text snippet:\\nThe UVIS channel of the HST WFC3 camera has provided superior NUV images of several deep fields (e.g, the Hubble Ultraviolet Ultra Deep Field; Teplitzetal 000; the Hubble Deep UV Legacy Survey; Oeschetal 000; and the recent UVCANDELS survey; PI, H.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "250 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper216_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.519.3749C\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST GO-000\\nTaken from this text snippet:\\nOngoing and future studies that explore these extremes, for example from the Hubble imaging Probe of Extreme Environments and Clusters (HiPEEC; Adamoetal 000 and the Clusters, Clumps, Dust, and Gas (CCDG; HST GO-000; PI: Chandar) projects, combined with the larger sample of spiral galaxies in the LEGUS (Calzettietal 000; Adamoetal, in preparation) and Physics at High Angular Resolution in Nearby Galaxies (PHANGS)- HST surveys, are needed to cover the full range of Σ SFR found in the nearby Universe.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper220_mission0_HST_SCIENCE\n",
      "Bibcode: 2023NewA...9901962J\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nhereafter HST\\nTaken from this text snippet:\\nThe ultraviolet spectra were acquired by STIS and GHRS spectrographs onboard Hubble Space Telescope (hereafter HST).')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "275 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper232_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023MNRAS.518.5953L\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble sequence\\nTaken from this text snippet:\\nThe most famous morphological classification scheme for galaxies is the Hubble sequence.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper242_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.5123M\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nGravitational wave radiation drives the loss of orbital angular momentum and causes many binary orbits to decay within a Hubble time, driving them to compact (orbital period of ≈000–000 min) configurations.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper245_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023MNRAS.521..662N\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST)/WFC3\\nTaken from this text snippet:\\n000 INTRODUCTION Only one spectroscopically confirmed very high redshift (z ≥ 000 galaxy is known to date, dubbed GN-z11, at redshift, when the age of the Universe was ≃000 website-z11 has been identified as a bright (M uv ≈ −000 mag), massive (stellar mass M * ≃ 000 000 M ⊙) Lyman break galaxy (LBG) in observations with the Hubble Space Telescope (HST)/WFC3/infrared and Spitzer /IRAC instruments.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper249_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.4579P\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nMoreover, the extremely low mass for the white dwarf 000.000 ± 000.000 M) must be the result of binary interaction, since a single star could not produce such a low mass white dwarf within a Hubble time, making it unlikely that the white dwarf is a distant companion to a binary.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper249_mission1_KEPLER_MENTION\n",
      "Bibcode: 2023MNRAS.518.4579P\n",
      "Mission: KEPLER\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nKepler ’s third law\\nTaken from this text snippet:\\nWith the masses of both stellar components in the binary as well as the orbital fits we can now determine the semi-major axis, a, using Kepler’s third law: G (MWD + MSG)Porb, 000 4π 000 where G is the gravitational constant.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper257_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJ...943...38S\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\n∼a Hubble time\\nTaken from this text snippet:\\nIf the duty cycle of quasar activity is independent of pair evolution, we expect to see a dramatic pileup of quasar pairs near the stall distance because these pairs spend a much longer time there (i.e, ∼a Hubble time) compared to their lifetime during the previous galactic inspiral.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "300 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper261_mission0_HST_SCIENCE\n",
      "Bibcode: 2023AJ....165...23T\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble HST\\nTaken from this text snippet:\\nHubble HST observed a single transit of Knumeric using the Wide Field Camera 000 (WFC3) on 000 May 000 (ID: 000; PI: B.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper262_mission0_HST_MENTION\n",
      "Bibcode: 2023MNRAS.518.2177G\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nAitken - Jeans - Lundmark - Hubble galaxy sequence\\nTaken from this text snippet:\\nThis notation is confined to high-surface brightness galaxies that define the galaxy classification grid seen in Authorsetal and built on the Aitken-Jeans-Lundmark-Hubble galaxy sequence discussed there.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper266_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.5893K\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nDATA ANALYSIS HST\\nTaken from this text snippet:\\nPart of its diffraction pattern is included in the displayed image. galaxies in the Local Volume are described in Section 000, and our conclusions are given in Section 000. 000 000.000 OBSERVATIONS AND DATA ANALYSIS HST The dwarf galaxy HIPASS J000–000 was observed with the Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS) on 000 July 000 for 000 seconds in each of the I- and V-bands as a part of the \"Every Known Nearby Galaxy\" survey (SNAP-000, PI R.B.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper272_mission0_HST_MENTION\n",
      "Bibcode: 2023MNRAS.519.3118W\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nkey Hubble\\nTaken from this text snippet:\\nThe observed SED of a star-forming galaxy at z = 000 and z = 000 alongside key Hubble, Spitzer, JWST /NIRCam, and JWST /MIRI filter transmission functions.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper274_mission0_HST_MENTION\n",
      "Bibcode: 2023MNRAS.518.2794C\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble Atlas\\nTaken from this text snippet:\\nE,etal, 000, MNRAS, 000, 000 Pozzetti L,etal, 000, A&A, 000, A000 Sandage A, 000, The Hubble Atlas of Galaxies Schuldt S, Suyu S.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper277_mission0_HST_MENTION\n",
      "Bibcode: 2023MNRAS.518.1427S\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nThis implies that for a satellite to merge within a Hubble time (the age of the Universe) the halo mass ratio between the host and a satellite must be closer than about 000: 000.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "325 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper298_mission0_HST_MENTION\n",
      "Bibcode: 2023ApJS..264....1T\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST - STIS\\nTaken from this text snippet:\\nThese have been extensively observed at high angular resolutions (∼000.′′000 at optical ([O i] 000/000 Å, [S ii] 000/000 Å, and [N ii] 000 Å—Dougadosetal 000; Woitasetal 000; Lopez-Martinetal 000; Coffeyetal 000 and NIR wavelengths ([Fe ii] and H 000 mainly, with the brightest lines at 000.000 μ m and 000.000 μ m, respectively—Pyoetal 000; Becketal 000; Hartigan Hillenbrand 000; Takamietal 000, using the Space Telescope Imaging Spectrograph (STIS) on board the Hubble Space Telescope (HST), STIS2 000 000 The detector for HST-STIS. and the Optically Adaptive System for Imaging Spectroscopy (OASIS) on the Canada–France–Hawaii Telescope (CFHT) with the PUE’O AO system, the Infrared Camera and Spectrograph (IRCS) on Subaru, the Near-Infrared Integral Field Spectrometer (NIFS) on the Gemini North telescope, and the Near-Infrared Spectrograph on the W.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper299_mission0_HST_MENTION\n",
      "Bibcode: 2023ApJ...944....5B\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST)/FUV flare\\nTaken from this text snippet:\\nThe unusual Hubble Space Telescope (HST)/FUV flare recently noted by Authorsetal had an even shorter duration, at less than 000 s.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper302_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023MNRAS.518.3935W\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble optical\\nTaken from this text snippet:\\nPrior to JWST however, it was possible, by combining ground based or Hubble optical and near-IR observations with Spitzer, to constrain the rest-frame UV - optical emission of z> 000 galaxies.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper303_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.519.1381W\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nCummingsetal 000, the same as used for the BASE-000 calculations), such a low white dwarf mass would yield a progenitor that should remain on the main sequence over a Hubble time, and is thus unphysical, instead requiring truncated (binary) stellar evolution.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "350 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper319_mission0_KEPLER_MENTION\n",
      "Bibcode: 2023MNRAS.518.2068G\n",
      "Mission: KEPLER\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nmission Kepler\\nTaken from this text snippet:\\nThe mission Kepler detected transiting planets mainly on close orbits also with Earth-like mass.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "375 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper339_mission0_KEPLER_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.3482K\n",
      "Mission: KEPLER\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nexomoon candidate Kepler\\nTaken from this text snippet:\\nExtensive searches for additional moon-like transits have been attempted (e.g. see Kippingetal 000 and subsequent papers in that series) but only one other Kepler candidate has been reported to exhibit such a signature, the exomoon candidate Kepler 000-i.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper348_mission0_KEPLER_SCIENCE\n",
      "Bibcode: 2023MNRAS.518..642J\n",
      "Mission: KEPLER\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nKepler identification number\\nTaken from this text snippet:\\nThe table shows the number of the campaign in which the star was observed, the Kepler identification number (EPIC ID), the positions of the stars (right ascension, RA, and declination, DEC), the pulsational period (P) in days that were available in the The International Variable Star Index (VSX6), the average Kepler magnitude of the light curve, the 000 website-and-data/k2, //website 000 website 000 website 000 website?page=main 000 website https: MNRAS000, 000–000 000 T2Cs and ACs in K2 000.000 000 Analysis using Fourier decomposition We performed the Fourier analysis with our own python code using the Lomb-Scargle (LS) method to calculate the Fourier spectrum.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper348_mission1_K2_SCIENCE\n",
      "Bibcode: 2023MNRAS.518..642J\n",
      "Mission: K2\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nK2 mission2\\nTaken from this text snippet:\\nFinally, Section 000 contains the summary of our results. 000 000.000 THE KEPLER - K2 DATASET Target selection The Kepler space telescope observed thousands of pre-selected targets during each campaign of the K2 mission2, including several Cepheids.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "400 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper359_mission1_KEPLER_SCIENCE\n",
      "Bibcode: 2023MNRAS.518..669D\n",
      "Mission: KEPLER\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nKepler ’s observations\\nTaken from this text snippet:\\nKey words: methods: planets and satellites: terrestrial planets – planets and satellites: general – techniques: photometric – instrumentation: high angular resolution – data analysis 000 INTRODUCTION The K2 mission, represented a way to continue Kepler’s observations after the failure of the spacecraft reaction wheels.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper359_mission2_K2_SCIENCE\n",
      "Bibcode: 2023MNRAS.518..669D\n",
      "Mission: K2\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nK2 D. del Ser,000,000 ★ O.\\nTaken from this text snippet:\\nMNRAS000, 000–000 000 Preprint 000 October 000 Compiled using MNRAS LATEX style file v3.000 TFAW survey II: 000 Newly Validated Planets and 000 Planet Candidates from K2 D. del Ser,000,000★ O.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper368_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.519.1189W\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble Proper motions\\nTaken from this text snippet:\\nIn this paper, we accelerate this exciting effort to unify data from HST and Gaia, presenting the method behind our software package HubPUG (Hubble Proper motions Utilizing Gaia), an application capable of recovering the systematic PMs of fields with two epochs of HST observations by measuring the motion of stars observed by Gaia in the co-moving frame of the target.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425 of 500 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper395_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJ...943...81S\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nOur Hubble\\nTaken from this text snippet:\\nOur Hubble/Spitzer-driven result hints at exciting future insights from JWST’s study of the earliest galaxies.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper400_mission0_HST_MENTION\n",
      "Bibcode: 2023ApJS..264....4M\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble flow\\nTaken from this text snippet:\\nAs quasars generally reside in galaxies, they have peculiar motions that can be several hundreds of kilometers per second relative to the Hubble flow.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper406_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJ...943...54J\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble Sequence\\nTaken from this text snippet:\\nThese include the emergence of the Hubble Sequence at z> 000 (e.g, Franxetal 000; Krieketal 000; Wuytsetal 000, and the dependence of the mass–size relation and Sérsic index n on specific star formation rate (sSFR; e.g, Williamsetal 000; Wuytsetal 000; Newmanetal 000; Barroetal 000; Pateletal 000; van der Weletal 000; Shibuyaetal 000.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper408_mission2_KEPLER_MENTION\n",
      "Bibcode: 2022SSRv..218...29H\n",
      "Mission: KEPLER\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nKepler instrument\\nTaken from this text snippet:\\nCharacteristics of Cool Stellar Environments Properties of Active Cool Stars The X-ray and UV observations by Chandra, the X-ray Multi-Mirror Mission (XMM-Newton), the Hubble Space Telescope, the Kepler instrument, and most recently the Transiting Exoplanet Survey Satellite (TESS) mission not only gave new insight on the diversity of stars but also on the evolution of Sun-like stars during different evolutionary stages.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "450 of 500 total texts have been processed...\n",
      "455 texts have been processed into modifs.\n",
      "45 texts skipped due to unknown ambig. phrases.\n",
      "Storing the data in train+validate+test directories...\n",
      "\n",
      "> Running generate_directory_TVT().\n",
      "Random seed set to: 10\n",
      "\n",
      "Dataset contains 455 papers with 337 unique bibcodes.\n",
      "\n",
      "\n",
      "Class breakdown of given dataset:\n",
      "Counter({'science': 214, 'mention': 208, 'data_influenced': 33})\n",
      "\n",
      "\n",
      "Given dataset inverted. Bibcode count: 337\n",
      "\n",
      "Number of processed text ids: 455\n",
      "Number of bibcodes with single unique classif: 275\n",
      "Number of bibcodes with multiple classifs: 62\n",
      "Number of bibcodes with multiple text ids: 106\n",
      "\n",
      "Bibcode partitioning of representative classifs.:\n",
      "{'science': 153, 'mention': 153, 'data_influenced': 31}\n",
      "\n",
      "Fractions given for TVT split: [0.8 0.1 0.1]\n",
      "Mode requested: uniform\n",
      "Target TVT partition for bibcodes:\n",
      "science: [ 25   3 125]\n",
      "mention: [ 25   3 125]\n",
      "data_influenced: [25  3  3]\n",
      "\n",
      "Indices split per bibcode, per TVT. Shuffling=True.\n",
      "Number of indices per class, per TVT:\n",
      "science: [25, 3, 125]\n",
      "mention: [25, 3, 125]\n",
      "data_influenced: [25, 3, 3]\n",
      "Created new directories for TVT files.\n",
      "Stored in: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_2\n",
      "\n",
      "Files saved to new TVT directories.\n",
      "Final partition of texts across classes and TVT dirs.:\n",
      "{'dir_train': {'science': 38, 'mention': 46, 'data_influenced': 25}, 'dir_validate': {'science': 5, 'mention': 5, 'data_influenced': 3}, 'dir_test': {'science': 171, 'mention': 157, 'data_influenced': 5}}\n",
      "Dictionary of TVT bibcode partitioning info saved at: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_2/dict_TVTinfo.npy.\n",
      "\n",
      "Run of generate_directory_TVT() complete.\n",
      "---\n",
      "\n",
      "Train+validate+test directories created in /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_2.\n",
      "Training new ML model on training data in /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_2...\n",
      "Loading datasets...\n",
      "Loading training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /var/folders/0g/b03r16sd1kl3_j3fml_xwl9h0002td/T/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 109 files belonging to 3 classes.\n",
      "Loading validation data...\n",
      "Found 13 files belonging to 3 classes.\n",
      "Loading testing data...\n",
      "Found 333 files belonging to 3 classes.\n",
      "Done loading datasets.\n",
      "Loading ML model components...\n",
      "Loaded ML preprocessor: https://kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-uncased-preprocess/versions/3\n",
      "Loaded ML encoder: https://kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/bert-en-uncased-l-4-h-512-a-8/versions/1\n",
      "Done loading ML model components.\n",
      "Building an empty ML model...\n",
      "Dropout fraction: 0.2\n",
      "Number of Dense layers: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Lamb optimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building an empty ML model.\n",
      "Setting up loss, metric, and optimization functions...\n",
      "# of training steps: 80\n",
      "# of warmup steps: 8\n",
      "Type of optimizer and initial lr: lamb, 3e-05\n",
      "Done compiling loss, metric, and optimization functions.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text (InputLayer)              [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " preprocessor (KerasLayer)      {'input_word_ids':   0           ['text[0][0]']                   \n",
      "                                (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_type_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " encoder (KerasLayer)           {'pooled_output': (  28763649    ['preprocessor[0][0]',           \n",
      "                                None, 512),                       'preprocessor[0][1]',           \n",
      "                                 'default': (None,                'preprocessor[0][2]']           \n",
      "                                512),                                                             \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 512),                                               \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512),                                                \n",
      "                                 (None, 128, 512)]}                                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['encoder[0][5]']                \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 3)            1539        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,765,188\n",
      "Trainable params: 28,765,187\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "Training the ML model...\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 14s 2s/step - loss: 1.3861 - accuracy: 0.2661 - val_loss: 1.4354 - val_accuracy: 0.3846\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.5305 - accuracy: 0.2202 - val_loss: 1.3551 - val_accuracy: 0.3846\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.4048 - accuracy: 0.2661 - val_loss: 1.2706 - val_accuracy: 0.2308\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.2708 - accuracy: 0.3394 - val_loss: 1.2113 - val_accuracy: 0.1538\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.2323 - accuracy: 0.4037 - val_loss: 1.1661 - val_accuracy: 0.2308\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.1912 - accuracy: 0.3578 - val_loss: 1.1345 - val_accuracy: 0.2308\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.1593 - accuracy: 0.3670 - val_loss: 1.1149 - val_accuracy: 0.3846\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.1941 - accuracy: 0.3761 - val_loss: 1.0988 - val_accuracy: 0.4615\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.1122 - accuracy: 0.4404 - val_loss: 1.0857 - val_accuracy: 0.5385\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.0950 - accuracy: 0.4312 - val_loss: 1.0740 - val_accuracy: 0.5385\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.1485 - accuracy: 0.4312 - val_loss: 1.0575 - val_accuracy: 0.5385\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.0849 - accuracy: 0.5321 - val_loss: 1.0438 - val_accuracy: 0.5385\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.0402 - accuracy: 0.4862 - val_loss: 1.0316 - val_accuracy: 0.5385\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.0860 - accuracy: 0.5046 - val_loss: 1.0211 - val_accuracy: 0.5385\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.0355 - accuracy: 0.4954 - val_loss: 1.0143 - val_accuracy: 0.5385\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.0409 - accuracy: 0.5321 - val_loss: 1.0084 - val_accuracy: 0.5385\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.9952 - accuracy: 0.5321 - val_loss: 1.0041 - val_accuracy: 0.5385\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.9956 - accuracy: 0.5138 - val_loss: 1.0009 - val_accuracy: 0.5385\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.0054 - accuracy: 0.4954 - val_loss: 0.9986 - val_accuracy: 0.5385\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.0289 - accuracy: 0.5138 - val_loss: 0.9978 - val_accuracy: 0.5385\n",
      "\n",
      "Testing the ML model...\n",
      "11/11 [==============================] - 6s 543ms/step - loss: 0.9575 - accuracy: 0.5586\n",
      "\n",
      "Done training and testing the ML model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as keras_layer_layer_call_fn, keras_layer_layer_call_and_return_conditional_losses, keras_layer_1_layer_call_fn, keras_layer_1_layer_call_and_return_conditional_losses, keras_layer_layer_call_fn while saving (showing 5 of 320). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_2/tfoutput_test_run_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_2/tfoutput_test_run_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete.\n",
      "New ML model trained and stored in /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_2.\n",
      "Run of train_model_ML() complete!\n",
      "Error string returned.\n",
      "Time to train the model with run = 716.1126170158386 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26849"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the Operator instance to train an ML model\n",
    "start=time.time()\n",
    "str_err = tabby_ML.train_model_ML(dir_model=dir_model, name_model=name_model, do_reuse_run=do_reuse_run,\n",
    "                        do_check_truematch=do_check_truematch,\n",
    "                        seed_ML=seed_ML, seed_TVT=seed_TVT, dict_texts=dict_texts, mapper=mapper,\n",
    "                        buffer=buffer, fraction_TVT=fraction_TVT, mode_TVT=mode_TVT, do_shuffle=do_shuffle,\n",
    "                        do_verbose=True, do_verbose_deep=False)\n",
    "\n",
    "print(f'Time to train the model with run = {time.time()-start} seconds.')\n",
    "\n",
    "#Save the output error string to a file\n",
    "if (str_err is not None):\n",
    "    with open(filesave_error, 'x') as openfile:\n",
    "        openfile.write(str_err)\n",
    "#\n",
    "\n",
    "#Save the unused bibcodes to a file\n",
    "if ((not do_reuse_run) or (not os.path.exists(filesave_unused_bibcodes))):\n",
    "    np.save(filesave_unused_bibcodes, dict_unused_indsandbibcodes)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df664592",
   "metadata": {},
   "source": [
    "And with that, we're done training a new ML model!  If run successfully, the model will be saved in the `dir_model` directory.\n",
    "\n",
    "We can then use the brand new model to classify some new text, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a002238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:using Lamb optimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance of Operator successfully initialized!\n",
      "Keyword objects:\n",
      "0: Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['HST', 'HT']\n",
      "Banned Overlap: ['Hubble Legacy Archive']\n",
      "\n",
      "1: Keyword Object:\n",
      "Name: Webb Telescope\n",
      "Keywords: ['James Webb Space Telescope', 'Webb Space Telescope', 'James Webb Telescope', 'Webb Telescope']\n",
      "Acronyms: ['JWST', 'JST', 'JT']\n",
      "Banned Overlap: []\n",
      "\n",
      "2: Keyword Object:\n",
      "Name: Transiting Exoplanet Survey Satellite\n",
      "Keywords: ['Transiting Exoplanet Survey Satellite']\n",
      "Acronyms: ['TESS']\n",
      "Banned Overlap: []\n",
      "\n",
      "3: Keyword Object:\n",
      "Name: Kepler\n",
      "Keywords: ['Kepler']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "4: Keyword Object:\n",
      "Name: Pan-STARRS\n",
      "Keywords: ['Panoramic Survey Telescope and Rapid Response System', 'Pan-STARRS1', 'Pan-STARRS']\n",
      "Acronyms: ['PanSTARRS1', 'PanSTARRS', 'PS1']\n",
      "Banned Overlap: []\n",
      "\n",
      "5: Keyword Object:\n",
      "Name: Galaxy Evolution Explorer\n",
      "Keywords: ['Galaxy Evolution Explorer']\n",
      "Acronyms: ['GALEX']\n",
      "Banned Overlap: []\n",
      "\n",
      "6: Keyword Object:\n",
      "Name: K2\n",
      "Keywords: ['K2']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "7: Keyword Object:\n",
      "Name: Hubble Legacy Archive\n",
      "Keywords: ['Hubble Legacy Archive']\n",
      "Acronyms: ['HLA']\n",
      "Banned Overlap: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set path to the new model output\n",
    "filepath_model = os.path.join(dir_model, (name_model+\".npy\"))\n",
    "fileloc_ML = os.path.join(dir_model, (config.tfoutput_prefix+name_model))\n",
    "#Load the new ML model into a new Classifier_ML instance\n",
    "classifier_ML = bibcat.Classifier_ML(filepath_model=filepath_model, fileloc_ML=fileloc_ML,\n",
    "                                    do_verbose=True)\n",
    "#\n",
    "#Load the instance into a new Operator\n",
    "tabby_ML = bibcat.Operator(classifier=classifier_ML, mode=mode_modif, keyword_objs=all_kobjs,\n",
    "                           do_verbose=True, load_check_truematch=True, do_verbose_deep=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8afdc5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running _fetch_keyword_object() for lookup term HST.\n",
      "Best matching keyword object (keyobj) for keyword HST:\n",
      "Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['HST', 'HT']\n",
      "Banned Overlap: ['Hubble Legacy Archive']\n",
      "\n",
      "\n",
      "Preprocessing and extracting modifs from the text...\n",
      "\n",
      "Running Grammar on the text...\n",
      "Text has been processed into modifs.\n",
      "Text has been processed into modif.\n",
      "\n",
      "Running classify_text for ML classifier:\n",
      "Class names from model:\n",
      "['data_influenced', 'mention', 'science']\n",
      "\n",
      "\n",
      "Method classify_text for ML classifier complete!\n",
      "Max verdict: science\n",
      "\n",
      "Uncertainties: {'data_influenced': 0.15019783, 'mention': 0.2538723, 'science': 0.5959298}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run the classifier for some sample text below\n",
    "lookup = \"HST\"\n",
    "text = \"In this study, we present our lovely HST observations of bright stars in the nearby star-forming region Taurus.\"\n",
    "threshold = 0.8\n",
    "#\n",
    "#Run the classifier\n",
    "result = tabby_ML.classify(text=text, lookup=lookup, buffer=0, #threshold=threshold,\n",
    "                            do_raise_innererror=False, do_check_truematch=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c2ba8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modif: In this study, we present our lovely HST observations of bright stars in the nearby star-forming region Taurus.\n",
      "\n",
      "Classification: science\n",
      "\n",
      "Uncertainties per class: {'data_influenced': 0.15019783, 'mention': 0.2538723, 'science': 0.5959298}\n",
      "\n",
      "Full classification output:\n",
      "{'verdict': 'science', 'scores_comb': None, 'scores_indiv': None, 'uncertainty': {'data_influenced': 0.15019783, 'mention': 0.2538723, 'science': 0.5959298}, 'modif': 'In this study, we present our lovely HST observations of bright stars in the nearby star-forming region Taurus.', 'modif_none': 'In this study, we present our lovely HST observations of bright stars in the nearby star-forming region Taurus.'}\n"
     ]
    }
   ],
   "source": [
    "#Print the classifier results\n",
    "print(\"Modif: {2}\\n\\nClassification: {0}\\n\\nUncertainties per class: {1}\\n\"\n",
    "      .format(result[\"verdict\"], result[\"uncertainty\"], result[\"modif\"]))\n",
    "print(\"Full classification output:\\n{0}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2813bea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ab14be5-db76-4745-82f1-829dc861df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tutorial completed successfully.\n"
     ]
    }
   ],
   "source": [
    "#Set end marker for this tutorial.\n",
    "print(\"This tutorial completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
