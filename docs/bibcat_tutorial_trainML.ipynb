{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71ff86",
   "metadata": {},
   "source": [
    "# Bibliography Categorization: 'BibCat'\n",
    "## Tutorial: Machine learning (ML) models in bibcat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ab7e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234a479",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction.\n",
    "\n",
    "In this tutorial, we will use bibcat to train a machine learning (ML) model on some raw input text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f050727",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86289680",
   "metadata": {},
   "source": [
    "## User Workflow: Training a machine learning (ML) model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af236d7a",
   "metadata": {},
   "source": [
    "The `Operator` class contains a user-friendly method `train_model_ML` that runs the full workflow for training an ML model, from the input raw text all the way to saving the output ML model.  We overview how this method can be run in the code blocks below.\n",
    "\n",
    "For this tutorial, we have two sets of data: either 1) some short, made-up text for a quick run of the code, or 2) an imported database of text from an external file of the user's choosing. The former case is useful for getting a quick sense of how the code works. The latter case is useful for building an actual model, but of course will take much longer on larger databases of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdea077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 16:26:17.206858: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Import external packages\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(\"./../main/\")\n",
    "#\n",
    "#Import bibcat packages\n",
    "import bibcat_classes as bibcat\n",
    "import bibcat_config as config\n",
    "import bibcat_constants as preset\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ef4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which data would you like to run the ML model on?  Choose from the booleans below.\n",
    "do_quick_run = True #This will train the ML model on short bits of text. Runs pretty quickly.\n",
    "do_real_run = False #This will train the ML model on external text. Will take longer for larger databases.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0ced33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The rest of these parameters can be left as-is for a first run-through.\n",
    "#\n",
    "num_papers = 500 #500 #None, or an integer; if an integer, will truncate external .json text dataset to this size\n",
    "#Set num_papers=None to use all available papers in external dataset\n",
    "#Note: If set to integer, final paper count might be a little more than target num_papers given\n",
    "#\n",
    "allowed_classifications = config.allowed_classifications #For external data; classifications to include\n",
    "#\n",
    "\n",
    "#Fetch filepaths for model and data\n",
    "name_model = config.name_model\n",
    "filepath_json = config.path_json\n",
    "dir_model = os.path.join(config.dir_allmodels, name_model)\n",
    "#\n",
    "#Set values for generating ML model\n",
    "do_reuse_run = True #Whether or not to reuse any existing output from previous training+validation+testing runs\n",
    "do_shuffle = True #Whether or not to shuffle contents of training vs validation vs testing datasets\n",
    "fraction_TVT = [0.8, 0.1, 0.1] #Fractional breakdown of training vs validation vs testing datasets\n",
    "#\n",
    "mode_TVT = \"uniform\" # \"uniform\" #\"available\"\n",
    "#\"uniform\" = all training datasets will have the same number of entries\n",
    "#\"available\" = all training datasets will use full fraction (from fraction_TVT) of data available\n",
    "#\n",
    "seed_TVT = 10 #Random seed for generating training vs validation vs testing datasets\n",
    "seed_ML = 8 #Random seed for ML model\n",
    "mode_modif = \"skim_trim_anon\" #Mode to use for processing and generating modifs from input raw text\n",
    "#NOTE: See other modif modes in workflow tutorial\n",
    "buffer = 0\n",
    "#\n",
    "#Prepare some Keyword objects\n",
    "kobj_hubble = bibcat.Keyword(\n",
    "                keywords=[\"Hubble\", \"Hubble Telescope\",\n",
    "                          \"Hubble Space Telescope\"],\n",
    "                acronyms=[\"hst\", \"ht\"])\n",
    "all_kobjs = [kobj_hubble]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97faa2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize an empty ML classifier\n",
    "classifier_ML = bibcat.Classifier_ML(filepath_model=None, fileloc_ML=None,\n",
    "                                    class_names=None, do_verbose=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0f528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance of Operator successfully initialized!\n",
      "Keyword objects:\n",
      "0: Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['hst', 'ht']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize an Operator\n",
    "tabby_ML = bibcat.Operator(classifier=classifier_ML, mode=mode_modif, keyword_objs=all_kobjs,\n",
    "                           do_verbose=True, load_check_truematch=False, do_verbose_deep=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac2f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up data for the quick example case. But in reality, ML models should be trained on MUCH more data than this!!!\n",
    "if do_quick_run:\n",
    "    #Make some fake data\n",
    "    dict_texts_raw = {\"science\":[\"We present HST observations in Figure 4.\",\n",
    "                            \"The HST stars are listed in Table 3b.\",\n",
    "                            \"Despite our efforts to smooth the data, there are still rings in the HST images.\",\n",
    "                            \"See Section 8c for more discussion of the Hubble images.\",\n",
    "                            \"The supernovae detected with HST tend to be brighter than initially predicted.\",\n",
    "                            \"Our spectra from HST fit well to the standard trend first published in Someone et al. 1990.\",\n",
    "                            \"We use the Hubble Space Telescope to build an ultraviolet database of the target stars.\",\n",
    "                            \"The blue points (HST) exhibit more scatter than the red points (JWST).\",\n",
    "                            \"The benefit, then, is the far higher S/N we achieved in our HST observations.\",\n",
    "                            \"Here we employ the Hubble Telescope to observe the edge of the photon-dominated region.\",\n",
    "                            \"The black line shows that the region targeted with Hubble has an extreme UV signature.\"],\n",
    "                     \"datainfluenced\":[\"The simulated Hubble data is plotted in Figure 4.\",\n",
    "                           \"Compared to the HST observations in Someone et al., our JWST follow-up reached higher S/N.\",\n",
    "                           \"We were able to reproduce the luminosities from Hubble using our latest models.\",\n",
    "                           \"We overplot Hubble-observed stars from Someone et al. in Figure 3b.\",\n",
    "                           \"We built the spectral templates using UV data in the Hubble archive.\",\n",
    "                           \"We simulate what our future HST observations will look like to predict the S/N.\",\n",
    "                           \"Our work here with JWST is inspired by our earlier HST study published in 2010.\",\n",
    "                           \"We therefore use the Hubble statistics from Author et al. to guide our stellar predictions.\",\n",
    "                           \"The stars in Figure 3 were plotted based on the HST-fitted trend line in Person et al.\",\n",
    "                           \"The final step is to use the HST exposure tool to put our modeled images in context.\"],\n",
    "                     \"mention\":[\"Person et al. used HST to measure the Hubble constant.\",\n",
    "                            \"We will present new HST observations in a future work.\",\n",
    "                            \"HST is a fantastic instrument that has revolutionized our view of space.\",\n",
    "                            \"The Hubble Space Telescope (HST) has its mission center at the STScI.\",\n",
    "                            \"We can use HST to power a variety of science in the ultraviolet regime.\",\n",
    "                            \"It is not clear when the star will be observable with HST.\",\n",
    "                            \"More data can be found and downloaded from the Hubble archive.\",\n",
    "                            \"We note that HST can be used to observe the stars as well, at higher S/N.\",\n",
    "                            \"However, we ended up using the JWST rather than HST observations in this work.\",\n",
    "                            \"We push the analysis of the Hubble component of the dataset to a future study.\",\n",
    "                            \"We expect the HST observations to be released in the fall.\",\n",
    "                            \"We look forward to any follow-up studies with, e.g., the Hubble Telescope.\"]}\n",
    "    #\n",
    "    #Convert into dictionary with: key:text,class,id,mission structure\n",
    "    i_track = 0\n",
    "    dict_texts = {}\n",
    "    for key in dict_texts_raw:\n",
    "        curr_set = dict_texts_raw[key]\n",
    "        for ii in range(0, len(curr_set)):\n",
    "            dict_texts[str(i_track)] = {\"text\":curr_set[ii], \"class\":key, \"id\":\"{0}_{1}\".format(key, ii),\n",
    "                                       \"mission\":\"HST\"}\n",
    "            i_track += 1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d25644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up data for the external data case.\n",
    "if do_real_run:\n",
    "    #Load the original data\n",
    "    with open(filepath_json, 'r') as openfile:\n",
    "        dataset = json.load(openfile)\n",
    "    #\n",
    "    #Organize a new version of the data with: key:text,class,id,mission structure\n",
    "    i_track = 0 #Track number of papers kept from original dataset\n",
    "    dict_texts = {}\n",
    "    for ii in range(0, len(dataset)):\n",
    "        #Extract mission classifications for current text\n",
    "        curr_data = dataset[ii]\n",
    "        #\n",
    "        #Skip if no valid text at all for this text\n",
    "        if (\"body\" not in curr_data):\n",
    "            continue\n",
    "        #\n",
    "        #Skip if no valid missions at all for this text\n",
    "        if (\"class_missions\" not in curr_data):\n",
    "            continue\n",
    "        #\n",
    "        #Otherwise, extract the missions\n",
    "        curr_missions = curr_data[\"class_missions\"]\n",
    "        #\n",
    "        \n",
    "        #Iterate through missions for this text\n",
    "        i_mission = 0\n",
    "        for curr_key in curr_missions:\n",
    "            #If this is not an allowed mission, skip\n",
    "            if (curr_missions[curr_key][\"papertype\"] not in allowed_classifications):\n",
    "                continue\n",
    "            #\n",
    "            #Otherwise, check if this mission is a target mission\n",
    "            fetched_kobj = tabby_ML._fetch_keyword_object(lookup=curr_key,\n",
    "                                                          do_verbose=False, do_raise_emptyerror=False)\n",
    "            #Skip if not a target\n",
    "            if (fetched_kobj is None):\n",
    "                continue\n",
    "            #\n",
    "            #Otherwise, store classification info for this entry\n",
    "            curr_class = curr_missions[curr_key][\"papertype\"]\n",
    "            new_dict = {\"text\":curr_data[\"body\"], #Text for this paper\n",
    "                        \"class\":curr_class, #Classification for this mission\n",
    "                        \"mission\":curr_key, #The mission itself\n",
    "                        \"id\":(\"paper{0}_mission{1}_{2}_{3}\".format(ii, i_mission,\n",
    "                                                                   curr_key, curr_class)) #ID for this entry\n",
    "                       }\n",
    "            dict_texts[str(i_track)] = new_dict\n",
    "            #\n",
    "            #Increment counters\n",
    "            i_mission += 1 #Count of kept missions for this paper\n",
    "            i_track += 1 #Count of kept classifications overall\n",
    "        #\n",
    "\n",
    "        #Terminate early if requested number of papers reached\n",
    "        if ((num_papers is not None) and (i_track >= num_papers)):\n",
    "            break\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af23be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throw error if not enough text entries collected\n",
    "if do_real_run:\n",
    "    if ((num_papers is not None) and (len(dict_texts) < num_papers)):\n",
    "        raise ValueError(\"Err: Something went wrong during initial processing. Insufficient number of texts extracted.\"\n",
    "                        +\"\\nRequested number of texts: {0}\\nActual number of texts: {1}\"\n",
    "                        .format(num_papers, len(dict_texts)))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25b4cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text #0:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_0\n",
      "Text snippet:\n",
      "We present HST observations in Figure 4.\n",
      "---\n",
      "\n",
      "\n",
      "Text #1:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_1\n",
      "Text snippet:\n",
      "The HST stars are listed in Table 3b.\n",
      "---\n",
      "\n",
      "\n",
      "Text #2:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_2\n",
      "Text snippet:\n",
      "Despite our efforts to smooth the data, there are still rings in the HST images.\n",
      "---\n",
      "\n",
      "\n",
      "Text #3:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_3\n",
      "Text snippet:\n",
      "See Section 8c for more discussion of the Hubble images.\n",
      "---\n",
      "\n",
      "\n",
      "Text #4:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_4\n",
      "Text snippet:\n",
      "The supernovae detected with HST tend to be brighter than initially predicted.\n",
      "---\n",
      "\n",
      "\n",
      "Text #5:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_5\n",
      "Text snippet:\n",
      "Our spectra from HST fit well to the standard trend first published in Someone et al. 1990.\n",
      "---\n",
      "\n",
      "\n",
      "Text #6:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_6\n",
      "Text snippet:\n",
      "We use the Hubble Space Telescope to build an ultraviolet database of the target stars.\n",
      "---\n",
      "\n",
      "\n",
      "Text #7:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_7\n",
      "Text snippet:\n",
      "The blue points (HST) exhibit more scatter than the red points (JWST).\n",
      "---\n",
      "\n",
      "\n",
      "Text #8:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_8\n",
      "Text snippet:\n",
      "The benefit, then, is the far higher S/N we achieved in our HST observations.\n",
      "---\n",
      "\n",
      "\n",
      "Text #9:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_9\n",
      "Text snippet:\n",
      "Here we employ the Hubble Telescope to observe the edge of the photon-dominated region.\n",
      "---\n",
      "\n",
      "\n",
      "Text #10:\n",
      "Classification: science\n",
      "Mission: HST\n",
      "ID: science_10\n",
      "Text snippet:\n",
      "The black line shows that the region targeted with Hubble has an extreme UV signature.\n",
      "---\n",
      "\n",
      "\n",
      "Text #11:\n",
      "Classification: datainfluenced\n",
      "Mission: HST\n",
      "ID: datainfluenced_0\n",
      "Text snippet:\n",
      "The simulated Hubble data is plotted in Figure 4.\n",
      "---\n",
      "\n",
      "\n",
      "Text #12:\n",
      "Classification: datainfluenced\n",
      "Mission: HST\n",
      "ID: datainfluenced_1\n",
      "Text snippet:\n",
      "Compared to the HST observations in Someone et al., our JWST follow-up reached higher S/N.\n",
      "---\n",
      "\n",
      "\n",
      "Text #13:\n",
      "Classification: datainfluenced\n",
      "Mission: HST\n",
      "ID: datainfluenced_2\n",
      "Text snippet:\n",
      "We were able to reproduce the luminosities from Hubble using our latest models.\n",
      "---\n",
      "\n",
      "\n",
      "Text #14:\n",
      "Classification: datainfluenced\n",
      "Mission: HST\n",
      "ID: datainfluenced_3\n",
      "Text snippet:\n",
      "We overplot Hubble-observed stars from Someone et al. in Figure 3b.\n",
      "---\n",
      "\n",
      "\n",
      "Text #15:\n",
      "Classification: datainfluenced\n",
      "Mission: HST\n",
      "ID: datainfluenced_4\n",
      "Text snippet:\n",
      "We built the spectral templates using UV data in the Hubble archive.\n",
      "---\n",
      "\n",
      "\n",
      "Text #16:\n",
      "Classification: datainfluenced\n",
      "Mission: HST\n",
      "ID: datainfluenced_5\n",
      "Text snippet:\n",
      "We simulate what our future HST observations will look like to predict the S/N.\n",
      "---\n",
      "\n",
      "\n",
      "Text #17:\n",
      "Classification: datainfluenced\n",
      "Mission: HST\n",
      "ID: datainfluenced_6\n",
      "Text snippet:\n",
      "Our work here with JWST is inspired by our earlier HST study published in 2010.\n",
      "---\n",
      "\n",
      "\n",
      "Text #18:\n",
      "Classification: datainfluenced\n",
      "Mission: HST\n",
      "ID: datainfluenced_7\n",
      "Text snippet:\n",
      "We therefore use the Hubble statistics from Author et al. to guide our stellar predictions.\n",
      "---\n",
      "\n",
      "\n",
      "Text #19:\n",
      "Classification: datainfluenced\n",
      "Mission: HST\n",
      "ID: datainfluenced_8\n",
      "Text snippet:\n",
      "The stars in Figure 3 were plotted based on the HST-fitted trend line in Person et al.\n",
      "---\n",
      "\n",
      "\n",
      "Text #20:\n",
      "Classification: datainfluenced\n",
      "Mission: HST\n",
      "ID: datainfluenced_9\n",
      "Text snippet:\n",
      "The final step is to use the HST exposure tool to put our modeled images in context.\n",
      "---\n",
      "\n",
      "\n",
      "Text #21:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_0\n",
      "Text snippet:\n",
      "Person et al. used HST to measure the Hubble constant.\n",
      "---\n",
      "\n",
      "\n",
      "Text #22:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_1\n",
      "Text snippet:\n",
      "We will present new HST observations in a future work.\n",
      "---\n",
      "\n",
      "\n",
      "Text #23:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_2\n",
      "Text snippet:\n",
      "HST is a fantastic instrument that has revolutionized our view of space.\n",
      "---\n",
      "\n",
      "\n",
      "Text #24:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_3\n",
      "Text snippet:\n",
      "The Hubble Space Telescope (HST) has its mission center at the STScI.\n",
      "---\n",
      "\n",
      "\n",
      "Text #25:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_4\n",
      "Text snippet:\n",
      "We can use HST to power a variety of science in the ultraviolet regime.\n",
      "---\n",
      "\n",
      "\n",
      "Text #26:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_5\n",
      "Text snippet:\n",
      "It is not clear when the star will be observable with HST.\n",
      "---\n",
      "\n",
      "\n",
      "Text #27:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_6\n",
      "Text snippet:\n",
      "More data can be found and downloaded from the Hubble archive.\n",
      "---\n",
      "\n",
      "\n",
      "Text #28:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_7\n",
      "Text snippet:\n",
      "We note that HST can be used to observe the stars as well, at higher S/N.\n",
      "---\n",
      "\n",
      "\n",
      "Text #29:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_8\n",
      "Text snippet:\n",
      "However, we ended up using the JWST rather than HST observations in this work.\n",
      "---\n",
      "\n",
      "\n",
      "Text #30:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_9\n",
      "Text snippet:\n",
      "We push the analysis of the Hubble component of the dataset to a future study.\n",
      "---\n",
      "\n",
      "\n",
      "Text #31:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_10\n",
      "Text snippet:\n",
      "We expect the HST observations to be released in the fall.\n",
      "---\n",
      "\n",
      "\n",
      "Text #32:\n",
      "Classification: mention\n",
      "Mission: HST\n",
      "ID: mention_11\n",
      "Text snippet:\n",
      "We look forward to any follow-up studies with, e.g., the Hubble Telescope.\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Uncomment the code below to print a snippet of each of the entries in the dataset.\n",
    "#\"\"\"\n",
    "for curr_key in dict_texts:\n",
    "    print(\"Text #{0}:\".format(curr_key))\n",
    "    print(\"Classification: {0}\".format(dict_texts[curr_key][\"class\"]))\n",
    "    print(\"Mission: {0}\".format(dict_texts[curr_key][\"mission\"]))\n",
    "    print(\"ID: {0}\".format(dict_texts[curr_key][\"id\"]))\n",
    "    print(\"Text snippet:\")\n",
    "    print(dict_texts[curr_key][\"text\"][0:500])\n",
    "    print(\"---\\n\\n\")\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6182daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target missions:\n",
      "Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['hst', 'ht']\n",
      "\n",
      "\n",
      "\n",
      "Number of valid text entries:\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "#Print number of texts that fell under given parameters\n",
    "print(\"Target missions:\")\n",
    "for curr_kobj in all_kobjs:\n",
    "    print(curr_kobj)\n",
    "    print(\"\")\n",
    "print(\"\")\n",
    "print(\"Number of valid text entries:\")\n",
    "print(len(dict_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42320d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Running train_model_ML()!\n",
      "Loading data from given .json file or dict. of texts...\n",
      "Text data has been loaded.\n",
      "Processing text data into modifs...\n",
      "25 of 33 total texts have been processed...\n",
      "Text data has been processed into modifs.\n",
      "Storing the data in train+validate+test directories...\n",
      "\n",
      "> Running generate_directory_TVT().\n",
      "Random seed set to: 10\n",
      "\n",
      "Class breakdown of given dataset:\n",
      "Counter({'mention': 12, 'science': 11, 'datainfluenced': 10})\n",
      "\n",
      "Fractions given for TVT split: [0.8 0.1 0.1]\n",
      "Mode requested: uniform\n",
      "TVT partition per class:\n",
      "science: [8 1 2]\n",
      "datainfluenced: [8 1 1]\n",
      "mention: [8 1 3]\n",
      "\n",
      "Indices split per class, per TVT. Shuffling=True.\n",
      "Created new directories for TVT files.\n",
      "Stored in: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/scratchwork/test_model_2023_08_25a\n",
      "Files saved to new TVT directories.\n",
      "dict_keys(['i_clausechain', 'i_clausetrail', 'word', 'wordchunk', 'is_important', 'dict_importance', 'is_useless', 'pos_main'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be saved when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Use the Operator instance to train an ML model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtabby_ML\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model_ML\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdir_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_reuse_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_reuse_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mseed_ML\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_ML\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_TVT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_TVT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfraction_TVT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfraction_TVT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_TVT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode_TVT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_shuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdo_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_verbose_deep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/docs/./../main/bibcat_classes.py:6327\u001b[0m, in \u001b[0;36mOperator.train_model_ML\u001b[0;34m(self, dir_model, name_model, do_reuse_run, seed_TVT, seed_ML, filename_json, dict_texts, buffer, fraction_TVT, mode_TVT, do_shuffle, print_freq, do_verbose, do_verbose_deep)\u001b[0m\n\u001b[1;32m   6323\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring the data in train+validate+test directories...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6324\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   6325\u001b[0m \n\u001b[1;32m   6326\u001b[0m \u001b[38;5;66;03m#Store the modifs in new TVT directories\u001b[39;00m\n\u001b[0;32m-> 6327\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_directory_TVT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdir_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6328\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfraction_TVT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfraction_TVT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_TVT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode_TVT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6329\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilename_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_modifs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6330\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_shuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_shuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed_TVT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6331\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_verbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6332\u001b[0m \u001b[38;5;66;03m#Print some notes\u001b[39;00m\n\u001b[1;32m   6333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_verbose:\n",
      "File \u001b[0;32m~/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/docs/./../main/bibcat_classes.py:3287\u001b[0m, in \u001b[0;36m_Classifier.generate_directory_TVT\u001b[0;34m(self, dir_model, fraction_TVT, mode_TVT, filename_json, dict_texts, do_shuffle, seed, do_verbose)\u001b[0m\n\u001b[1;32m   3285\u001b[0m \u001b[38;5;66;03m#!!!\u001b[39;00m\n\u001b[1;32m   3286\u001b[0m tmp_filesave \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict_textinfo.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3287\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_filesave\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3288\u001b[0m \u001b[38;5;28mprint\u001b[39m(woo)\n\u001b[1;32m   3289\u001b[0m \u001b[38;5;66;03m#Print some notes\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py3p10_bibcat/lib/python3.10/site-packages/numpy/lib/npyio.py:522\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n\u001b[0;32m--> 522\u001b[0m     \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfix_imports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imports\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py3p10_bibcat/lib/python3.10/site-packages/numpy/lib/format.py:707\u001b[0m, in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# We contain Python objects so we cannot write out the data\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# directly.  Instead, we will pickle it out\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject arrays cannot be saved when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m         pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be saved when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "#Use the Operator instance to train an ML model\n",
    "tabby_ML.train_model_ML(dir_model=dir_model, name_model=name_model, do_reuse_run=do_reuse_run,\n",
    "                        seed_ML=seed_ML, seed_TVT=seed_TVT, filename_json=None, dict_texts=dict_texts,\n",
    "                        buffer=buffer, fraction_TVT=fraction_TVT, mode_TVT=mode_TVT, do_shuffle=do_shuffle,\n",
    "                        do_verbose=True, do_verbose_deep=None)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df664592",
   "metadata": {},
   "source": [
    "And with that, we're done training a new ML model!  If run successfully, the model will be saved in the `dir_model` directory.\n",
    "\n",
    "We can then use the brand new model to classify some new text, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a002238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set path to the new model output\n",
    "filepath_model = os.path.join(dir_model, (name_model+\".npy\"))\n",
    "fileloc_ML = os.path.join(dir_model, (preset.tfoutput_prefix+name_model))\n",
    "#Load the new ML model into a new Classifier_ML instance\n",
    "classifier_ML = bibcat.Classifier_ML(filepath_model=filepath_model, fileloc_ML=fileloc_ML,\n",
    "                                    do_verbose=True)\n",
    "#\n",
    "#Load the instance into a new Operator\n",
    "tabby_ML = bibcat.Operator(classifier=classifier_ML, mode=mode_modif, keyword_objs=all_kobjs,\n",
    "                           do_verbose=True, load_check_truematch=True, do_verbose_deep=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the classifier for some sample text below\n",
    "lookup = \"HST\"\n",
    "text = \"In this study, we present our HST observations of stars in the star-forming region Taurus.\"\n",
    "threshold = 0.8\n",
    "#\n",
    "#Run the classifier\n",
    "result = tabby_ML.classify(text=text, lookup=lookup, buffer=0, threshold=threshold,\n",
    "                            do_raise_innererror=False, do_check_truematch=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ba8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the classifier results\n",
    "print(\"Modif: {2}\\n\\nClassification: {0}\\n\\nUncertainties per class: {1}\\n\"\n",
    "      .format(result[\"verdict\"], result[\"uncertainty\"], result[\"modif\"]))\n",
    "print(\"Full classification output:\\n{0}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2813bea",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
