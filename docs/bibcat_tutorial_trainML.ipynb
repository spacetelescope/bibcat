{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71ff86",
   "metadata": {},
   "source": [
    "# Bibliography Categorization: 'BibCat'\n",
    "## Tutorial: Machine learning (ML) models in bibcat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ab7e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234a479",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction.\n",
    "\n",
    "In this tutorial, we will use bibcat to train a machine learning (ML) model on some raw input text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f050727",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86289680",
   "metadata": {},
   "source": [
    "## User Workflow: Training a machine learning (ML) model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af236d7a",
   "metadata": {},
   "source": [
    "The `Operator` class contains a user-friendly method `train_model_ML` that runs the full workflow for training an ML model, from the input raw text all the way to saving the output ML model.  We overview how this method can be run in the code blocks below.\n",
    "\n",
    "For this tutorial, we have two sets of data: either 1) some short, made-up text for a quick run of the code, or 2) an imported database of text from an external file of the user's choosing. The former case is useful for getting a quick sense of how the code works. The latter case is useful for building an actual model, but of course will take much longer on larger databases of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e1149ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdea077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import external packages\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f14b847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/docs\n",
      "Source directory: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src\n"
     ]
    }
   ],
   "source": [
    "# Set up for fetching necessary bibcat modules for the tutorial\n",
    "# Check work directories: src/ is where all source python scripts are available. \n",
    "current_dir= os.path.dirname(os.path.abspath('__file__'))\n",
    "_parent = os.path.dirname(current_dir)\n",
    "src_dir = os.path.join(_parent, \"src\")\n",
    "\n",
    "print(f'Current Directory: {current_dir}')\n",
    "print(f'Source directory: {src_dir}')\n",
    "\n",
    "# move to the ../src/ directory to import necessary modules. \n",
    "os.chdir(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8643e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory =/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src, parent directory=/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat\n",
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models folder already exists.\n",
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/output folder already exists.\n"
     ]
    }
   ],
   "source": [
    "#Import bibcat packages\n",
    "import bibcat_classes as bibcat\n",
    "import bibcat_config as config\n",
    "import bibcat_parameters as params #Temporary file until contents moved elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ef4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which data would you like to run the ML model on?  Choose from the booleans below.\n",
    "do_quick_run = False #This will train the ML model on short bits of text. Runs pretty quickly.\n",
    "do_real_run = True #This will train the ML model on external text. Will take longer for larger databases.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0ced33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The rest of these parameters can be left as-is for a first run-through.\n",
    "#\n",
    "do_check_truematch = True #A very important parameter - discuss with J.P. first!!!  Set it to either True or False.\n",
    "#If any papers in dataset encountered within the codebase that have unknown ambiguous phrases...\n",
    "#...then a note will be printed and those papers will not be used for training-validation-testing.\n",
    "#...Add the identified ambiguous phrase to the external ambiguous phrase database and rerun to include those papers.\n",
    "#\n",
    "num_papers = None #500 #None, or an integer; if an integer, will truncate external .json text dataset to this size\n",
    "#Set num_papers=None to use all available papers in external dataset\n",
    "#Note: If set to integer, final paper count might be a little more than target num_papers given\n",
    "#\n",
    "allowed_classifications = params.allowed_classifications #For external data; classifications to include\n",
    "mapper = params.map_papertypes #For masking of classes (e.g., masking 'supermention' as 'mention')\n",
    "#\n",
    "\n",
    "#Fetch filepaths for model and data\n",
    "name_model = config.name_model\n",
    "filepath_json = config.path_json\n",
    "dir_model = os.path.join(config.dir_allmodels, name_model)\n",
    "filesave_error = os.path.join(dir_model,\n",
    "                              \"{0}_processing_errors.txt\".format(name_model)) #Where to save processing errors\n",
    "filesave_unused_bibcodes = os.path.join(dir_model,\n",
    "                              \"{0}_bibcodes_unused_during_trainML.npy\".format(name_model)) #Where to save processing errors\n",
    "#\n",
    "#Set values for generating ML model\n",
    "do_reuse_run = True #Whether or not to reuse any existing output from previous training+validation+testing runs\n",
    "do_shuffle = True #Whether or not to shuffle contents of training vs validation vs testing datasets\n",
    "fraction_TVT = [0.8, 0.1, 0.1] #Fractional breakdown of training vs validation vs testing datasets\n",
    "#\n",
    "mode_TVT = \"uniform\" # \"uniform\" #\"available\"\n",
    "#\"uniform\" = all training datasets will have the same number of entries\n",
    "#\"available\" = all training datasets will use full fraction (from fraction_TVT) of data available\n",
    "#\n",
    "seed_TVT = 10 #Random seed for generating training vs validation vs testing datasets\n",
    "seed_ML = 8 #Random seed for ML model\n",
    "mode_modif = \"none\" #\"skim_anon\" #\"skim_trim_anon\" #Mode to use for processing and generating modifs from input raw text\n",
    "#NOTE: See other modif modes in workflow tutorial\n",
    "buffer = 0\n",
    "#\n",
    "all_kobjs = params.all_kobjs\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97faa2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize an empty ML classifier\n",
    "classifier_ML = bibcat.Classifier_ML(filepath_model=None, fileloc_ML=None, do_verbose=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0f528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance of Operator successfully initialized!\n",
      "Keyword objects:\n",
      "0: Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['HST', 'HT']\n",
      "Banned Overlap: ['Hubble Legacy Archive']\n",
      "\n",
      "1: Keyword Object:\n",
      "Name: Webb Telescope\n",
      "Keywords: ['James Webb Space Telescope', 'Webb Space Telescope', 'James Webb Telescope', 'Webb Telescope']\n",
      "Acronyms: ['JWST', 'JST', 'JT']\n",
      "Banned Overlap: []\n",
      "\n",
      "2: Keyword Object:\n",
      "Name: Transiting Exoplanet Survey Satellite\n",
      "Keywords: ['Transiting Exoplanet Survey Satellite']\n",
      "Acronyms: ['TESS']\n",
      "Banned Overlap: []\n",
      "\n",
      "3: Keyword Object:\n",
      "Name: Kepler\n",
      "Keywords: ['Kepler']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "4: Keyword Object:\n",
      "Name: Pan-STARRS\n",
      "Keywords: ['Panoramic Survey Telescope and Rapid Response System', 'Pan-STARRS1', 'Pan-STARRS']\n",
      "Acronyms: ['PanSTARRS1', 'PanSTARRS', 'PS1']\n",
      "Banned Overlap: []\n",
      "\n",
      "5: Keyword Object:\n",
      "Name: Galaxy Evolution Explorer\n",
      "Keywords: ['Galaxy Evolution Explorer']\n",
      "Acronyms: ['GALEX']\n",
      "Banned Overlap: []\n",
      "\n",
      "6: Keyword Object:\n",
      "Name: K2\n",
      "Keywords: ['K2']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "7: Keyword Object:\n",
      "Name: Hubble Legacy Archive\n",
      "Keywords: ['Hubble Legacy Archive']\n",
      "Acronyms: ['HLA']\n",
      "Banned Overlap: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize an Operator\n",
    "tabby_ML = bibcat.Operator(classifier=classifier_ML, mode=mode_modif, keyword_objs=all_kobjs,\n",
    "                           do_verbose=True, load_check_truematch=do_check_truematch, do_verbose_deep=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dac2f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up data for the quick example case. But in reality, ML models should be trained on MUCH more data than this!!!\n",
    "if do_quick_run:\n",
    "    #Make some fake data\n",
    "    dict_texts_raw = {\"science\":[\"We present HST observations in Figure 4.\",\n",
    "                            \"The HST stars are listed in Table 3b.\",\n",
    "                            \"Despite our efforts to smooth the data, there are still rings in the HST images.\",\n",
    "                            \"See Section 8c for more discussion of the Hubble images.\",\n",
    "                            \"The supernovae detected with HST tend to be brighter than initially predicted.\",\n",
    "                            \"Our spectra from HST fit well to the standard trend first published in Someone et al. 1990.\",\n",
    "                            \"We use the Hubble Space Telescope to build an ultraviolet database of the target stars.\",\n",
    "                            \"The blue points (HST) exhibit more scatter than the red points (JWST).\",\n",
    "                            \"The benefit, then, is the far higher S/N we achieved in our HST observations.\",\n",
    "                            \"Here we employ the Hubble Telescope to observe the edge of the photon-dominated region.\",\n",
    "                            \"The black line shows that the region targeted with Hubble has an extreme UV signature.\"],\n",
    "                     \"datainfluenced\":[\"The simulated Hubble data is plotted in Figure 4.\",\n",
    "                           \"Compared to the HST observations in Someone et al., our JWST follow-up reached higher S/N.\",\n",
    "                           \"We were able to reproduce the luminosities from Hubble using our latest models.\",\n",
    "                           \"We overplot Hubble-observed stars from Someone et al. in Figure 3b.\",\n",
    "                           \"We built the spectral templates using UV data in the Hubble archive.\",\n",
    "                           \"We simulate what our future HST observations will look like to predict the S/N.\",\n",
    "                           \"Our work here with JWST is inspired by our earlier HST study published in 2010.\",\n",
    "                           \"We therefore use the Hubble statistics from Author et al. to guide our stellar predictions.\",\n",
    "                           \"The stars in Figure 3 were plotted based on the HST-fitted trend line in Person et al.\",\n",
    "                           \"The final step is to use the HST exposure tool to put our modeled images in context.\"],\n",
    "                     \"mention\":[\"Person et al. used HST to measure the Hubble constant.\",\n",
    "                            \"We will present new HST observations in a future work.\",\n",
    "                            \"HST is do_a fantastic instrument that has revolutionized our view of space.\",\n",
    "                            \"The Hubble Space Telescope (HST) has its mission center at the STScI.\",\n",
    "                            \"We can use HST to power a variety of science in the ultraviolet regime.\",\n",
    "                            \"It is not clear when the star will be observable with HST.\",\n",
    "                            \"More data can be found and downloaded from the Hubble archive.\",\n",
    "                            \"We note that HST can be used to observe the stars as well, at higher S/N.\",\n",
    "                            \"However, we ended up using the JWST rather than HST observations in this work.\",\n",
    "                            \"We push the analysis of the Hubble component of the dataset to a future study.\",\n",
    "                            \"We expect the HST observations to be released in the fall.\",\n",
    "                            \"We look forward to any follow-up studies with, e.g., the Hubble Telescope.\"]}\n",
    "    #\n",
    "    #Convert into dictionary with: key:text,class,id,mission structure\n",
    "    i_track = 0\n",
    "    dict_texts = {}\n",
    "    for key in dict_texts_raw:\n",
    "        curr_set = dict_texts_raw[key]\n",
    "        for ii in range(0, len(curr_set)):\n",
    "            dict_texts[str(i_track)] = {\"text\":curr_set[ii], \"class\":key, \"id\":\"{0}_{1}\".format(key, ii),\n",
    "                                       \"mission\":\"HST\", \"bibcode\":\"{0}_{1}\".format(key, ii)}\n",
    "            i_track += 1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d25644b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60157"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate bibcode encountered: 2022MNRAS.516.5618P. Skipping.\n",
      "Duplicate bibcode encountered: 2022NatAs...6..141B. Skipping.\n",
      "Duplicate bibcode encountered: 2022Natur.603..815W. Skipping.\n",
      "Duplicate bibcode encountered: 2022MNRAS.515.2386R. Skipping.\n",
      "Duplicate bibcode encountered: 2022MNRAS.515.3336F. Skipping.\n",
      "Duplicate bibcode encountered: 2022Galax..10...76S. Skipping.\n",
      "Duplicate bibcode encountered: 2022MNRAS.515.2951S. Skipping.\n",
      "Duplicate bibcode encountered: 2022PSJ.....3..117G. Skipping.\n",
      "Duplicate bibcode encountered: 2022MNRAS.516.1573Z. Skipping.\n",
      "Duplicate bibcode encountered: 2022MNRAS.515.3319R. Skipping.\n",
      "Duplicate bibcode encountered: 2022MNRAS.515.4201G. Skipping.\n",
      "Duplicate bibcode encountered: 2022MNRAS.516.1977G. Skipping.\n",
      "Duplicate bibcode encountered: 2022MNRAS.513.3663K. Skipping.\n",
      "Duplicate bibcode encountered: 2022JGRE..12706853F. Skipping.\n",
      "Duplicate bibcode encountered: 2022MNRAS.515.2698A. Skipping.\n",
      "Duplicate bibcode encountered: 2022ApJ...930....2M. Skipping.\n",
      "Duplicate bibcode encountered: 2022ApJ...931...91S. Skipping.\n",
      "Duplicate bibcode encountered: 2022A&A...665L...4S. Skipping.\n",
      "Duplicate bibcode encountered: 2022AJ....164...84A. Skipping.\n",
      "Duplicate bibcode encountered: 2022A&A...664L..15M. Skipping.\n",
      "Duplicate bibcode encountered: 2022A&A...668A..22T. Skipping.\n",
      "Duplicate bibcode encountered: 2022A&A...666A..15S. Skipping.\n",
      "Duplicate bibcode encountered: 2022AJ....164...19G. Skipping.\n",
      "Duplicate bibcode encountered: 2022A&A...666A.104E. Skipping.\n",
      "Duplicate bibcode encountered: 2022AdAst2022E...5S. Skipping.\n",
      "Duplicate bibcode encountered: 2022A&A...662L...8P. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1323L. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1003W. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1261C. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1933S. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1437N. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1232F. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1158J. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1038L. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..998A. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1476H. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..962K. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500L..12K. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1466H. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1404S. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500L...1A. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1139H. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1178P. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1222S. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1313B. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1547S. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500L..37D. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1054C. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1343S. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1018M. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500L..47K. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500.1340D. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..358B. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..663M. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..942D. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..310D. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..272S. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..590C. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..118Z. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500...54N. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..531A. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..506V. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500...34T. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..558M. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..232P. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..817C. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..432A. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..926G. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..520R. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..259S. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..859M. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..109Z. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..548L. Skipping.\n",
      "Duplicate bibcode encountered: 2021MNRAS.500..211G. Skipping.\n",
      "Duplicate bibcode encountered: 2020NatAs...4..419B. Skipping.\n",
      "Duplicate bibcode encountered: 2020NatAs...4..246G. Skipping.\n",
      "Duplicate bibcode encountered: 2020NatAs...4..252S. Skipping.\n",
      "Duplicate bibcode encountered: 2020NatAs...4..266P. Skipping.\n",
      "Duplicate bibcode encountered: 2020NatAs...4..399S. Skipping.\n",
      "Duplicate bibcode encountered: 2020NatAs...4..408H. Skipping.\n",
      "Duplicate bibcode encountered: 2020NatAs...4..240P. Skipping.\n",
      "Duplicate bibcode encountered: 2020A&A...633A...1L. Skipping.\n",
      "Duplicate bibcode encountered: 2020A&A...633A...4K. Skipping.\n",
      "Duplicate bibcode encountered: 2020A&A...633A...3K. Skipping.\n",
      "Duplicate bibcode encountered: 2020A&A...633A...5B. Skipping.\n",
      "Duplicate bibcode encountered: 2020A&A...633A...7V. Skipping.\n",
      "Duplicate bibcode encountered: 2020A&A...633A...6E. Skipping.\n",
      "Duplicate bibcode encountered: 2019Natur.565..240H. Skipping.\n"
     ]
    }
   ],
   "source": [
    "#Set up data for the external data case.\n",
    "if do_real_run:\n",
    "    #Load the original data\n",
    "    with open(filepath_json, 'r') as openfile:\n",
    "        dataset = json.load(openfile)\n",
    "        len(dataset)\n",
    "    #\n",
    "    #Initialize holder to keep track of bibcodes used (avoids duplicate dataset entries)\n",
    "    list_bibcodes = []\n",
    "    dict_unused_indsandbibcodes = {} #Dictionary preserves uniqueness of unused bibcodes\n",
    "    #\n",
    "    #Organize a new version of the data with: key:text,class,id,mission structure\n",
    "    i_track = 0 #Track number of papers kept from original dataset\n",
    "    dict_texts = {}\n",
    "    for ii in range(0, len(dataset)):\n",
    "        #Extract mission classifications for current text\n",
    "        curr_data = dataset[ii]\n",
    "        curr_bibcode = curr_data[\"bibcode\"]\n",
    "        #\n",
    "        #Skip if no valid text at all for this text\n",
    "        if (\"body\" not in curr_data):\n",
    "            continue\n",
    "        #\n",
    "        #Skip if no valid missions at all for this text\n",
    "        if (\"class_missions\" not in curr_data):\n",
    "            dict_unused_indsandbibcodes[curr_bibcode] = ii\n",
    "            continue\n",
    "        #\n",
    "        #Otherwise, extract the missions\n",
    "        curr_missions = curr_data[\"class_missions\"]\n",
    "        #print(curr_bibcode)\n",
    "        #print(curr_missions)\n",
    "        #\n",
    "        #Skip if bibcode already encountered (and so duplicate entry)\n",
    "        if (curr_bibcode in list_bibcodes):\n",
    "            print(\"Duplicate bibcode encountered: {0}. Skipping.\".format(curr_bibcode))\n",
    "            continue\n",
    "        #\n",
    "        #Iterate through missions for this text\n",
    "        i_mission = 0\n",
    "        for curr_key in curr_missions:\n",
    "            #If this is not an allowed classification, skip\n",
    "            if (curr_missions[curr_key][\"papertype\"] not in allowed_classifications):\n",
    "                dict_unused_indsandbibcodes[curr_bibcode] = ii\n",
    "                continue\n",
    "            #\n",
    "            #Otherwise, check if this mission is a target mission\n",
    "            fetched_kobj = tabby_ML._fetch_keyword_object(lookup=curr_key,\n",
    "                                                          do_verbose=False, do_raise_emptyerror=False)\n",
    "            #Skip if not a target mission\n",
    "            if (fetched_kobj is None):\n",
    "                dict_unused_indsandbibcodes[curr_bibcode] = ii\n",
    "                continue\n",
    "            #\n",
    "            #Otherwise, store classification info for this entry\n",
    "            curr_class = curr_missions[curr_key][\"papertype\"]\n",
    "            new_dict = {\"text\":curr_data[\"body\"], #Text for this paper\n",
    "                        \"bibcode\":curr_data[\"bibcode\"], #Bibcode for this paper\n",
    "                        \"class\":curr_class, #Classification for this mission\n",
    "                        \"mission\":curr_key, #The mission itself\n",
    "                        \"id\":(\"paper{0}_mission{1}_{2}_{3}\".format(ii, i_mission,\n",
    "                                                                   curr_key, curr_class)) #ID for this entry\n",
    "                       }\n",
    "            dict_texts[str(i_track)] = new_dict\n",
    "            #\n",
    "            #Increment counters\n",
    "            i_mission += 1 #Count of kept missions for this paper\n",
    "            i_track += 1 #Count of kept classifications overall\n",
    "        #\n",
    "        \n",
    "        #Record this bibcode as stored\n",
    "        list_bibcodes.append(curr_bibcode)\n",
    "\n",
    "        #Terminate early if requested number of papers reached\n",
    "        if ((num_papers is not None) and (i_track >= num_papers)):\n",
    "            #Store the remaining bibcodes as unused\n",
    "            dict_unused_indsandbibcodes.update({dataset[jj][\"bibcode\"]:jj\n",
    "                                             for jj in range((ii+1), len(dataset))})\n",
    "            break\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af23be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throw error if not enough text entries collected\n",
    "if do_real_run:\n",
    "    if ((num_papers is not None) and (len(dict_texts) < num_papers)):\n",
    "        raise ValueError(\"Err: Something went wrong during initial processing. Insufficient number of texts extracted.\"\n",
    "                        +\"\\nRequested number of texts: {0}\\nActual number of texts: {1}\"\n",
    "                        .format(num_papers, len(dict_texts)))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d25b4cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Number of processed texts: {0}={1}\\n\".format(i_track, len(dict_texts)))\\nfor curr_key in dict_texts:\\n    print(\"Text #{0}:\".format(curr_key))\\n    print(\"Classification: {0}\".format(dict_texts[curr_key][\"class\"]))\\n    print(\"Mission: {0}\".format(dict_texts[curr_key][\"mission\"]))\\n    print(\"ID: {0}\".format(dict_texts[curr_key][\"id\"]))\\n    print(\"Bibcode: {0}\".format(dict_texts[curr_key][\"bibcode\"]))\\n    print(\"Text snippet:\")\\n    print(dict_texts[curr_key][\"text\"][0:500])\\n    print(\"---\\n\\n\")\\n#'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uncomment the code below to print a snippet of each of the entries in the dataset.\n",
    "\"\"\"\n",
    "print(\"Number of processed texts: {0}={1}\\n\".format(i_track, len(dict_texts)))\n",
    "for curr_key in dict_texts:\n",
    "    print(\"Text #{0}:\".format(curr_key))\n",
    "    print(\"Classification: {0}\".format(dict_texts[curr_key][\"class\"]))\n",
    "    print(\"Mission: {0}\".format(dict_texts[curr_key][\"mission\"]))\n",
    "    print(\"ID: {0}\".format(dict_texts[curr_key][\"id\"]))\n",
    "    print(\"Bibcode: {0}\".format(dict_texts[curr_key][\"bibcode\"]))\n",
    "    print(\"Text snippet:\")\n",
    "    print(dict_texts[curr_key][\"text\"][0:500])\n",
    "    print(\"---\\n\\n\")\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6182daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target missions:\n",
      "Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['HST', 'HT']\n",
      "Banned Overlap: ['Hubble Legacy Archive']\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Webb Telescope\n",
      "Keywords: ['James Webb Space Telescope', 'Webb Space Telescope', 'James Webb Telescope', 'Webb Telescope']\n",
      "Acronyms: ['JWST', 'JST', 'JT']\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Transiting Exoplanet Survey Satellite\n",
      "Keywords: ['Transiting Exoplanet Survey Satellite']\n",
      "Acronyms: ['TESS']\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Kepler\n",
      "Keywords: ['Kepler']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Pan-STARRS\n",
      "Keywords: ['Panoramic Survey Telescope and Rapid Response System', 'Pan-STARRS1', 'Pan-STARRS']\n",
      "Acronyms: ['PanSTARRS1', 'PanSTARRS', 'PS1']\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Galaxy Evolution Explorer\n",
      "Keywords: ['Galaxy Evolution Explorer']\n",
      "Acronyms: ['GALEX']\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: K2\n",
      "Keywords: ['K2']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "Keyword Object:\n",
      "Name: Hubble Legacy Archive\n",
      "Keywords: ['Hubble Legacy Archive']\n",
      "Acronyms: ['HLA']\n",
      "Banned Overlap: []\n",
      "\n",
      "\n",
      "\n",
      "Number of valid text entries:\n",
      "28950\n"
     ]
    }
   ],
   "source": [
    "#Print number of texts that fell under given parameters\n",
    "print(\"Target missions:\")\n",
    "for curr_kobj in all_kobjs:\n",
    "    print(curr_kobj)\n",
    "    print(\"\")\n",
    "print(\"\")\n",
    "print(\"Number of valid text entries:\")\n",
    "print(len(dict_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42320d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Running train_model_ML()!\n",
      "Processing text data into modifs...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper2_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJS..265....5H\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST Advanced Camera\\nTaken from this text snippet:\\nTo date, deep-field imaging observations have reached detection limits of ≃000 mag in the wavelength range of 000.000–000.000 μ m with the HST Advanced Camera for Surveys (ACS) and the Wide Field Camera 000 (WFC3) instruments in the Hubble Ultra Deep Field (Beckwithetal 000; see Bouwensetal 000 and references therein) with the moderately deep ultraviolet (UV) extension, UVUDF 000.000–000.000 μ m; Windhorstetal 000; Teplitzetal 000.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper16_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.4755A\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\n000 Hubble E\\nTaken from this text snippet:\\nW, Bouwens R, Oesch P, Smit R, Illingworth G, Labbe I, 000, ApJ, 000, 000 Hubble E, Humason M.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "25 of 28950 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper26_mission0_HST_SCIENCE\n",
      "Bibcode: 2023AJ....165...13W\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST diffraction limit\\nTaken from this text snippet:\\nThe tendency of faint galaxies to bunch up against the HST diffraction limit at brighter flux levels was first suggested based on the Hubble Deep Field images by Authorsetal and Authorsetal, and later by Welchetal (2022c), Authorsetal, and references therein based on more recent HST images.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "50 of 28950 total texts have been processed...\n",
      "75 of 28950 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper70_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.519..157W\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST /ACS All - Wavelength Extended Groth Strip International Survey\\nTaken from this text snippet:\\nThe ACS mosaics used in this work were produced as part of the Complete Hubble Archive for Galaxy Evolution (CHArGE) project (Kokorev (in prep.)), and include observations obtained from the HST /ACS All-Wavelength Extended Groth Strip International Survey (AEGIS; Davisetal 000, the Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey (CANDELS; Groginetal 000; Koekemoeretal 000, and the Ultraviolet Imaging of the Cosmic Assembly Near-infrared Deep Extragalactic Legacy Survey Fields (UVCANDELS 000; PI Teplitz).')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper71_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023MNRAS.519.4632D\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nWe also show the yields expected for stellar sources assuming a maximum population age of 000 Myr (computed as the difference between the Hubble time at z = 000.000, the measured redshift of COS-000, and the Hubble time at z = 000, assumed to be the onset redshift of star formation).')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "100 of 28950 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper87_mission0_HST_MENTION\n",
      "Bibcode: 2023AJ....165...91B\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST WFC3\\nTaken from this text snippet:\\nMost of the spectroscopic observations either during transit or secondary eclipse use space-based platforms like Hubble Space Telescope (HST) and Spitzer (and currently James Webb Space Telescope (JWST)) with low-to-moderate spectral resolution (R ∼ 000–000 or photometry-based instruments (primarily HST WFC3/Space Telescope Imaging Spectrograph (STIS) and Spitzer IRAC; Singetal 000; Guillotetal 000.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper92_mission0_HST_MENTION\n",
      "Bibcode: 2023ApJ...946...71C\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble sequence\\nTaken from this text snippet:\\nIntroduction The emergence of the Hubble sequence is one of the fundamental challenges of the hierarchical picture of galaxy assembly.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper105_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023ApJ...944...94T\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble parameter\\nTaken from this text snippet:\\nHence, a necessary (not sufficient) criterion a sample should satisfy is approximate constancy in the Hubble parameter for individual galaxies, H i = f i cz i / d i, averaged in velocity bins.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "125 of 28950 total texts have been processed...\n",
      "150 of 28950 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper128_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.1260S\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nprograms HST - GO-000\\nTaken from this text snippet:\\nWhile information about cycle 000 targets are listed in Authorsetal, a brief description of the main characteristics and respective discovery of the cycle 000 sample can be found below in Section 000.000. 000.000 Data and Data Reduction The observations of the lenses in our sample were taken by the Hubble Space Telescope under cycle 000 and cycle 000 programs HST-GO-000 and HST-GO-000 (PI: Treu), respectively, using the Wide Field Camera 000 (WFC3).')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper130_mission1_HST_SCIENCE\n",
      "Bibcode: 2023ApJ...944L..14W\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble compact clusters\\nTaken from this text snippet:\\nWhite circles show the clusters from the Hubble compact clusters catalog. “Historical” clusters are identified by the gold circles.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper134_mission0_HST_MENTION\n",
      "Bibcode: 2023MNRAS.518..305L\n",
      "Mission: HST\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble flow\\nTaken from this text snippet:\\nThis is assuming expansion purely due to the Hubble flow and H0 = 000 For a discussion on the differences among the various luminosity functions obtained with different techniques, see Authorsetal.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper148_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJ...943L..27F\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nSuch a high value is unlikely at high z because the time to grow such a Balmer break would be higher than the Hubble time (e.g, Maraston 000; Nolletal 000.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "175 of 28950 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper152_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJ...943..110S\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nThe demise of most stars in the universe that evolve in a Hubble time (i.e, in the 000–000 M ⊙ range) is believed to occur as a result of heavy mass loss (with rates up to 000 −000 M ⊙ yr −000 on the Asymptotic Giant Branch (AGB), when the stars are very luminous (L ∼ 000–000, 000 L ⊙) and cool (T eff 000 K) (see, e.g, the review by Decin 000.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 of 28950 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper179_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.2123Z\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nprogram number HST - HFnumeric - A\\nTaken from this text snippet:\\nFinally, PB was also partially supported through program number HST-HFnumeric-A, provided by NASA through a Hubble Fellowship grant from the Space Telescope Science Institute, under NASA contract NASnumeric.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "225 of 28950 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper200_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023MNRAS.518..456D\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble ’s constant\\nTaken from this text snippet:\\nIn total, 000 snapshots spaced in logarithmic intervals of growth factor from redshift z = 000 to z = 000 were produced using the Authorsetal ΛCDM cosmology with cosmological parameters: H0 = h × 100Mpc kms−000, h = 000.000; Ωm = 000.000, Ωb = 000.000 and ΩΛ = 000.000 being the Hubble’s constant, matter density, baryon density and Λ density respectively.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper202_mission0_HST_SCIENCE\n",
      "Bibcode: 2023ApJS..264...40M\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST WFC3 camera\\nTaken from this text snippet:\\nThe UVIS channel of the HST WFC3 camera has provided superior NUV images of several deep fields (e.g, the Hubble Ultraviolet Ultra Deep Field; Teplitzetal 000; the Hubble Deep UV Legacy Survey; Oeschetal 000; and the recent UVCANDELS survey; PI, H.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "250 of 28950 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper216_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.519.3749C\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST GO-000\\nTaken from this text snippet:\\nOngoing and future studies that explore these extremes, for example from the Hubble imaging Probe of Extreme Environments and Clusters (HiPEEC; Adamoetal 000 and the Clusters, Clumps, Dust, and Gas (CCDG; HST GO-000; PI: Chandar) projects, combined with the larger sample of spiral galaxies in the LEGUS (Calzettietal 000; Adamoetal, in preparation) and Physics at High Angular Resolution in Nearby Galaxies (PHANGS)- HST surveys, are needed to cover the full range of Σ SFR found in the nearby Universe.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper220_mission0_HST_SCIENCE\n",
      "Bibcode: 2023NewA...9901962J\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nhereafter HST\\nTaken from this text snippet:\\nThe ultraviolet spectra were acquired by STIS and GHRS spectrographs onboard Hubble Space Telescope (hereafter HST).')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "275 of 28950 total texts have been processed...\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper232_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023MNRAS.518.5953L\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble sequence\\nTaken from this text snippet:\\nThe most famous morphological classification scheme for galaxies is the Hubble sequence.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper242_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.5123M\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nGravitational wave radiation drives the loss of orbital angular momentum and causes many binary orbits to decay within a Hubble time, driving them to compact (orbital period of ≈000–000 min) configurations.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper245_mission0_HST_DATA_INFLUENCED\n",
      "Bibcode: 2023MNRAS.521..662N\n",
      "Mission: HST\n",
      "Masked class: data_influenced\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST)/WFC3\\nTaken from this text snippet:\\n000 INTRODUCTION Only one spectroscopically confirmed very high redshift (z ≥ 000 galaxy is known to date, dubbed GN-z11, at redshift, when the age of the Universe was ≃000 website-z11 has been identified as a bright (M uv ≈ −000 mag), massive (stellar mass M * ≃ 000 000 M ⊙) Lyman break galaxy (LBG) in observations with the Hubble Space Telescope (HST)/WFC3/infrared and Spitzer /IRAC instruments.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper249_mission0_HST_SCIENCE\n",
      "Bibcode: 2023MNRAS.518.4579P\n",
      "Mission: HST\n",
      "Masked class: science\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nMoreover, the extremely low mass for the white dwarf 000.000 ± 000.000 M) must be the result of binary interaction, since a single star could not produce such a low mass white dwarf within a Hubble time, making it unlikely that the white dwarf is a distant companion to a binary.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n",
      "\n",
      "-\n",
      "Printing Error:\n",
      "ID: paper249_mission1_KEPLER_MENTION\n",
      "Bibcode: 2023MNRAS.518.4579P\n",
      "Mission: KEPLER\n",
      "Masked class: mention\n",
      "The following err. was encountered in train_model_ML:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nKepler ’s third law\\nTaken from this text snippet:\\nWith the masses of both stellar components in the binary as well as the orbital fits we can now determine the semi-major axis, a, using Kepler’s third law: G (MWD + MSG)Porb, 000 4π 000 where G is the gravitational constant.')\n",
      "Error was noted. Skipping this paper.\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "#Use the Operator instance to train an ML model\n",
    "start=time.time()\n",
    "str_err = tabby_ML.train_model_ML(dir_model=dir_model, name_model=name_model, do_reuse_run=do_reuse_run,\n",
    "                        do_check_truematch=do_check_truematch,\n",
    "                        seed_ML=seed_ML, seed_TVT=seed_TVT, dict_texts=dict_texts, mapper=mapper,\n",
    "                        buffer=buffer, fraction_TVT=fraction_TVT, mode_TVT=mode_TVT, do_shuffle=do_shuffle,\n",
    "                        do_verbose=True, do_verbose_deep=False)\n",
    "\n",
    "print(f'Time to train the model with run = {time.time()-start} seconds.')\n",
    "\n",
    "#Save the output error string to a file\n",
    "if (str_err is not None):\n",
    "    with open(filesave_error, 'x') as openfile:\n",
    "        openfile.write(str_err)\n",
    "#\n",
    "\n",
    "#Save the unused bibcodes to a file\n",
    "if ((not do_reuse_run) or (not os.path.exists(filesave_unused_bibcodes))):\n",
    "    np.save(filesave_unused_bibcodes, dict_unused_indsandbibcodes)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df664592",
   "metadata": {},
   "source": [
    "And with that, we're done training a new ML model!  If run successfully, the model will be saved in the `dir_model` directory.\n",
    "\n",
    "We can then use the brand new model to classify some new text, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a002238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set path to the new model output\n",
    "filepath_model = os.path.join(dir_model, (name_model+\".npy\"))\n",
    "fileloc_ML = os.path.join(dir_model, (config.tfoutput_prefix+name_model))\n",
    "#Load the new ML model into a new Classifier_ML instance\n",
    "classifier_ML = bibcat.Classifier_ML(filepath_model=filepath_model, fileloc_ML=fileloc_ML,\n",
    "                                    do_verbose=True)\n",
    "#\n",
    "#Load the instance into a new Operator\n",
    "tabby_ML = bibcat.Operator(classifier=classifier_ML, mode=mode_modif, keyword_objs=all_kobjs,\n",
    "                           do_verbose=True, load_check_truematch=True, do_verbose_deep=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the classifier for some sample text below\n",
    "lookup = \"HST\"\n",
    "text = \"In this study, we present our lovely HST observations of bright stars in the nearby star-forming region Taurus.\"\n",
    "threshold = 0.8\n",
    "#\n",
    "#Run the classifier\n",
    "result = tabby_ML.classify(text=text, lookup=lookup, buffer=0, #threshold=threshold,\n",
    "                            do_raise_innererror=False, do_check_truematch=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ba8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the classifier results\n",
    "print(\"Modif: {2}\\n\\nClassification: {0}\\n\\nUncertainties per class: {1}\\n\"\n",
    "      .format(result[\"verdict\"], result[\"uncertainty\"], result[\"modif\"]))\n",
    "print(\"Full classification output:\\n{0}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2813bea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab14be5-db76-4745-82f1-829dc861df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set end marker for this tutorial.\n",
    "print(\"This tutorial completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
