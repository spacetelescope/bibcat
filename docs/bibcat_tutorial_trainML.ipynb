{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71ff86",
   "metadata": {},
   "source": [
    "# Bibliography Categorization: 'BibCat'\n",
    "## Tutorial: Machine learning (ML) models in bibcat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ab7e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234a479",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction.\n",
    "\n",
    "In this tutorial, we will use bibcat to train a machine learning (ML) model on some raw input text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f050727",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86289680",
   "metadata": {},
   "source": [
    "## User Workflow: Training a machine learning (ML) model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af236d7a",
   "metadata": {},
   "source": [
    "The `Operator` class contains a user-friendly method `train_model_ML` that runs the full workflow for training an ML model, from the input raw text all the way to saving the output ML model.  We overview how this method can be run in the code blocks below.\n",
    "\n",
    "For this tutorial, we have two sets of data: either 1) some short, made-up text for a quick run of the code, or 2) an imported database of text from an external file of the user's choosing. The former case is useful for getting a quick sense of how the code works. The latter case is useful for building an actual model, but of course will take much longer on larger databases of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1149ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdea077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import external packages\n",
    "import os\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for fetching necessary bibcat modules for the tutorial\n",
    "# Check work directories: src/ is where all source python scripts are available. \n",
    "current_dir= os.path.dirname(os.path.abspath('__file__'))\n",
    "_parent = os.path.dirname(current_dir)\n",
    "src_dir = os.path.join(_parent, \"src\")\n",
    "\n",
    "print(f'Current Directory: {current_dir}')\n",
    "print(f'Source directory: {src_dir}')\n",
    "\n",
    "# move to the ../src/ directory to import necessary modules. \n",
    "os.chdir(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8643e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import bibcat packages\n",
    "import bibcat_classes as bibcat\n",
    "import bibcat_config as config\n",
    "import bibcat_parameters as params #Temporary file until contents moved elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which data would you like to run the ML model on?  Choose from the booleans below.\n",
    "do_quick_run = False #This will train the ML model on short bits of text. Runs pretty quickly.\n",
    "do_real_run = True #This will train the ML model on external text. Will take longer for larger databases.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ced33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The rest of these parameters can be left as-is for a first run-through.\n",
    "#\n",
    "do_check_truematch = True #A very important parameter - discuss with J.P. first!!!  Set it to either True or False.\n",
    "#If any papers in dataset encountered within the codebase that have unknown ambiguous phrases...\n",
    "#...then a note will be printed and those papers will not be used for training-validation-testing.\n",
    "#...Add the identified ambiguous phrase to the external ambiguous phrase database and rerun to include those papers.\n",
    "#\n",
    "num_papers = 500 #500 #None, or an integer; if an integer, will truncate external .json text dataset to this size\n",
    "#Set num_papers=None to use all available papers in external dataset\n",
    "#Note: If set to integer, final paper count might be a little more than target num_papers given\n",
    "#\n",
    "allowed_classifications = params.allowed_classifications #For external data; classifications to include\n",
    "mapper = params.map_papertypes #For masking of classes (e.g., masking 'supermention' as 'mention')\n",
    "#\n",
    "\n",
    "#Fetch filepaths for model and data\n",
    "name_model = config.name_model\n",
    "filepath_json = config.path_json\n",
    "dir_model = os.path.join(config.dir_allmodels, name_model)\n",
    "filesave_error = os.path.join(dir_model,\n",
    "                              \"{0}_processing_errors.txt\".format(name_model)) #Where to save processing errors\n",
    "#\n",
    "#Set values for generating ML model\n",
    "do_reuse_run = True #Whether or not to reuse any existing output from previous training+validation+testing runs\n",
    "do_shuffle = True #Whether or not to shuffle contents of training vs validation vs testing datasets\n",
    "fraction_TVT = [0.8, 0.1, 0.1] #Fractional breakdown of training vs validation vs testing datasets\n",
    "#\n",
    "mode_TVT = \"uniform\" # \"uniform\" #\"available\"\n",
    "#\"uniform\" = all training datasets will have the same number of entries\n",
    "#\"available\" = all training datasets will use full fraction (from fraction_TVT) of data available\n",
    "#\n",
    "seed_TVT = 10 #Random seed for generating training vs validation vs testing datasets\n",
    "seed_ML = 8 #Random seed for ML model\n",
    "mode_modif = \"skim_anon\" #\"skim_trim_anon\" #Mode to use for processing and generating modifs from input raw text\n",
    "#NOTE: See other modif modes in workflow tutorial\n",
    "buffer = 0\n",
    "#\n",
    "all_kobjs = params.all_kobjs\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97faa2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize an empty ML classifier\n",
    "classifier_ML = bibcat.Classifier_ML(filepath_model=None, fileloc_ML=None, do_verbose=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize an Operator\n",
    "tabby_ML = bibcat.Operator(classifier=classifier_ML, mode=mode_modif, keyword_objs=all_kobjs,\n",
    "                           do_verbose=True, load_check_truematch=do_check_truematch, do_verbose_deep=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up data for the quick example case. But in reality, ML models should be trained on MUCH more data than this!!!\n",
    "if do_quick_run:\n",
    "    #Make some fake data\n",
    "    dict_texts_raw = {\"science\":[\"We present HST observations in Figure 4.\",\n",
    "                            \"The HST stars are listed in Table 3b.\",\n",
    "                            \"Despite our efforts to smooth the data, there are still rings in the HST images.\",\n",
    "                            \"See Section 8c for more discussion of the Hubble images.\",\n",
    "                            \"The supernovae detected with HST tend to be brighter than initially predicted.\",\n",
    "                            \"Our spectra from HST fit well to the standard trend first published in Someone et al. 1990.\",\n",
    "                            \"We use the Hubble Space Telescope to build an ultraviolet database of the target stars.\",\n",
    "                            \"The blue points (HST) exhibit more scatter than the red points (JWST).\",\n",
    "                            \"The benefit, then, is the far higher S/N we achieved in our HST observations.\",\n",
    "                            \"Here we employ the Hubble Telescope to observe the edge of the photon-dominated region.\",\n",
    "                            \"The black line shows that the region targeted with Hubble has an extreme UV signature.\"],\n",
    "                     \"datainfluenced\":[\"The simulated Hubble data is plotted in Figure 4.\",\n",
    "                           \"Compared to the HST observations in Someone et al., our JWST follow-up reached higher S/N.\",\n",
    "                           \"We were able to reproduce the luminosities from Hubble using our latest models.\",\n",
    "                           \"We overplot Hubble-observed stars from Someone et al. in Figure 3b.\",\n",
    "                           \"We built the spectral templates using UV data in the Hubble archive.\",\n",
    "                           \"We simulate what our future HST observations will look like to predict the S/N.\",\n",
    "                           \"Our work here with JWST is inspired by our earlier HST study published in 2010.\",\n",
    "                           \"We therefore use the Hubble statistics from Author et al. to guide our stellar predictions.\",\n",
    "                           \"The stars in Figure 3 were plotted based on the HST-fitted trend line in Person et al.\",\n",
    "                           \"The final step is to use the HST exposure tool to put our modeled images in context.\"],\n",
    "                     \"mention\":[\"Person et al. used HST to measure the Hubble constant.\",\n",
    "                            \"We will present new HST observations in a future work.\",\n",
    "                            \"HST is do_a fantastic instrument that has revolutionized our view of space.\",\n",
    "                            \"The Hubble Space Telescope (HST) has its mission center at the STScI.\",\n",
    "                            \"We can use HST to power a variety of science in the ultraviolet regime.\",\n",
    "                            \"It is not clear when the star will be observable with HST.\",\n",
    "                            \"More data can be found and downloaded from the Hubble archive.\",\n",
    "                            \"We note that HST can be used to observe the stars as well, at higher S/N.\",\n",
    "                            \"However, we ended up using the JWST rather than HST observations in this work.\",\n",
    "                            \"We push the analysis of the Hubble component of the dataset to a future study.\",\n",
    "                            \"We expect the HST observations to be released in the fall.\",\n",
    "                            \"We look forward to any follow-up studies with, e.g., the Hubble Telescope.\"]}\n",
    "    #\n",
    "    #Convert into dictionary with: key:text,class,id,mission structure\n",
    "    i_track = 0\n",
    "    dict_texts = {}\n",
    "    for key in dict_texts_raw:\n",
    "        curr_set = dict_texts_raw[key]\n",
    "        for ii in range(0, len(curr_set)):\n",
    "            dict_texts[str(i_track)] = {\"text\":curr_set[ii], \"class\":key, \"id\":\"{0}_{1}\".format(key, ii),\n",
    "                                       \"mission\":\"HST\", \"bibcode\":\"{0}_{1}\".format(key, ii)}\n",
    "            i_track += 1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up data for the external data case.\n",
    "if do_real_run:\n",
    "    #Load the original data\n",
    "    with open(filepath_json, 'r') as openfile:\n",
    "        dataset = json.load(openfile)\n",
    "        len(dataset)\n",
    "    #\n",
    "    #Initialize holder to keep track of bibcodes used (avoids duplicate dataset entries)\n",
    "    list_bibcodes = []\n",
    "    #\n",
    "    #Organize a new version of the data with: key:text,class,id,mission structure\n",
    "    i_track = 0 #Track number of papers kept from original dataset\n",
    "    dict_texts = {}\n",
    "    for ii in range(0, len(dataset)):\n",
    "        #Extract mission classifications for current text\n",
    "        curr_data = dataset[ii]\n",
    "        #\n",
    "        #Skip if no valid text at all for this text\n",
    "        if (\"body\" not in curr_data):\n",
    "            continue\n",
    "        #\n",
    "        #Skip if no valid missions at all for this text\n",
    "        if (\"class_missions\" not in curr_data):\n",
    "            continue\n",
    "        #\n",
    "        #Otherwise, extract the bibcodes and missions\n",
    "        curr_bibcode = curr_data[\"bibcode\"]\n",
    "        curr_missions = curr_data[\"class_missions\"]\n",
    "        print(curr_bibcode)\n",
    "        print(curr_missions)\n",
    "        #\n",
    "        #Skip if bibcode already encountered (and so duplicate entry)\n",
    "        if (curr_bibcode in list_bibcodes):\n",
    "            print(\"Duplicate bibcode encountered: {0}. Skipping.\".format(curr_bibcode))\n",
    "            continue\n",
    "        #\n",
    "        #Iterate through missions for this text\n",
    "        i_mission = 0\n",
    "        for curr_key in curr_missions:\n",
    "            #If this is not an allowed mission, skip\n",
    "            if (curr_missions[curr_key][\"papertype\"] not in allowed_classifications):\n",
    "                continue\n",
    "            #\n",
    "            #Otherwise, check if this mission is a target mission\n",
    "            fetched_kobj = tabby_ML._fetch_keyword_object(lookup=curr_key,\n",
    "                                                          do_verbose=False, do_raise_emptyerror=False)\n",
    "            #Skip if not a target\n",
    "            if (fetched_kobj is None):\n",
    "                continue\n",
    "            #\n",
    "            #Otherwise, store classification info for this entry\n",
    "            curr_class = curr_missions[curr_key][\"papertype\"]\n",
    "            new_dict = {\"text\":curr_data[\"body\"], #Text for this paper\n",
    "                        \"bibcode\":curr_data[\"bibcode\"], #Bibcode for this paper\n",
    "                        \"class\":curr_class, #Classification for this mission\n",
    "                        \"mission\":curr_key, #The mission itself\n",
    "                        \"id\":(\"paper{0}_mission{1}_{2}_{3}\".format(ii, i_mission,\n",
    "                                                                   curr_key, curr_class)) #ID for this entry\n",
    "                       }\n",
    "            dict_texts[str(i_track)] = new_dict\n",
    "            #\n",
    "            #Increment counters\n",
    "            i_mission += 1 #Count of kept missions for this paper\n",
    "            i_track += 1 #Count of kept classifications overall\n",
    "        #\n",
    "        \n",
    "        #Record this bibcode as stored\n",
    "        list_bibcodes.append(curr_bibcode)\n",
    "\n",
    "        #Terminate early if requested number of papers reached\n",
    "        if ((num_papers is not None) and (i_track >= num_papers)):\n",
    "            break\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af23be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throw error if not enough text entries collected\n",
    "if do_real_run:\n",
    "    if ((num_papers is not None) and (len(dict_texts) < num_papers)):\n",
    "        raise ValueError(\"Err: Something went wrong during initial processing. Insufficient number of texts extracted.\"\n",
    "                        +\"\\nRequested number of texts: {0}\\nActual number of texts: {1}\"\n",
    "                        .format(num_papers, len(dict_texts)))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the code below to print a snippet of each of the entries in the dataset.\n",
    "#\"\"\"\n",
    "print(\"Number of processed texts: {0}={1}\\n\".format(i_track, len(dict_texts)))\n",
    "for curr_key in dict_texts:\n",
    "    print(\"Text #{0}:\".format(curr_key))\n",
    "    print(\"Classification: {0}\".format(dict_texts[curr_key][\"class\"]))\n",
    "    print(\"Mission: {0}\".format(dict_texts[curr_key][\"mission\"]))\n",
    "    print(\"ID: {0}\".format(dict_texts[curr_key][\"id\"]))\n",
    "    print(\"Bibcode: {0}\".format(dict_texts[curr_key][\"bibcode\"]))\n",
    "    print(\"Text snippet:\")\n",
    "    print(dict_texts[curr_key][\"text\"][0:500])\n",
    "    print(\"---\\n\\n\")\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print number of texts that fell under given parameters\n",
    "print(\"Target missions:\")\n",
    "for curr_kobj in all_kobjs:\n",
    "    print(curr_kobj)\n",
    "    print(\"\")\n",
    "print(\"\")\n",
    "print(\"Number of valid text entries:\")\n",
    "print(len(dict_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42320d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use the Operator instance to train an ML model\n",
    "start=time.time()\n",
    "str_err = tabby_ML.train_model_ML(dir_model=dir_model, name_model=name_model, do_reuse_run=do_reuse_run,\n",
    "                        do_check_truematch=do_check_truematch,\n",
    "                        seed_ML=seed_ML, seed_TVT=seed_TVT, dict_texts=dict_texts, mapper=mapper,\n",
    "                        buffer=buffer, fraction_TVT=fraction_TVT, mode_TVT=mode_TVT, do_shuffle=do_shuffle,\n",
    "                        do_verbose=True, do_verbose_deep=False)\n",
    "\n",
    "print(f'Time to train the model with run = {time.time()-start} seconds.')\n",
    "\n",
    "#Save the output error string to a file\n",
    "with open(filesave_error, 'x') as openfile:\n",
    "    openfile.write(str_err)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df664592",
   "metadata": {},
   "source": [
    "And with that, we're done training a new ML model!  If run successfully, the model will be saved in the `dir_model` directory.\n",
    "\n",
    "We can then use the brand new model to classify some new text, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a002238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set path to the new model output\n",
    "filepath_model = os.path.join(dir_model, (name_model+\".npy\"))\n",
    "fileloc_ML = os.path.join(dir_model, (config.tfoutput_prefix+name_model))\n",
    "#Load the new ML model into a new Classifier_ML instance\n",
    "classifier_ML = bibcat.Classifier_ML(filepath_model=filepath_model, fileloc_ML=fileloc_ML,\n",
    "                                    do_verbose=True)\n",
    "#\n",
    "#Load the instance into a new Operator\n",
    "tabby_ML = bibcat.Operator(classifier=classifier_ML, mode=mode_modif, keyword_objs=all_kobjs,\n",
    "                           do_verbose=True, load_check_truematch=True, do_verbose_deep=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the classifier for some sample text below\n",
    "lookup = \"HST\"\n",
    "text = \"In this study, we present our lovely HST observations of bright stars in the nearby star-forming region Taurus.\"\n",
    "threshold = 0.8\n",
    "#\n",
    "#Run the classifier\n",
    "result = tabby_ML.classify(text=text, lookup=lookup, buffer=0, threshold=threshold,\n",
    "                            do_raise_innererror=False, do_check_truematch=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ba8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the classifier results\n",
    "print(\"Modif: {2}\\n\\nClassification: {0}\\n\\nUncertainties per class: {1}\\n\"\n",
    "      .format(result[\"verdict\"], result[\"uncertainty\"], result[\"modif\"]))\n",
    "print(\"Full classification output:\\n{0}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2813bea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab14be5-db76-4745-82f1-829dc861df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set end marker for this tutorial.\n",
    "print(\"This tutorial completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
