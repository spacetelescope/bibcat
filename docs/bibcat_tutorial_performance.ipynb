{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71ff86",
   "metadata": {},
   "source": [
    "# Bibliography Categorization: 'BibCat'\n",
    "## Tutorial: Estimating performance of classifiers in bibcat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ab7e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234a479",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction.\n",
    "\n",
    "In this tutorial, we will use bibcat to estimate the performance of classifiers on sets of texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f050727",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86289680",
   "metadata": {},
   "source": [
    "## User Workflow: Training a machine learning (ML) model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af236d7a",
   "metadata": {},
   "source": [
    "The `Performance` class contains user-friendly methods for estimating the performance of given classifiers and outputting that performance as, e.g., confusion matrices.  We overview how this method can be run in the code blocks below.\n",
    "\n",
    "For this tutorial, we assume that the user has already run the trainML tutorial, and so has generated and saved a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdea077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 04:59:31.623788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Import external packages\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "sys.path.append(\"./../main/\")\n",
    "#\n",
    "#Import bibcat packages\n",
    "import bibcat_classes as bibcat\n",
    "import bibcat_config as config\n",
    "import bibcat_constants as preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710913d3-0a11-4cda-8971-1b845d542fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch filepaths for model and data\n",
    "name_model = config.name_model\n",
    "filepath_json = config.path_json\n",
    "dir_model = os.path.join(config.dir_allmodels, name_model)\n",
    "#\n",
    "#Set directories for storing performance output\n",
    "filepath_output = dir_model #Where to store the performance output, such as the confusion matrices\n",
    "fileroot_evaluation = \"test_eval\" #Root name of the file within which to store the performance evaluation output\n",
    "fileroot_misclassif = \"test_misclassif\" #Root name of the file within which to store misclassified text information\n",
    "#\n",
    "#Set directories for fetching text\n",
    "dir_info = dir_model\n",
    "dir_test = os.path.join(dir_model, \"dir_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e0a06e-61fc-438a-b642-3669910f9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set some global variables\n",
    "seed_test = 10 #Random seed for shuffling text dataset\n",
    "do_shuffle = True #Whether or not to shuffle the text dataset\n",
    "max_tests = 100 #Number of text entries to test the performance for; None for all tests available\n",
    "is_text_processed = False #We are using preprocessed text for this tutorial (previously generated by trainML in a test set directory)\n",
    "mode_modif = \"skim_trim_anon\" #None #We are using preprocessed data in this tutorial, so we do not need a processing mode at all\n",
    "buffer = 0\n",
    "#\n",
    "#Prepare some Keyword objects\n",
    "kobj_hubble = bibcat.Keyword(\n",
    "                keywords=[\"Hubble\", \"Hubble Telescope\",\n",
    "                          \"Hubble Space Telescope\"],\n",
    "                acronyms=[\"hst\", \"ht\"])\n",
    "all_kobjs = [kobj_hubble]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188dbd8-388d-4f8e-95e9-ebede99435c2",
   "metadata": {},
   "source": [
    "Let's build a set of classifiers for which we'd like to test the performance.  We'll then feed each of those classifiers into an instance of the Operator class to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116fd35a-994f-452c-87dc-233733ab8749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#Create a list of classifiers\n",
    "#This can be modified to use whatever classifiers you'd like.\n",
    "#Load a previously trained ML model\n",
    "filepath_model = os.path.join(dir_model, (name_model+\".npy\"))\n",
    "fileloc_ML = os.path.join(dir_model, (preset.tfoutput_prefix+name_model))\n",
    "classifier_ML = bibcat.Classifier_ML(filepath_model=filepath_model, fileloc_ML=fileloc_ML, do_verbose=True)\n",
    "#\n",
    "\n",
    "#Load a rule-based classifier\n",
    "classifier_rules = bibcat.Classifier_Rules()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4a36a1-e8cc-4576-8aab-904f61813988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance of Operator successfully initialized!\n",
      "Keyword objects:\n",
      "0: Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['hst', 'ht']\n",
      "\n",
      "Instance of Operator successfully initialized!\n",
      "Keyword objects:\n",
      "0: Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['hst', 'ht']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load models into instances of the Operator class\n",
    "operator_1 = bibcat.Operator(classifier=classifier_ML, mode=mode_modif, keyword_objs=all_kobjs,\n",
    "                            name=\"Operator_1\", do_verbose=True, load_check_truematch=True, do_verbose_deep=False)\n",
    "operator_2 = bibcat.Operator(classifier=classifier_rules,\n",
    "                            name=\"Operator_2\", mode=mode_modif, keyword_objs=all_kobjs, do_verbose=True, do_verbose_deep=False)\n",
    "list_operators = [operator_1, operator_2] #Feel free to add more/less operators here.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82951251-fc59-4e33-a978-bc8fa16915dd",
   "metadata": {},
   "source": [
    "Now, let's fetch some text for our classifiers to classify. For this tutorial, we'll load previously processed texts from the directory containing the test set for the ML classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8075c1f-bc7b-422e-89cc-82fac058bb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Load information for the processed text\\ndict_allinfo = np.load(os.path.join(dir_info, \"dict_textinfo.npy\"), allow_pickle=True).item()\\n\\n#Prepare filepaths for each class directory of text\\nlist_subdirs = os.listdir(dir_test)\\nlist_joinedfilenames = [os.path.join(item1, item2) for item1 in list_subdirs\\n                  for item2 in os.listdir(os.path.join(dir_test, item1)) if item2.endswith(\".txt\")]\\n\\n#Shuffle the tests, if so requested\\nif do_shuffle:\\n    np.random.seed(seed_test)\\n    np.random.shuffle(list_joinedfilenames)\\n#\\n\\n#Truncate the number of tests, if so requested\\nif (max_tests is not None):\\n    list_joinedfilenames = list_joinedfilenames[0:max_tests]\\n#\\n\\ndict_texts = {}\\n#Process the tests into a dictionary of texts\\nfor ii in range(0, len(list_joinedfilenames)):\\n    curr_filename = list_joinedfilenames[ii]\\n    curr_fileroot = re.sub(\"\\\\.txt$\", \"\", curr_filename.split(\"/\")[1]) #Remove extension\\n    curr_info = dict_allinfo[curr_fileroot]\\n\\n    #Load the text from this file\\n    with open(os.path.join(dir_test, curr_filename), \\'r\\') as openfile:\\n        curr_text = openfile.read()\\n    #\\n    \\n    #Store info for this current text entry\\n    curr_dict = {\"text\":curr_text, \"mission\":curr_info[\"mission\"], \"forest\":curr_info[\"forest\"],\\n                \"class\":curr_info[\"class\"], \"id\":curr_info[\"id\"]}\\n    dict_texts[str(ii)] = curr_dict\\n#\\n#Print some information about the sets of texts\\nprint(\"Texts were pulled from: {0}\".format(dir_test))\\nprint(\"Number of valid .txt files in this directory: {0}\".format(len(list_joinedfilenames)))\\nprint(\"Number of texts in text set: {0}\".format(len(dict_texts)))\\n#'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Below code blocked - need to instead split papers themselves into TVT, rather than paragraphs into TVT...\n",
    "#...otherwise, cannot reprocess each paper (because single paper could contain both training and test data, for example)\n",
    "\"\"\"\n",
    "#Load information for the processed text\n",
    "dict_allinfo = np.load(os.path.join(dir_info, \"dict_textinfo.npy\"), allow_pickle=True).item()\n",
    "\n",
    "#Prepare filepaths for each class directory of text\n",
    "list_subdirs = os.listdir(dir_test)\n",
    "list_joinedfilenames = [os.path.join(item1, item2) for item1 in list_subdirs\n",
    "                  for item2 in os.listdir(os.path.join(dir_test, item1)) if item2.endswith(\".txt\")]\n",
    "\n",
    "#Shuffle the tests, if so requested\n",
    "if do_shuffle:\n",
    "    np.random.seed(seed_test)\n",
    "    np.random.shuffle(list_joinedfilenames)\n",
    "#\n",
    "\n",
    "#Truncate the number of tests, if so requested\n",
    "if (max_tests is not None):\n",
    "    list_joinedfilenames = list_joinedfilenames[0:max_tests]\n",
    "#\n",
    "\n",
    "dict_texts = {}\n",
    "#Process the tests into a dictionary of texts\n",
    "for ii in range(0, len(list_joinedfilenames)):\n",
    "    curr_filename = list_joinedfilenames[ii]\n",
    "    curr_fileroot = re.sub(\"\\.txt$\", \"\", curr_filename.split(\"/\")[1]) #Remove extension\n",
    "    curr_info = dict_allinfo[curr_fileroot]\n",
    "\n",
    "    #Load the text from this file\n",
    "    with open(os.path.join(dir_test, curr_filename), 'r') as openfile:\n",
    "        curr_text = openfile.read()\n",
    "    #\n",
    "    \n",
    "    #Store info for this current text entry\n",
    "    curr_dict = {\"text\":curr_text, \"mission\":curr_info[\"mission\"], \"forest\":curr_info[\"forest\"],\n",
    "                \"class\":curr_info[\"class\"], \"id\":curr_info[\"id\"]}\n",
    "    dict_texts[str(ii)] = curr_dict\n",
    "#\n",
    "#Print some information about the sets of texts\n",
    "print(\"Texts were pulled from: {0}\".format(dir_test))\n",
    "print(\"Number of valid .txt files in this directory: {0}\".format(len(list_joinedfilenames)))\n",
    "print(\"Number of texts in text set: {0}\".format(len(dict_texts)))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2194a016-51fc-47fc-a756-7540e5674774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts in text set: 33\n",
      "\n",
      "{'text': 'We present HST observations in Figure 4.', 'id': 'science_0', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'The HST stars are listed in Table 3b.', 'id': 'science_1', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'Despite our efforts to smooth the data, there are still rings in the HST images.', 'id': 'science_2', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'See Section 8c for more discussion of the Hubble images.', 'id': 'science_3', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'The supernovae detected with HST tend to be brighter than initially predicted.', 'id': 'science_4', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'Our spectra from HST fit well to the standard trend first published in Someone et al. 1990.', 'id': 'science_5', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'We use the Hubble Space Telescope to build an ultraviolet database of the target stars.', 'id': 'science_6', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'The blue points (HST) exhibit more scatter than the red points (JWST).', 'id': 'science_7', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'The benefit, then, is the far higher S/N we achieved in our HST observations.', 'id': 'science_8', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'Here we employ the Hubble Telescope to observe the edge of the photon-dominated region.', 'id': 'science_9', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'The black line shows that the region targeted with Hubble has an extreme UV signature.', 'id': 'science_10', 'missions': {'mission': 'HST', 'class': 'science'}}\n",
      "-\n",
      "{'text': 'The simulated Hubble data is plotted in Figure 4.', 'id': 'datainfluenced_0', 'missions': {'mission': 'HST', 'class': 'datainfluenced'}}\n",
      "-\n",
      "{'text': 'Compared to the HST observations in Someone et al., our JWST follow-up reached higher S/N.', 'id': 'datainfluenced_1', 'missions': {'mission': 'HST', 'class': 'datainfluenced'}}\n",
      "-\n",
      "{'text': 'We were able to reproduce the luminosities from Hubble using our latest models.', 'id': 'datainfluenced_2', 'missions': {'mission': 'HST', 'class': 'datainfluenced'}}\n",
      "-\n",
      "{'text': 'We overplot Hubble-observed stars from Someone et al. in Figure 3b.', 'id': 'datainfluenced_3', 'missions': {'mission': 'HST', 'class': 'datainfluenced'}}\n",
      "-\n",
      "{'text': 'We built the spectral templates using UV data in the Hubble archive.', 'id': 'datainfluenced_4', 'missions': {'mission': 'HST', 'class': 'datainfluenced'}}\n",
      "-\n",
      "{'text': 'We simulate what our future HST observations will look like to predict the S/N.', 'id': 'datainfluenced_5', 'missions': {'mission': 'HST', 'class': 'datainfluenced'}}\n",
      "-\n",
      "{'text': 'Our work here with JWST is inspired by our earlier HST study published in 2010.', 'id': 'datainfluenced_6', 'missions': {'mission': 'HST', 'class': 'datainfluenced'}}\n",
      "-\n",
      "{'text': 'We therefore use the Hubble statistics from Author et al. to guide our stellar predictions.', 'id': 'datainfluenced_7', 'missions': {'mission': 'HST', 'class': 'datainfluenced'}}\n",
      "-\n",
      "{'text': 'The stars in Figure 3 were plotted based on the HST-fitted trend line in Person et al.', 'id': 'datainfluenced_8', 'missions': {'mission': 'HST', 'class': 'datainfluenced'}}\n",
      "-\n",
      "{'text': 'The final step is to use the HST exposure tool to put our modeled images in context.', 'id': 'datainfluenced_9', 'missions': {'mission': 'HST', 'class': 'datainfluenced'}}\n",
      "-\n",
      "{'text': 'Person et al. used HST to measure the Hubble constant.', 'id': 'mention_0', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'We will present new HST observations in a future work.', 'id': 'mention_1', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'HST is a fantastic instrument that has revolutionized our view of space.', 'id': 'mention_2', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'The Hubble Space Telescope (HST) has its mission center at the STScI.', 'id': 'mention_3', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'We can use HST to power a variety of science in the ultraviolet regime.', 'id': 'mention_4', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'It is not clear when the star will be observable with HST.', 'id': 'mention_5', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'More data can be found and downloaded from the Hubble archive.', 'id': 'mention_6', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'We note that HST can be used to observe the stars as well, at higher S/N.', 'id': 'mention_7', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'However, we ended up using the JWST rather than HST observations in this work.', 'id': 'mention_8', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'We push the analysis of the Hubble component of the dataset to a future study.', 'id': 'mention_9', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'We expect the HST observations to be released in the fall.', 'id': 'mention_10', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n",
      "{'text': 'We look forward to any follow-up studies with, e.g., the Hubble Telescope.', 'id': 'mention_11', 'missions': {'mission': 'HST', 'class': 'mention'}}\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "#!!!!!\n",
    "#Borrowing fake data from trainML tutorial, just for now, to get everything running\n",
    "#Make some fake data\n",
    "dict_texts_raw = {\"science\":[\"We present HST observations in Figure 4.\",\n",
    "                        \"The HST stars are listed in Table 3b.\",\n",
    "                        \"Despite our efforts to smooth the data, there are still rings in the HST images.\",\n",
    "                        \"See Section 8c for more discussion of the Hubble images.\",\n",
    "                        \"The supernovae detected with HST tend to be brighter than initially predicted.\",\n",
    "                        \"Our spectra from HST fit well to the standard trend first published in Someone et al. 1990.\",\n",
    "                        \"We use the Hubble Space Telescope to build an ultraviolet database of the target stars.\",\n",
    "                        \"The blue points (HST) exhibit more scatter than the red points (JWST).\",\n",
    "                        \"The benefit, then, is the far higher S/N we achieved in our HST observations.\",\n",
    "                        \"Here we employ the Hubble Telescope to observe the edge of the photon-dominated region.\",\n",
    "                        \"The black line shows that the region targeted with Hubble has an extreme UV signature.\"],\n",
    "                 \"datainfluenced\":[\"The simulated Hubble data is plotted in Figure 4.\",\n",
    "                       \"Compared to the HST observations in Someone et al., our JWST follow-up reached higher S/N.\",\n",
    "                       \"We were able to reproduce the luminosities from Hubble using our latest models.\",\n",
    "                       \"We overplot Hubble-observed stars from Someone et al. in Figure 3b.\",\n",
    "                       \"We built the spectral templates using UV data in the Hubble archive.\",\n",
    "                       \"We simulate what our future HST observations will look like to predict the S/N.\",\n",
    "                       \"Our work here with JWST is inspired by our earlier HST study published in 2010.\",\n",
    "                       \"We therefore use the Hubble statistics from Author et al. to guide our stellar predictions.\",\n",
    "                       \"The stars in Figure 3 were plotted based on the HST-fitted trend line in Person et al.\",\n",
    "                       \"The final step is to use the HST exposure tool to put our modeled images in context.\"],\n",
    "                 \"mention\":[\"Person et al. used HST to measure the Hubble constant.\",\n",
    "                        \"We will present new HST observations in a future work.\",\n",
    "                        \"HST is a fantastic instrument that has revolutionized our view of space.\",\n",
    "                        \"The Hubble Space Telescope (HST) has its mission center at the STScI.\",\n",
    "                        \"We can use HST to power a variety of science in the ultraviolet regime.\",\n",
    "                        \"It is not clear when the star will be observable with HST.\",\n",
    "                        \"More data can be found and downloaded from the Hubble archive.\",\n",
    "                        \"We note that HST can be used to observe the stars as well, at higher S/N.\",\n",
    "                        \"However, we ended up using the JWST rather than HST observations in this work.\",\n",
    "                        \"We push the analysis of the Hubble component of the dataset to a future study.\",\n",
    "                        \"We expect the HST observations to be released in the fall.\",\n",
    "                        \"We look forward to any follow-up studies with, e.g., the Hubble Telescope.\"]}\n",
    "#\n",
    "#Convert into dictionary with: key:text,class,id,mission structure\n",
    "i_track = 0\n",
    "dict_texts = {}\n",
    "mission = \"HST\" #Store subheadings by mission, to avoid duplicating and processing the same text across different missions\n",
    "for key in dict_texts_raw:\n",
    "    curr_set = dict_texts_raw[key]\n",
    "    for ii in range(0, len(curr_set)):\n",
    "        dict_texts[str(i_track)] = {\"text\":curr_set[ii], \"id\":\"{0}_{1}\".format(key, ii),\n",
    "                                    \"missions\":{\"mission\":mission, \"class\":key}}\n",
    "        i_track += 1\n",
    "#\n",
    "print(\"Number of texts in text set: {0}\".format(len(dict_texts)))\n",
    "print(\"\")\n",
    "for key in dict_texts:\n",
    "    print(dict_texts[key])\n",
    "    print(\"-\")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eacfa22-ba0d-48e2-b878-e6e665b61e3b",
   "metadata": {},
   "source": [
    "Next, let's prepare some additional information for each of these classifiers.  We'll need to set, for example, the uncertainty thresholds for accepting or rejecting each classifier's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52317109-f682-4555-a6fe-187424b69116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters for each operator and its internal classifier\n",
    "#Global parameters\n",
    "do_verify_truematch = False #Off for now, turn on later\n",
    "do_raise_innererror = False\n",
    "\n",
    "#For operator 1\n",
    "mapper_1 = None #Mapper to mask classifications; None if no masking\n",
    "dict_texts_1 = dict_texts #Dictionary of texts to classify\n",
    "threshold_1 = 0.70 #Uncertainty threshold for this classifier\n",
    "buffer_1 = 0 #None since text already preprocessed\n",
    "\n",
    "#For operator 2\n",
    "mapper_2 = None #Mapper to mask classifications; None if no masking\n",
    "dict_texts_2 = dict_texts #Dictionary of texts to classify\n",
    "threshold_2 = 0.70 #Uncertainty threshold for this classifier\n",
    "buffer_2 = 0 #None since text already preprocessed\n",
    "\n",
    "#Gather parameters into lists\n",
    "list_mappers = [mapper_1, mapper_2]\n",
    "list_thresholds = [threshold_1, threshold_2]\n",
    "list_dict_texts = [dict_texts_1, dict_texts_2]\n",
    "list_buffers = [buffer_1, buffer_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc781e0-10f6-4eee-a431-23727b1ef0e2",
   "metadata": {},
   "source": [
    "Now, let's evaluate the performance of these classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09831c4e-c557-4f2d-87fe-134a12329bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of the Performance class\n",
    "performer = bibcat.Performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c73bc293-bb66-4d66-bb0e-6643a1e15ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Running evaluate_performance_basic()!\n",
      "Generating evaluation for the given operators...\n",
      "\n",
      "> Running _generate_evaluation()!\n",
      "Iterating through Operators to classify each set of text...\n",
      "Classifying with Operator #0...\n",
      "\n",
      "> Running classify_set()!\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 1s 642ms/step\n",
      "Classification for text #1 of 33 complete...\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Classification for text #26 of 33 complete...\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "> Running _fetch_keyword_object() for lookup term Hubble.\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "\n",
      "Run of classify_set() complete!\n",
      "\n",
      "Classification complete for Operator #0.\n",
      "Generating the performance counter...\n",
      "\n",
      "> Running _generate_performance_counter()!\n",
      "Accumulating performance over 33 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'science']\n",
      "Measured class names: ['datainfluenced', 'mention', 'science']\n",
      "dict_keys(['verdict', 'scores_comb', 'scores_indiv', 'uncertainty', 'modif'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Run the pipeline for a basic evaluation of model performance\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mperformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_performance_basic\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_operators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdicts_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_dict_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_mappers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_thresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlist_buffers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text_processed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text_processed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdo_verify_truematch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_verify_truematch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_raise_innererror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_raise_innererror\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdo_save_evaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_save_misclassif\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mfileroot_evaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileroot_evaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileroot_misclassif\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileroot_misclassif\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_verbose_deep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/docs/./../main/bibcat_classes.py:6463\u001b[0m, in \u001b[0;36mPerformance.evaluate_performance_basic\u001b[0;34m(self, operators, dicts_texts, mappers, thresholds, buffers, is_text_processed, do_verify_truematch, filepath_output, do_raise_innererror, do_save_evaluation, do_save_misclassif, filename_plot, fileroot_evaluation, fileroot_misclassif, figsize, fontsize, hspace, cmap_abs, cmap_norm, print_freq, do_verbose, do_verbose_deep)\u001b[0m\n\u001b[1;32m   6459\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating evaluation for the given operators...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6460\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   6461\u001b[0m \n\u001b[1;32m   6462\u001b[0m \u001b[38;5;66;03m#Evaluate classifier within each operator\u001b[39;00m\n\u001b[0;32m-> 6463\u001b[0m dict_evaluations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6464\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdicts_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdicts_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmappers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6465\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbuffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text_processed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text_processed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6466\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_verify_truematch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_verify_truematch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6467\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_raise_innererror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_raise_innererror\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6468\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_save_evaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_save_evaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6469\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_save_misclassif\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_save_misclassif\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6470\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilepath_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6471\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfileroot_evaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileroot_evaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6472\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfileroot_misclassif\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileroot_misclassif\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6473\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6474\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_verbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_verbose_deep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_verbose_deep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6475\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   6476\u001b[0m \u001b[38;5;66;03m#Print some notes\u001b[39;00m\n\u001b[1;32m   6477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_verbose:\n",
      "File \u001b[0;32m~/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/docs/./../main/bibcat_classes.py:6591\u001b[0m, in \u001b[0;36mPerformance._generate_evaluation\u001b[0;34m(self, operators, dicts_texts, mappers, thresholds, buffers, is_text_processed, do_verify_truematch, do_raise_innererror, do_save_evaluation, do_save_misclassif, filepath_output, fileroot_evaluation, fileroot_misclassif, print_freq, do_verbose, do_verbose_deep)\u001b[0m\n\u001b[1;32m   6587\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating the performance counter...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6588\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   6589\u001b[0m \n\u001b[1;32m   6590\u001b[0m \u001b[38;5;66;03m#Measure performance of current operator against actual answers\u001b[39;00m\n\u001b[0;32m-> 6591\u001b[0m tmp_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_performance_counter\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6592\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmappers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_actdicts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_actdicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6593\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlist_measdicts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdo_verbose_deep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_verbose_deep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6596\u001b[0m curr_counter \u001b[38;5;241m=\u001b[39m tmp_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   6597\u001b[0m curr_misclassifs \u001b[38;5;241m=\u001b[39m tmp_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmisclassifs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/docs/./../main/bibcat_classes.py:6745\u001b[0m, in \u001b[0;36mPerformance._generate_performance_counter\u001b[0;34m(self, operator, mapper, list_actdicts, list_measdicts, print_freq, do_verbose, do_verbose_deep)\u001b[0m\n\u001b[1;32m   6743\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6744\u001b[0m     curr_actval \u001b[38;5;241m=\u001b[39m curr_actdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 6745\u001b[0m     curr_measval \u001b[38;5;241m=\u001b[39m \u001b[43mcurr_measdict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurr_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   6746\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   6747\u001b[0m \n\u001b[1;32m   6748\u001b[0m \u001b[38;5;66;03m#Increment current counter\u001b[39;00m\n\u001b[1;32m   6749\u001b[0m dict_counters[curr_actval][curr_measval] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "#Run the pipeline for a basic evaluation of model performance\n",
    "performer.evaluate_performance_basic(operators=list_operators, dicts_texts=list_dict_texts, mappers=list_mappers,\n",
    "                                     thresholds=list_thresholds, buffers=list_buffers, is_text_processed=is_text_processed,\n",
    "                                     do_verify_truematch=do_verify_truematch, do_raise_innererror=do_raise_innererror,\n",
    "                                     do_save_evaluation=True, do_save_misclassif=True, filepath_output=filepath_output,\n",
    "                                     fileroot_evaluation=fileroot_evaluation, fileroot_misclassif=fileroot_misclassif,\n",
    "                                     print_freq=25, do_verbose=True, do_verbose_deep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ceb74-7087-4256-8044-59f4cb97db51",
   "metadata": {},
   "source": [
    "And with that, you should have new confusion matrices summarizing the basic performance for these classifiers saved in your requested directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee10749-9dc1-4460-a34c-18ecb413f450",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8f912-089e-462c-b345-9710f35a03f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
