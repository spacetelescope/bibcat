{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71ff86",
   "metadata": {},
   "source": [
    "# Bibliography Categorization: 'BibCat'\n",
    "## Tutorial: Estimating performance of classifiers in bibcat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ab7e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234a479",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction.\n",
    "\n",
    "In this tutorial, we will use bibcat to estimate the performance of classifiers on sets of texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f050727",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86289680",
   "metadata": {},
   "source": [
    "## User Workflow: Training a machine learning (ML) model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af236d7a",
   "metadata": {},
   "source": [
    "The `Performance` class contains user-friendly methods for estimating the performance of given classifiers and outputting that performance as, e.g., confusion matrices.  We overview how this method can be run in the code blocks below.\n",
    "\n",
    "For this tutorial, we assume that the user has already run the trainML tutorial, and so has generated and saved a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdea077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import external packages\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272dbb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/docs\n",
      "Source directory: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src\n"
     ]
    }
   ],
   "source": [
    "# Set up for fetching necessary bibcat modules for the tutorial\n",
    "# Check work directories: src/ is where all source python scripts are available. \n",
    "current_dir= os.path.dirname(os.path.abspath('__file__'))\n",
    "_parent = os.path.dirname(current_dir)\n",
    "src_dir = os.path.join(_parent, \"src\")\n",
    "\n",
    "print(f'Current Directory: {current_dir}')\n",
    "print(f'Source directory: {src_dir}')\n",
    "\n",
    "# move to the ../src/ directory to import necessary modules. \n",
    "os.chdir(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d040df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory =/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src, parent directory=/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat\n",
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models folder already exists.\n",
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/output folder already exists.\n"
     ]
    }
   ],
   "source": [
    "#Import bibcat packages\n",
    "import bibcat_classes as bibcat\n",
    "import bibcat_config as config\n",
    "import bibcat_parameters as params #Temporary file until contents moved elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0edceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters for each operator and its internal classifier\n",
    "#Global parameters\n",
    "do_verify_truematch = True #A very important parameter - discuss with J.P. first!!!  Set it to either True or False.\n",
    "do_raise_innererror = False #If True, will stop if exception encountered; if False, will print error and continue\n",
    "do_reuse_run = True\n",
    "#\n",
    "do_include_trainML_unused_bibcodes_in_testset = False #If True, will include the bibcodes from the trainML tutorial that were skipped because e.g. not target missions\n",
    "#\n",
    "list_threshold_arrays = [np.arange(0.35, 0.95+0.05, 0.05)]*2 #For uncertainty test\n",
    "class_mapper = params.map_papertypes #Mapper for class types; None for no mapper\n",
    "fileroot_evaluation = \"test_eval\" #Root name of the file within which to store the performance evaluation output\n",
    "threshold = 0.7 #0.9\n",
    "\n",
    "#For operator 1\n",
    "mapper_1 = class_mapper #Mapper to mask classifications; None if no masking\n",
    "threshold_1 = threshold #Uncertainty threshold for this classifier\n",
    "buffer_1 = 0\n",
    "\n",
    "#For operator 2\n",
    "mapper_2 = class_mapper #Mapper to mask classifications; None if no masking\n",
    "threshold_2 = threshold #Uncertainty threshold for this classifier\n",
    "buffer_2 = 0\n",
    "\n",
    "#Gather parameters into lists\n",
    "list_mappers = [mapper_1, mapper_2]\n",
    "list_thresholds = [threshold_1, threshold_2]\n",
    "list_buffers = [buffer_1, buffer_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e0a06e-61fc-438a-b642-3669910f9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set some overarching global variables\n",
    "seed_test = 20 #Random seed for shuffling text dataset\n",
    "np.random.seed(seed_test)\n",
    "do_shuffle = True #Whether or not to shuffle the text dataset\n",
    "do_real_testdata = True #If True, will use real papers to test performance; if False, will use fake texts below\n",
    "#\n",
    "max_tests = 100 #None #100 #Number of text entries to test the performance for; None for all tests available\n",
    "mode_modif = \"none\" #\"skim_anon\" #\"skim_trim_anon\" #None #We are using preprocessed data in this tutorial, so we do not need a processing mode at all\n",
    "target_classifs_basic = [\"science\", \"mention\", \"datainfluenced\"]\n",
    "target_classifs_uncertainty = [\"science\", \"mention\", \"datainfluenced\", \"other\", \"zlowprob\"]\n",
    "minmax_exclude_classifs = ([item.lower().replace(\"_\",\"\") for item in config.list_other_verdicts] + [\"other\"])\n",
    "\n",
    "#\n",
    "#Prepare some Keyword objects\n",
    "all_kobjs = params.all_kobjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710913d3-0a11-4cda-8971-1b845d542fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output will be saved to: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_rule/output\n"
     ]
    }
   ],
   "source": [
    "#Fetch filepaths for model and data\n",
    "name_model = config.name_model\n",
    "filepath_json = config.path_json\n",
    "dir_model = os.path.join(config.dir_allmodels, name_model)\n",
    "#\n",
    "#Set filepath for unused bibcodes from trainML, if so requested\n",
    "if do_include_trainML_unused_bibcodes_in_testset:\n",
    "    filesave_unused_bibcodes = os.path.join(dir_model,\n",
    "                              \"{0}_bibcodes_unused_during_trainML.npy\".format(name_model)) #Where to save processing errors\n",
    "#\n",
    "#Set and create (as needed) directories for storing performance output\n",
    "filepath_output = os.path.join(dir_model, \"output\") #Where to store performance output, such as confusion matrices\n",
    "if (not os.path.exists(filepath_output)):\n",
    "    os.makedirs(filepath_output)\n",
    "    print(\"Output folder created at: {0}\".format(filepath_output))\n",
    "print(\"Output will be saved to: {0}\".format(filepath_output))\n",
    "#\n",
    "#Set directories for fetching text\n",
    "dir_info = dir_model\n",
    "folder_test = config.folders_TVT[\"test\"]\n",
    "dir_test = os.path.join(dir_model, folder_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188dbd8-388d-4f8e-95e9-ebede99435c2",
   "metadata": {},
   "source": [
    "Let's build a set of classifiers for which we'd like to test the performance.  We'll then feed each of those classifiers into an instance of the Operator class to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116fd35a-994f-452c-87dc-233733ab8749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/bibcat_classes.py:4304: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  scoring_matrix = {key:np.linalg.lstsq(input_matr, classif_vec[key])[0]\n"
     ]
    }
   ],
   "source": [
    "#Create a list of classifiers\n",
    "#This can be modified to use whatever classifiers you'd like.\n",
    "#Load a previously trained ML model\n",
    "filepath_model = os.path.join(dir_model, (name_model+\".npy\"))\n",
    "fileloc_ML = os.path.join(dir_model, (config.tfoutput_prefix+name_model))\n",
    "# !!! classifier_ML = bibcat.Classifier_ML(filepath_model=filepath_model, fileloc_ML=fileloc_ML, do_verbose=True)\n",
    "#\n",
    "\n",
    "#Load a rule-based classifier\n",
    "classifier_rules = bibcat.Classifier_Rules()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e4a36a1-e8cc-4576-8aab-904f61813988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance of Operator successfully initialized!\n",
      "Keyword objects:\n",
      "0: Keyword Object:\n",
      "Name: Hubble\n",
      "Keywords: ['Hubble Space Telescope', 'Hubble Telescope', 'Hubble']\n",
      "Acronyms: ['HST', 'HT']\n",
      "Banned Overlap: ['Hubble Legacy Archive']\n",
      "\n",
      "1: Keyword Object:\n",
      "Name: Webb Telescope\n",
      "Keywords: ['James Webb Space Telescope', 'Webb Space Telescope', 'James Webb Telescope', 'Webb Telescope']\n",
      "Acronyms: ['JWST', 'JST', 'JT']\n",
      "Banned Overlap: []\n",
      "\n",
      "2: Keyword Object:\n",
      "Name: Transiting Exoplanet Survey Satellite\n",
      "Keywords: ['Transiting Exoplanet Survey Satellite']\n",
      "Acronyms: ['TESS']\n",
      "Banned Overlap: []\n",
      "\n",
      "3: Keyword Object:\n",
      "Name: Kepler\n",
      "Keywords: ['Kepler']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "4: Keyword Object:\n",
      "Name: Pan-STARRS\n",
      "Keywords: ['Panoramic Survey Telescope and Rapid Response System', 'Pan-STARRS1', 'Pan-STARRS']\n",
      "Acronyms: ['PanSTARRS1', 'PanSTARRS', 'PS1']\n",
      "Banned Overlap: []\n",
      "\n",
      "5: Keyword Object:\n",
      "Name: Galaxy Evolution Explorer\n",
      "Keywords: ['Galaxy Evolution Explorer']\n",
      "Acronyms: ['GALEX']\n",
      "Banned Overlap: []\n",
      "\n",
      "6: Keyword Object:\n",
      "Name: K2\n",
      "Keywords: ['K2']\n",
      "Acronyms: []\n",
      "Banned Overlap: []\n",
      "\n",
      "7: Keyword Object:\n",
      "Name: Hubble Legacy Archive\n",
      "Keywords: ['Hubble Legacy Archive']\n",
      "Acronyms: ['HLA']\n",
      "Banned Overlap: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load models into instances of the Operator class\n",
    "# !!! operator_1 = bibcat.Operator(classifier=classifier_ML, mode=mode_modif, keyword_objs=all_kobjs,\n",
    "#                            name=\"Operator_ML\", do_verbose=True, load_check_truematch=True, do_verbose_deep=False)\n",
    "operator_2 = bibcat.Operator(classifier=classifier_rules,\n",
    "                            name=\"Operator_RB\", mode=mode_modif, keyword_objs=all_kobjs,\n",
    "                            do_verbose=True, do_verbose_deep=False)\n",
    "list_operators = [operator_2] #[operator_1, operator_2] #Feel free to add more/less operators here.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82951251-fc59-4e33-a978-bc8fa16915dd",
   "metadata": {},
   "source": [
    "Now, let's fetch some text for our classifiers to classify. For this tutorial, we'll load previously processed texts from the directory containing the test set for the ML classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8075c1f-bc7b-422e-89cc-82fac058bb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts in text set: 100\n"
     ]
    }
   ],
   "source": [
    "#For use of real papers from test dataset to test on\n",
    "if (do_real_testdata and ((not do_reuse_run) or (not os.path.exists(os.path.join(filepath_output, (fileroot_evaluation+\".npy\")))))):\n",
    "    #Load information for processed bibcodes reserved for testing\n",
    "    dict_TVTinfo = np.load(os.path.join(dir_info, \"dict_TVTinfo.npy\"), allow_pickle=True).item()\n",
    "    list_test_bibcodes = [key for key in dict_TVTinfo if (dict_TVTinfo[key][\"folder_TVT\"] == folder_test)]\n",
    "    \n",
    "    #Load the original data\n",
    "    with open(filepath_json, 'r') as openfile:\n",
    "        dataset = json.load(openfile)\n",
    "    #\n",
    "    \n",
    "    #Extract text information for the bibcodes reserved for testing\n",
    "    list_test_indanddata_raw = [(ii, dataset[ii]) for ii in range(0, len(dataset))\n",
    "                                if (dataset[ii][\"bibcode\"] in list_test_bibcodes)] #Data for test set\n",
    "    #\n",
    "    #Add in unused bibcodes from trainML tutorial, if so requested\n",
    "    if do_include_trainML_unused_bibcodes_in_testset:\n",
    "        tmp_dict = np.load(filesave_unused_bibcodes, allow_pickle=True).item()\n",
    "        list_test_indanddata_raw += [(tmp_dict[key], dataset[tmp_dict[key]]) for key in tmp_dict]\n",
    "    #\n",
    "    \n",
    "    #Shuffle, if requested\n",
    "    if do_shuffle:\n",
    "        np.random.shuffle(list_test_indanddata_raw)\n",
    "    #\n",
    "    \n",
    "    #Extract target number of test papers from the test bibcodes\n",
    "    if (max_tests is not None): #Fetch subset of tests\n",
    "        list_test_indanddata = list_test_indanddata_raw[0:max_tests]\n",
    "    else: #Use all tests\n",
    "        list_test_indanddata = list_test_indanddata_raw\n",
    "    #\n",
    "    \n",
    "    #Process the text input into dictionary format for inputting into the codebase\n",
    "    dict_texts = {} #To hold formatted text entries\n",
    "    for ii in range(0, len(list_test_indanddata)):\n",
    "        curr_ind = list_test_indanddata[ii][0]\n",
    "        curr_data = list_test_indanddata[ii][1]\n",
    "        #\n",
    "        #Convert this data entry into dictionary with: key:text,id,bibcode,mission structure\n",
    "        curr_info = {\"text\":curr_data[\"body\"], \"id\":str(curr_ind), \"bibcode\":curr_data[\"bibcode\"],\n",
    "                    \"missions\":{}}\n",
    "        \n",
    "        #Initialize all mission entries as non-matches\n",
    "        for curr_kobj in all_kobjs: #Iterate through declared Keyword objects\n",
    "            curr_name = curr_kobj.get_name()\n",
    "            curr_info[\"missions\"][curr_name] = {\"mission\":curr_name, \"class\":config.verdict_rejection}                    \n",
    "            \n",
    "        #If using unused bibcodes and no class_missions, store as is\n",
    "        if (do_include_trainML_unused_bibcodes_in_testset and (\"class_missions\" not in curr_data)):\n",
    "            #Store this data entry and skip ahead\n",
    "            dict_texts[str(curr_ind)] = curr_info\n",
    "            continue\n",
    "        \n",
    "        #Iterate through missions\n",
    "        for curr_mission in curr_data[\"class_missions\"]: #Iterate through missions for this paper\n",
    "            for curr_kobj in all_kobjs: #Iterate through declared Keyword objects\n",
    "                curr_name = curr_kobj.get_name()\n",
    "                #Store mission data under keyword name, if applicable\n",
    "                if (curr_kobj.identify_keyword(curr_mission)[\"bool\"]):\n",
    "                    curr_info[\"missions\"][curr_name] = {\"mission\":curr_name,\n",
    "                                                    \"class\":curr_data[\"class_missions\"][curr_mission][\"papertype\"]}\n",
    "                #\n",
    "                #Otherwise, store that this mission was not detected for this text\n",
    "                #else:\n",
    "                #    curr_info[\"missions\"][curr_name] = {\"mission\":curr_name, \"class\":config.verdict_rejection}                    \n",
    "            #\n",
    "        #\n",
    "        #Store this data entry\n",
    "        dict_texts[str(curr_ind)] = curr_info\n",
    "    #\n",
    "    \n",
    "    #Print some notes about the testing data\n",
    "    print(\"Number of texts in text set: {0}\".format(len(dict_texts)))\n",
    "    \"\"\"\n",
    "    print(\"\")\n",
    "    for key in dict_texts:\n",
    "        print(\"Entry {0}:\".format(key))\n",
    "        print(\"ID: {0}\".format(dict_texts[key][\"id\"]))\n",
    "        print(\"Bibcode: {0}\".format(dict_texts[key][\"bibcode\"]))\n",
    "        print(\"Missions: {0}\".format(dict_texts[key][\"missions\"]))\n",
    "        print(\"Start of text:\\n{0}\".format(dict_texts[key][\"text\"][0:500]))\n",
    "        print(\"-\\n\")\n",
    "    #\"\"\"\n",
    "#\n",
    "else:\n",
    "    dict_texts = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2194a016-51fc-47fc-a756-7540e5674774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For use of fake, made-up data entries to test on\n",
    "if (not do_real_testdata):\n",
    "    print(\"Using fake test data for testing.\")\n",
    "    #Make some fake data\n",
    "    dict_texts_raw = {\"science\":[\"We present HST observations in Figure 4.\",\n",
    "                        \"The HST stars are listed in Table 3b.\",\n",
    "                        \"Despite our efforts to smooth the data, there are still rings in the HST images.\",\n",
    "                        \"See Section 8c for more discussion of the Hubble images.\",\n",
    "                        \"The supernovae detected with HST tend to be brighter than initially predicted.\",\n",
    "                        \"Our spectra from HST fit well to the standard trend first published in Someone et al. 1990.\",\n",
    "                        \"We use the Hubble Space Telescope to build an ultraviolet database of the target stars.\",\n",
    "                        \"The blue points (HST) exhibit more scatter than the red points (JWST).\",\n",
    "                        \"The benefit, then, is the far higher S/N we achieved in our HST observations.\",\n",
    "                        \"Here we employ the Hubble Telescope to observe the edge of the photon-dominated region.\",\n",
    "                        \"The black line shows that the region targeted with Hubble has an extreme UV signature.\"],\n",
    "                 \"datainfluenced\":[\"The simulated Hubble data is plotted in Figure 4.\",\n",
    "                       \"Compared to the HST observations in Someone et al., our JWST follow-up reached higher S/N.\",\n",
    "                       \"We were able to reproduce the luminosities from Hubble using our latest models.\",\n",
    "                       \"We overplot Hubble-observed stars from Someone et al. in Figure 3b.\",\n",
    "                       \"We built the spectral templates using UV data in the Hubble archive.\",\n",
    "                       \"We simulate what our future HST observations will look like to predict the S/N.\",\n",
    "                       \"Our work here with JWST is inspired by our earlier HST study published in 2010.\",\n",
    "                       \"We therefore use the Hubble statistics from Author et al. to guide our stellar predictions.\",\n",
    "                       \"The stars in Figure 3 were plotted based on the HST-fitted trend line in Person et al.\",\n",
    "                       \"The final step is to use the HST exposure tool to put our modeled images in context.\"],\n",
    "                 \"mention\":[\"Person et al. used HST to measure the Hubble constant.\",\n",
    "                        \"We will present new HST observations in a future work.\",\n",
    "                        \"HST is a fantastic instrument that has revolutionized our view of space.\",\n",
    "                        \"The Hubble Space Telescope (HST) has its mission center at the STScI.\",\n",
    "                        \"We can use HST to power a variety of science in the ultraviolet regime.\",\n",
    "                        \"It is not clear when the star will be observable with HST.\",\n",
    "                        \"More data can be found and downloaded from the Hubble archive.\",\n",
    "                        \"We note that HST can be used to observe the stars as well, at higher S/N.\",\n",
    "                        \"However, we ended up using the JWST rather than HST observations in this work.\",\n",
    "                        \"We push the analysis of the Hubble component of the dataset to a future study.\",\n",
    "                        \"We expect the HST observations to be released in the fall.\",\n",
    "                        \"We look forward to any follow-up studies with, e.g., the Hubble Telescope.\"]}\n",
    "    #\n",
    "    #Convert into dictionary with: key:text,class,id,mission structure\n",
    "    i_track = 0\n",
    "    dict_texts = {}\n",
    "    #Store subheadings by mission, to avoid duplicating and processing the same text across different missions\n",
    "    mission = operator_1._fetch_keyword_object(lookup=\"HST\")._get_info(\"name\")\n",
    "    for key in dict_texts_raw:\n",
    "        curr_set = dict_texts_raw[key]\n",
    "        for ii in range(0, len(curr_set)):\n",
    "            dict_texts[str(i_track)] = {\"text\":curr_set[ii], \"id\":\"{0}_{1}\".format(key, ii), \"bibcode\":str(i_track),\n",
    "                                        \"missions\":{mission:{\"mission\":mission, \"class\":key}}}\n",
    "            i_track += 1\n",
    "    #\n",
    "    print(\"Mission: {0}\".format(mission))\n",
    "    print(\"Number of texts in text set: {0}\".format(len(dict_texts)))\n",
    "    print(\"\")\n",
    "    for key in dict_texts:\n",
    "        print(dict_texts[key])\n",
    "        print(\"-\")\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eacfa22-ba0d-48e2-b878-e6e665b61e3b",
   "metadata": {},
   "source": [
    "Next, let's prepare some additional information for each of these classifiers.  We'll need to set, for example, the uncertainty thresholds for accepting or rejecting each classifier's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52317109-f682-4555-a6fe-187424b69116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store texts for each operator and its internal classifier\n",
    "#For operator 1\n",
    "dict_texts_1 = dict_texts #Dictionary of texts to classify\n",
    "\n",
    "#For operator 2\n",
    "dict_texts_2 = dict_texts #Dictionary of texts to classify\n",
    "\n",
    "#Gather into list\n",
    "list_dict_texts = [dict_texts_1, dict_texts_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc781e0-10f6-4eee-a431-23727b1ef0e2",
   "metadata": {},
   "source": [
    "Now, let's evaluate the performance of these classifiers in different ways.  We will consider these performance tests:\n",
    "* Basic: We generate confusion matrices for the set of Operators (containing the different classifiers).\n",
    "* Uncertainty: We plot performance as a function of uncertainty level for the set of Operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09831c4e-c557-4f2d-87fe-134a12329bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of the Performance class\n",
    "performer = bibcat.Performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1be08",
   "metadata": {},
   "source": [
    "The Basic evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c73bc293-bb66-4d66-bb0e-6643a1e15ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Running evaluate_performance_basic()!\n",
      "Generating classifications for the given operators...\n",
      "\n",
      "> Running _generate_classifications()!\n",
      "Iterating through Operators to classify each set of text...\n",
      "Classifying with Operator #0...\n",
      "\n",
      "> Running classify_set()!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/bibcat_classes.py:5567: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  (np.abs(tmp_unnorm - curr_scores[other_key])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification for text #1 of 100 complete...\n",
      "Classification for text #2 of 100 complete...\n",
      "Classification for text #3 of 100 complete...\n",
      "Classification for text #4 of 100 complete...\n",
      "Classification for text #5 of 100 complete...\n",
      "Classification for text #6 of 100 complete...\n",
      "Classification for text #7 of 100 complete...\n",
      "Classification for text #8 of 100 complete...\n",
      "Classification for text #9 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #10 of 100 complete...\n",
      "Classification for text #11 of 100 complete...\n",
      "Classification for text #12 of 100 complete...\n",
      "Classification for text #13 of 100 complete...\n",
      "Classification for text #14 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nKepler PSF\\nTaken from this text snippet:\\nSince the PSF of TESS is substantially larger than the Kepler PSF and the duration of a typical light curve is much shorter than Kepler ’s four years, the centroid method (Section 000.000 is far less efficient.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #15 of 100 complete...\n",
      "Classification for text #16 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nsimple Kepler solver\\nTaken from this text snippet:\\nThe latter can be a different code, for example, a simple Kepler solver or some high-order symplectic N -body solver.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #17 of 100 complete...\n",
      "Classification for text #18 of 100 complete...\n",
      "Classification for text #19 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nmany Kepler rapid rotators\\nTaken from this text snippet:\\nIf the star is actually an unresolved binary, as could be the case for many Kepler rapid rotators, it would likely be tidally locked, decoupling P rot from age, causing the predictions of gyrochronology models to fail.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #20 of 100 complete...\n",
      "Classification for text #21 of 100 complete...\n",
      "Classification for text #22 of 100 complete...\n",
      "Classification for text #23 of 100 complete...\n",
      "Classification for text #24 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nsignificant Hubble tension\\nTaken from this text snippet:\\nIn particular, we emphasize that the statistically significant Hubble tension does not appear in this model but does exist in the standard ΛCDM model, independently showing ΛCDM to be invalid.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #25 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nPHANGS - HST\\nTaken from this text snippet:\\nLarge imaging surveys using the Hubble Space Telescope (HST) have made significant progress in cataloging and characterizing star cluster populations in nearby 000–000 Mpc) galaxies (e.g, LEGUS, PHANGS-HST; Adamoetal 000; Leeetal 000.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #26 of 100 complete...\n",
      "Classification for text #27 of 100 complete...\n",
      "Classification for text #28 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble users\\nTaken from this text snippet:\\nSome are relatively “standard” and familiar to Hubble users, but offered with unprecedented sensitivity and wavelength coverage: e.g. imaging, long-slit spectroscopy, Lyot coronagraphy with occulting spots – while other capabilities are being offered for the first time in a space facility at these wavelengths: multi-object spectroscopy with a programmable shutter array, integral field spectroscopy, and high-contrast imaging through the use of aperture masking interferometry (NIRISS) or 000-quadrant phase masks (4QPM, MIRI).')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #29 of 100 complete...\n",
      "Classification for text #30 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble flow\\nTaken from this text snippet:\\nHowever, as shown by deviations of galaxy recession velocities from the Hubble flow and via direct galaxy catalogs, the local universe is lumpy 000% fluctuations or more) on length scales as large as 000 Mpc. 000 000 This is not at all surprising given the 000 Mpc Baryon Acoustic Oscillations (BAO) length scale.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #31 of 100 complete...\n",
      "Classification for text #32 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #33 of 100 complete...\n",
      "Classification for text #34 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble type\\nTaken from this text snippet:\\nThis is in contrast to results based on CALIFA data, which did not find any correlation between the metallicity gradient and Hubble type (Sánchezetal 000; Sánchez-Menguianoetal 000.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #35 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #36 of 100 complete...\n",
      "Classification for text #37 of 100 complete...\n",
      "Classification for text #38 of 100 complete...\n",
      "Classification for text #39 of 100 complete...\n",
      "Classification for text #40 of 100 complete...\n",
      "Classification for text #41 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHubble time\\nTaken from this text snippet:\\nIf t iso is greater than the Hubble time, then we substitute N env, the number of ISO-spawning regions that have ever existed in the Milky Way, for R env t iso.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #42 of 100 complete...\n",
      "Classification for text #43 of 100 complete...\n",
      "Classification for text #44 of 100 complete...\n",
      "Classification for text #45 of 100 complete...\n",
      "Classification for text #46 of 100 complete...\n",
      "Classification for text #47 of 100 complete...\n",
      "Classification for text #48 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #49 of 100 complete...\n",
      "Classification for text #50 of 100 complete...\n",
      "Classification for text #51 of 100 complete...\n",
      "Classification for text #52 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #53 of 100 complete...\n",
      "Classification for text #54 of 100 complete...\n",
      "Classification for text #55 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #56 of 100 complete...\n",
      "Classification for text #57 of 100 complete...\n",
      "Classification for text #58 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nK2 We\\nTaken from this text snippet:\\nOne of these flares is found to have the largest amplitude relative to the photospheric level and also the largest energy among all the L dwarf flares monitored by the Kepler / K 000 mission. 000.000 Sample of L dwarfs observed by K2 We analysed the light curves of 000 L dwarfs for which we obtained K 000 photometry despite being very faint objects.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #59 of 100 complete...\n",
      "Classification for text #60 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nextensive Kepler dataset\\nTaken from this text snippet:\\nAn extensive Kepler dataset showed HAT-P-000 b to have an approximately polar and eccentric orbit, evidencing a dynamically disturbed history for the system 000, 000.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nK2 - K4V\\nTaken from this text snippet:\\nWe know that HAT-P-000 is an active, K2-K4V (effective temperature T eff ≈ 000 K), high-metallicity ([Fe/H] = 000.000 star 000.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #61 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #62 of 100 complete...\n",
      "Classification for text #63 of 100 complete...\n",
      "Classification for text #64 of 100 complete...\n",
      "Classification for text #65 of 100 complete...\n",
      "Classification for text #66 of 100 complete...\n",
      "Classification for text #67 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nKepler detector\\nTaken from this text snippet:\\nThe ability of the Kepler detector to resolve multi-star systems is limited due to it having a relatively large pixel size (approximately 000′′ on sky. 000 000 Characteristics of the Kepler space telescope: websitewebsite.)')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #68 of 100 complete...\n",
      "Classification for text #69 of 100 complete...\n",
      "Classification for text #70 of 100 complete...\n",
      "Classification for text #71 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #72 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #73 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nHST - STIS\\nTaken from this text snippet:\\nThe current goal of the University of Colorado ultraviolet rocket program is to develop the technical capabilities to enable a future, highly multiplexed ultraviolet spectrograph (with both high-resolution and imaging spectroscopy modes), e.g, an analog to the successful Hubble Space Telescope-Space Telescope Imaging Spectrograph (HST-STIS) instrument, with an order-of-magnitude higher efficiency.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #74 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #75 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #76 of 100 complete...\n",
      "Classification for text #77 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #78 of 100 complete...\n",
      "Classification for text #79 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nK2 V\\nTaken from this text snippet:\\nThe P rot ∼ 000 day K dwarfs (see Tables 000 and 000 for stellar parameter references): comparing the similar planet-hosting stars HD000 (K0 V, T eff = 000 K, P rot = 000.000 days), Eri (K2 V, T eff = 000 K, P rot = 000.000 days), and HD000 (K3 V, T eff = 000 K, P rot = 000 days) with the non-planet-hosting K dwarf HR000 (K1 V, T eff = 000 K, P rot = 000.000 days), we find the average F Si iv / F bolom value for the planet-hosting stars is 000.000 (±000.000 × 000 −000, while HR000 displays the identical 000.000 (±000.000 × 000 −000. 000.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #80 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #81 of 100 complete...\n",
      "Classification for text #82 of 100 complete...\n",
      "Classification for text #83 of 100 complete...\n",
      "Classification for text #84 of 100 complete...\n",
      "Classification for text #85 of 100 complete...\n",
      "Classification for text #86 of 100 complete...\n",
      "Classification for text #87 of 100 complete...\n",
      "Classification for text #88 of 100 complete...\n",
      "Classification for text #89 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nKepler ’s SN remnant\\nTaken from this text snippet:\\nAuthorsetal suggest that Fe clumps may also be responsible for the protrusions or ‘ears’ seen in Kepler ’s SN remnant (SNR) and SNR G1.000 + 000.000.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #90 of 100 complete...\n",
      "Classification for text #91 of 100 complete...\n",
      "Classification for text #92 of 100 complete...\n",
      "Classification for text #93 of 100 complete...\n",
      "Classification for text #94 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "TypeError(\"'NoneType' object is not subscriptable\")\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #95 of 100 complete...\n",
      "-\n",
      "The following err. was encountered in operate:\n",
      "NotImplementedError('Err: Unrecognized ambig. phrase:\\nlocal Kepler time\\nTaken from this text snippet:\\nThe viscous spreading timescale 000 000 The viscous evolution timescale is t ν = (α Ω K) −000 × (h / r) −000, where α is the Shakura-Sunyaev viscosity parameter, Ω K is the local Kepler time, and (h / r) is the scale height-to-radius ratio. from 000 au is t ∼ 000 000 yr, assuming the viscosity parameter is α = 000 −000.')\n",
      "Error was noted. Returning error as verdict.\n",
      "-\n",
      "Classification for text #96 of 100 complete...\n",
      "Classification for text #97 of 100 complete...\n",
      "Classification for text #98 of 100 complete...\n",
      "Classification for text #99 of 100 complete...\n",
      "Classification for text #100 of 100 complete...\n",
      "\n",
      "Run of classify_set() complete!\n",
      "\n",
      "Classification complete for Operator #0.\n",
      "Generating the performance counter...\n",
      "All work complete for Operator #0.\n",
      "!\n",
      "\n",
      "Evaluation saved at: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_rule/output/test_eval.npy\n",
      "\n",
      "Run of _generate_classifications() complete!\n",
      "\n",
      "Classifications generated.\n",
      "Evaluating classifications...\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "Saving misclassifications...\n",
      "\n",
      "Misclassifications for Operator_RB saved at: /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_rule/output/test_misclassif_basic_Operator_RB.txt\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 0\n",
      "Actual mention vs Measured mention: 12\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 17\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 44\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 22\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 7\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 7\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 5\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 16\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "\n",
      "Evaluations generated.\n",
      "Plotting confusion matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/bibcat_classes.py:9284: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  confmatr_norm[yy,xx] = (confmatr_abs[yy,xx] / row_total)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Running _combine_performance_across_evaluations()!\n",
      "Combining evaluations across these operators: ['Operator_RB']\n",
      "All possible operator combinations: []\n",
      "Run of _combine_performance_across_evaluations() complete!\n",
      "Confusion matrices have been plotted at:\n",
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_rule/output\n",
      "\n",
      "Run of evaluate_performance_basic() complete!\n"
     ]
    }
   ],
   "source": [
    "#Parameters for this evaluation\n",
    "filename_root = \"performance_confmatr_basic_{0}\".format(name_model)\n",
    "for ii in range(0, len(list_operators)):\n",
    "    if (list_thresholds[ii] is not None):\n",
    "        filename_root += \"_unc{0}of{1:.2f}\".format((ii+1), list_thresholds[ii]).replace(\".\",\"p\")\n",
    "fileroot_misclassif = \"test_misclassif_basic\" #Root name of the file within which to store misclassified text information\n",
    "figsize = (20, 12)\n",
    "\n",
    "#Run the pipeline for a basic evaluation of model performance\n",
    "performer.evaluate_performance_basic(operators=list_operators, dicts_texts=list_dict_texts, mappers=list_mappers,\n",
    "                                     thresholds=list_thresholds, buffers=list_buffers, is_text_processed=False,\n",
    "                                     do_reuse_run=do_reuse_run, filename_root=filename_root,\n",
    "                                     do_verify_truematch=do_verify_truematch, do_raise_innererror=do_raise_innererror,\n",
    "                                     do_save_evaluation=True, do_save_misclassif=True, filepath_output=filepath_output,\n",
    "                                     fileroot_evaluation=fileroot_evaluation, fileroot_misclassif=fileroot_misclassif,\n",
    "                                     print_freq=1, do_verbose=True, do_verbose_deep=False, figsize=figsize,\n",
    "                                     target_classifs=target_classifs_basic,\n",
    "                                     minmax_exclude_classifs=minmax_exclude_classifs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f12f0d3",
   "metadata": {},
   "source": [
    "The Uncertainty evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f68ae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Running evaluate_performance_uncertainty()!\n",
      "Generating classifications for operators...\n",
      "\n",
      "> Running _generate_classifications()!\n",
      "Iterating through Operators to classify each set of text...\n",
      "Previous evaluation exists at /Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_rule/output/test_eval.npy\n",
      "Loading that eval...\n",
      "\n",
      "Classifications generated.\n",
      "Evaluating classifications...\n",
      "Threshold #1 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 1\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 0\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 3\n",
      "Actual mention vs Measured mention: 25\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 45\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 0\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 1\n",
      "Actual science vs Measured mention: 1\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 27\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 0\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 13\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 15\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 0\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #2 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 1\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 0\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 3\n",
      "Actual mention vs Measured mention: 24\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 44\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 2\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 1\n",
      "Actual science vs Measured mention: 1\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 27\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 0\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 13\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 15\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 0\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #3 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 1\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 0\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 1\n",
      "Actual mention vs Measured mention: 23\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 42\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 7\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 1\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 27\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 1\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 13\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 15\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 0\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #4 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 1\n",
      "Actual mention vs Measured mention: 19\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 41\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 12\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 26\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 3\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 11\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 15\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 2\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #5 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 1\n",
      "Actual mention vs Measured mention: 17\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 34\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 21\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 26\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 3\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 10\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 13\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 5\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #6 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 0\n",
      "Actual mention vs Measured mention: 17\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 29\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 27\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 25\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 4\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 9\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 11\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 8\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #7 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 0\n",
      "Actual mention vs Measured mention: 16\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 21\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 36\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 22\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 7\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 8\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 6\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 14\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #8 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 0\n",
      "Actual mention vs Measured mention: 12\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 17\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 44\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 22\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 7\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 7\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 5\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 16\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #9 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 0\n",
      "Actual mention vs Measured mention: 12\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 17\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 44\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 22\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 7\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 7\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 5\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 16\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #10 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 0\n",
      "Actual mention vs Measured mention: 11\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 16\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 46\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 22\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 7\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 6\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 5\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 17\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #11 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 0\n",
      "Actual mention vs Measured mention: 9\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 15\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 49\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 22\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 7\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 6\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 5\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 17\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #12 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 0\n",
      "Actual mention vs Measured mention: 8\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 14\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 51\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 22\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 7\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 3\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 5\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 20\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "Threshold #13 of 13:\n",
      "\n",
      "> Running _generate_performance_counter() for: Operator_RB\n",
      "Accumulating performance over 100 texts.\n",
      "Actual class names: ['datainfluenced', 'mention', 'other', 'science', 'znotmatch']\n",
      "Measured class names: ['datainfluenced', 'mention', 'other', 'science', 'zerror', 'zlowprob', 'znotmatch']\n",
      "\n",
      "-\n",
      "Performance counter generated:\n",
      "Actual datainfluenced total: 2\n",
      "Actual datainfluenced vs Measured datainfluenced: 0\n",
      "Actual datainfluenced vs Measured mention: 0\n",
      "Actual datainfluenced vs Measured other: 0\n",
      "Actual datainfluenced vs Measured science: 0\n",
      "Actual datainfluenced vs Measured zerror: 1\n",
      "Actual datainfluenced vs Measured zlowprob: 1\n",
      "Actual datainfluenced vs Measured znotmatch: 0\n",
      "Actual datainfluenced vs Measured _total: 2\n",
      "Actual mention total: 86\n",
      "Actual mention vs Measured datainfluenced: 0\n",
      "Actual mention vs Measured mention: 2\n",
      "Actual mention vs Measured other: 0\n",
      "Actual mention vs Measured science: 14\n",
      "Actual mention vs Measured zerror: 13\n",
      "Actual mention vs Measured zlowprob: 57\n",
      "Actual mention vs Measured znotmatch: 0\n",
      "Actual mention vs Measured _total: 86\n",
      "Actual other total: 0\n",
      "Actual other vs Measured datainfluenced: 0\n",
      "Actual other vs Measured mention: 0\n",
      "Actual other vs Measured other: 0\n",
      "Actual other vs Measured science: 0\n",
      "Actual other vs Measured zerror: 0\n",
      "Actual other vs Measured zlowprob: 0\n",
      "Actual other vs Measured znotmatch: 0\n",
      "Actual other vs Measured _total: 0\n",
      "Actual science total: 51\n",
      "Actual science vs Measured datainfluenced: 0\n",
      "Actual science vs Measured mention: 0\n",
      "Actual science vs Measured other: 0\n",
      "Actual science vs Measured science: 22\n",
      "Actual science vs Measured zerror: 19\n",
      "Actual science vs Measured zlowprob: 7\n",
      "Actual science vs Measured znotmatch: 3\n",
      "Actual science vs Measured _total: 51\n",
      "Actual znotmatch total: 661\n",
      "Actual znotmatch vs Measured datainfluenced: 0\n",
      "Actual znotmatch vs Measured mention: 1\n",
      "Actual znotmatch vs Measured other: 0\n",
      "Actual znotmatch vs Measured science: 4\n",
      "Actual znotmatch vs Measured zerror: 14\n",
      "Actual znotmatch vs Measured zlowprob: 23\n",
      "Actual znotmatch vs Measured znotmatch: 619\n",
      "Actual znotmatch vs Measured _total: 661\n",
      "\n",
      "-\n",
      "\n",
      "Run of _generate_performance_counter() complete!\n",
      "\n",
      "Evaluations generated.\n",
      "Plotting performance with respect to uncertainty...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been plotted at:\n",
      "/Users/jamila.pegues/Documents/STScI_Fellowship/Functional/Library/BibTracking/repo_stsci/bibcat/src/models/test_run_rule/output\n",
      "\n",
      "Run of evaluate_performance_uncertainty() complete!\n"
     ]
    }
   ],
   "source": [
    "#Parameters for this evaluation\n",
    "filename_root = \"performance_grid_uncertainty_{0}\".format(name_model)\n",
    "figsize = (40, 12)\n",
    "colors = [\"tomato\", \"dodgerblue\", \"silver\", \"purple\", \"dimgray\", \"darkgoldenrod\", \"darkgreen\", \"green\", \"cyan\"]\n",
    "linestyles = [\"-\", \"-\", \"--\", \"-\", \"--\", \"--\", \":\", \":\", \":\"]\n",
    "\n",
    "#Run the pipeline for an evaluation of model performance as a function of uncertainty\n",
    "performer.evaluate_performance_uncertainty(operators=list_operators, dicts_texts=list_dict_texts, mappers=list_mappers,\n",
    "                                     threshold_arrays=list_threshold_arrays, buffers=list_buffers,\n",
    "                                     is_text_processed=False, do_reuse_run=do_reuse_run,\n",
    "                                     do_verify_truematch=do_verify_truematch, do_raise_innererror=do_raise_innererror,\n",
    "                                     do_save_evaluation=True, filepath_output=filepath_output,\n",
    "                                     fileroot_evaluation=fileroot_evaluation,\n",
    "                                     filename_root=filename_root,\n",
    "                                     print_freq=25, do_verbose=True, do_verbose_deep=False, figsize=figsize,\n",
    "                                     target_classifs=target_classifs_uncertainty,\n",
    "                                     colors=colors, linestyles=linestyles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ceb74-7087-4256-8044-59f4cb97db51",
   "metadata": {},
   "source": [
    "And with that, you should have new confusion matrices summarizing the basic performance for these classifiers saved in your requested directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee10749-9dc1-4460-a34c-18ecb413f450",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8c8f912-089e-462c-b345-9710f35a03f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tutorial completed successfully.\n"
     ]
    }
   ],
   "source": [
    "#Set end marker for this tutorial.\n",
    "print(\"This tutorial completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
